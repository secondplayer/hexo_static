[{"title":"从SQLAlchemy的“缓存”问题说起","date":"2017-11-21T15:37:21.000Z","path":"2017/11/21/sqlalchemy-cache/","text":"问题描述最近在排查一个问题，为了方便说明，我们假设现在有如下一个API： 1234567891011@app.route(\"/sqlalchemy/test\", methods=['GET'])def sqlalchemy_test_api(): data = &#123;&#125; # 获取商品价格 product = Product.query.get(1) data['old_price'] = product.present_price # 休眠10秒，等待外部修改价格 time.sleep(10) product = Product.query.get(1) data['new_price'] = product.present_price return jsonify(status='ok', data=data) 这里我们的后台使用了Flask作为服务端框架，SQLAlchemy作为数据库ORM框架。Product是一张商品表的ORM模型，假设原来id=1的商品价格为10，在程序休眠的10秒内价格被修改为20，那么你觉得返回的结果是多少？ old_price显然是10，那么new_price呢？讲道理的话由于外部修改价格为20了，同时程序在sleep后立刻又query了一次，你可能觉得new_price应该是20。但结果并不是，真实测试的结果是10，给人感觉就像是SQLAlchemy“缓存”了上一次的结果。 另外在测试的过程还发现一个现象，虽然在第一次API调用时两个price都是10，但是在第二次调用API时，读到的price是20。也就是说，在一个新的API开始时，之前“缓存”的结果被清除了。 SQLAlchemy的session状态管理之前我们提出了一个猜测：第二次查询是否“缓存”了第一次查询。为了验证这个猜想，我们可以把SQLALCHEMY_ECHO这个配置项打开，这是个全局配置项，官方文档定义如下： 配置项 说明 SQLALCHEMY_ECHO If set to True SQLAlchemy will log all the statements issued to stderr which can be useful for debugging. 在这个配置项打开的情况下，我们可以看到查询语句输出到终端下。我们再次调用API，可以发现第一次查询会输出类似SELECT * FROM product WHERE id = 1的语句，而第二次查询则没有这样的输出。如此看来，SQLAlchemy确实缓存了上次的结果，在第二次查询的时候直接使用了上次的结果。 实际上，当执行第一句product = Product.query.get(1)时，product这个对象处于持久状态(persistent)了，我们可以通过一些工具看到ORM对象目前处于的状态。详细的状态列表可在官方文档中找到。 123456789&gt;&gt;&gt; from sqlalchemy import inspect&gt;&gt;&gt; insp = inspect(product)&gt;&gt;&gt; insp.persistentTrue&gt;&gt;&gt; product.__dict__&#123; 'id': 1, 'present_price': 10, '_sa_instance_state': &lt;sqlalchemy.orm.state.InstanceState object at 0x1106a3350&gt;,&#125; 为了清除该对象的缓存，程度从低到高有下面几种做法。expire会清除对象里缓存的数据，这样下次查询时会直接从数据库进行查询。refresh不仅清除对象里缓存的数据，还会立刻触发一次数据库查询更新数据。expire_all的效果和expire一样，只不过会清除session里所有对象的缓存。flush会把所有本地修改写入到数据库，但没有提交。commit不仅把所有本地修改写入到数据库，同时也提交了该事务。 12345db.session.expire(product)db.session.refresh(product)db.session.expire_all()db.session.flush()db.session.commit() 我们对这几种方法依次做实验，结果发现这5个操作都会让下次查询直接从数据库进行查询，但只有commit会读到最新的price。那这个又是什么原因呢，我们已经强制每次查询走数据库，为何还是读到“缓存”的数据。这个就要用数据库的事务隔离机制来解释了。 事务隔离在数据库系统中，事务隔离级别)(isolation level)决定了数据在系统中的可见性。隔离级别从低到高分为四种：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可串行化(Serializable)。他们的区别如下表所示。 隔离级别 脏读 不可重复读 幻读 未提交读(RU) 可能 可能 可能 已提交读(RC) 不可能 可能 可能 可重复读(RR) 不可能 不可能 可能 可串行化 不可能 不可能 不可能 脏读(dirty read)是指一个事务可以读到其他事务还未提交的数据。不可重复读(non-repeatable read)是指在一个事务中同一行被读取了多次，可以读到不同的值。幻读(phantom read)是指在一个事务中执行同一个语句多次，读到的数据行发生了改变，即可能行数增加了或减少了。 前面提到的问题其实就涉及到不可重复读这个特性，即在一个事务中我们query了product.id=1的数据多次，但读到了重复的数据。对于MySQL来说，默认的事务隔离级别是RR，通过上表我们可知RR是可重复读的，因此可以解释这个现象。 事务A 事务B | ```BEGIN;``` | 1234567891011121314151617181920| ``` SELECT present_price FROM product WHERE id = 1; /* id=1的商品价格为10 */ ``` | x | | x | ```UPDATE product SET present_price = 20 WHERE id = 1; /* 修改id=1的商品价格为20 */ ``` || x | ```COMMIT;``` || ``` SELECT present_price FROM product WHERE id = 1; /* 再次查询id=1的商品价格 */ ``` | x | | ``` COMMIT; ``` | y | 对于前面的问题，我们可以把两个事务的执行时序图画出来如上所示。因此为了使第二次查询得到正确的值，我们可以把隔离级别设为RC，或者在第二次查询前进行`COMMIT`新起一个事务。## Flask-SQLAlchemy的自动提交前面还遗留一个问题没有搞清楚：在一个新的API开始时，之前“缓存”的结果似乎被清除了。由于打开了`SQLALCHEMY_ECHO`配置项，我们可以观察到每次API结束的时候都会自动触发一次`COMMIT`，而正是这个自动提交清空了所有的“缓存”。通过查找源代码，我们发现是下面这段代码在起作用：``` python@teardowndef shutdown_session(response_or_exc): if app.config[&apos;SQLALCHEMY_COMMIT_ON_TEARDOWN&apos;]: if response_or_exc is None: self.session.commit() self.session.remove() return response_or_exc 如果配置项SQLALCHEMY_COMMIT_ON_TEARDOWN为True，那么首先触发COMMIT，最后统一执行session.remove()操作，即释放连接并回滚事务操作。 有意思的是，这个配置项在Flask2.0版本的Changelog中被移除了。 Flask2.0 Changelog 关于删除的原因，作者在stackoverflow的一个帖子里进行了说明。这个帖子同时也解释了为什么在我们的生产环境中经常报这个错误：InvalidRequestError: This session is in &#39;prepared&#39; state; no further SQL can be emitted within this transaction.，而且只有重启才能解决问题。有兴趣的同学可以深入阅读一下。 总结在MySQL的同一个事务中，多次查询同一行的数据得到的结果是相同的，这里既有SQLAlchemy本身“缓存”结果的原因，也受到数据库隔离级别的影响。如果要强制读取最新的结果，最简单的办法就是在查询前手动COMMIT一次。根据这个原则，我们可以再仔细阅读下自己项目中的代码，看看会不会有一些隐藏的问题。","tags":[{"name":"Flask","slug":"Flask","permalink":"http://www.secondplayer.top/tags/Flask/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://www.secondplayer.top/tags/SQLAlchemy/"}]},{"title":"迁移博客到阿里云","date":"2017-10-31T15:00:38.000Z","path":"2017/10/31/migrate-to-aliyun-ecs/","text":"起因去年在AWS上搭建的博客已经过去一年多了，之前在使用Hexo搭建个人静态博客这篇文章中提到，使用AWS可以免费使用一年的VPS，然而一年到了后发现一个月要收费12.94美元，感觉实在性价比不高。听说阿里云在2016年开始进军海外业务，所以趁这次机会迁移过去。于是在官网上购买了美国西部（硅谷）节点的服务器，目前在双11活动期间处于优惠价，有兴趣的朋友可以趁现在入手试一下。 购买阿里云ECS ECS环境配置购买完服务器后就开始配置环境了。首先是登录服务器，默认是密码方式登录。然而每次输入密码实在是太麻烦了，建议使用密钥方式登录，在ECS后台-网络和安全-密钥对里创建一个新的密钥对，然后将其与你的实例绑定，之后就可以用私钥登录了。注意密钥对创建完成后一定要马上下载私钥，因为阿里云只给你一次下载私钥的机会，并且不要将私钥泄露给别人。 登录到服务器之后开始安装环境，在此之前需要检查一下服务器是否能访问外网。如果无法访问外网，需要到ECS后台-网络和安全-安全组里新建安全组，给安全组配置默认规则，默认规则的出方向即为允许访问任意ip的任意端口。这个安全组后面还会用到，如果你想开放一个自定义端口允许外网访问，也需要新建一个安全组并配置相应规则。 配置安全组规则 迁移博客一切准备就绪后开始迁移博客。由于hexo是静态博客，所以只需把相应的静态文件拷贝的新机器上即可。这里列一下遇到的坑以及一些升级改动。 全局安装hexo报错旧服务器上的node版本是v4.4.5，转眼一年过去了，最新版本是v8.4.0。在新版本下执行npm install hexo-cli -g安装hexo会有报错，解决办法详见官方issues，简而言之就是先执行一句npm config set unsafe-perm true再安装即可。 升级主题我的博客一直在使用这个Material Design风格的主题，名叫indigo。在一年内这个主题也有了较大的更新，升级之后界面变得更简洁了，优化了分享功能，增加了赞赏功能。升级的话也很简单，直接将代码更新到最新，按照文档更新配置即可。 评论系统切换旧博客使用的评论系统是多说，然而这家公司业务调整，已经关闭该系统了。知乎上有很多关于替代方案的讨论)，最终我选择了用gitment作为新博客的评论系统。这套评论系统最大的特点是基于GitHub Issues的评论系统，主要面向程序员群体。使用上也很方便，而且indigo主题已经支持gitment，所以只需简单配置几个参数就能使用了。 总结整个迁移步骤，主要在熟悉阿里云后台配置上花的时间最多。由于AWS是行业先行者，可以看得出阿里云的后台功能有点仿照AWS的意思，但可能是功能太多的缘故，给人感觉布局很拥挤。不管怎样，博客还是成功迁移了，在阿里云海外服务器上搭建科学上网工具也很流畅。 最后打个广告，如果有兴趣购买阿里云的相关产品可以使用这个推广链接，点击链接可以领取优惠券。","tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.secondplayer.top/tags/Hexo/"},{"name":"阿里云","slug":"阿里云","permalink":"http://www.secondplayer.top/tags/阿里云/"}]},{"title":"重构: 改善既有代码的设计","date":"2017-09-18T16:12:24.000Z","path":"2017/09/19/refactoring-book/","text":"重构这本书由著名的世界软件开发大师Martin Fowler编写，是软件开发领域的经典书籍。书中的部分内容在refactoring.com上也有提及。 重构: 改善既有代码的设计 什么是重构视上下文不同，重构有两个定义： 重构(名词)：对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本 重构(动词)：使用一系列重构手法，在不改变软件可观察行为的前提下，调整其结构 为什么要重构重构是个工具，它可以用于以下几个目的： 重构改进软件设计 重构使软件更容易理解 重构帮助找到bug 重构提高编程速度 何时重构不需要专门拨出时间进行重构，重构应该随时随地进行。你之所以重构，是因为你想做别的什么事，而重构可以帮助你把那些事做好。 事不过三，三则重构 添加功能时重构 修补错误时重构 复审代码时重构 何时不该重构 当既有代码实在太混乱，重构不如重写来得简单 当项目已接近最后期限，应该避免进行重构，因为已经没有时间了 代码的坏味道「如果尿布臭了，就换掉它」。代码的坏味道指出了重构的可能性。 重复代码 (Duplicated Code) 过长函数 (Long Method) 过大的类 (Large Class) 过长参数列 (Long Parameter List) 发散式变化 (Divergent Change) switch语句 (Switch Statements) 中间人 (Middle Man) 异曲同工的类 (Alternative Classes with Different Interfaces) 过多的注释 (Comments) … 构筑测试体系重构的基本技巧「小步前进，频繁测试」已经得到了多年的实践检验。因此如果你想进行重构，首要前提就是拥有一个可靠的测试体系。 常用重构方法提炼函数 (Extract Method) 当我看见一个过长的函数或者一段需要注释才能让人理解用途的代码，我就会将这段代码放进一个独立函数中 1234567void printOwing() &#123; printBanner(); //print details System.out.println (\"name: \" + _name); System.out.println (\"amount \" + amount);&#125; 123456789void printOwing() &#123; printBanner(); printDetails(amount);&#125;void printDetails (double amount) &#123; System.out.println (\"name: \" + _name); System.out.println (\"amount \" + amount);&#125; 引入解释性变量 (Introduce Explaining Variable) 表达式有可能非常复杂而难以阅读。这种情况下，临时变量可以帮助你将表达式分解为比较容易管理的形式。 123456if ((platform.toUpperCase().indexOf(\"MAC\") &gt; -1) &amp;&amp; (browser.toUpperCase().indexOf(\"IE\") &gt; -1) &amp;&amp; wasInitialized() &amp;&amp; resize &gt; 0)&#123; // do something&#125; 123456final boolean isMacOs = platform.toUpperCase().indexOf(\"MAC\") &gt; -1;final boolean isIEBrowser = browser.toUpperCase().indexOf(\"IE\") &gt; -1;final boolean wasResized = resize &gt; 0;if (isMacOs &amp;&amp; isIEBrowser &amp;&amp; wasInitialized() &amp;&amp; wasResized) &#123; // do something&#125; 分解临时变量 (Split Temporary Variable) 如果临时变量承担多个责任，它就应该被替换(分解)为多个临时变量，每个变量只承担一个责任。同一个临时变量承担两件不同的事情，会令代码阅读者糊涂。 1234double temp = 2 * (_height + _width);System.out.println (temp);temp = _height * _width;System.out.println (temp); 1234final double perimeter = 2 * (_height + _width);System.out.println (perimeter);final double area = _height * _width;System.out.println (area); 移除对参数的赋值 (Remove Assignments to Parameters) 我之所以不喜欢(对参数赋值)这样的做法，因为它降低了代码的清晰度，而且混淆了按值传递和按引用传递这两种参数传递方式。当然，面对那些使用「输出式参数」(output parameters)的语言，你不必遵循这条规则。不过在那些语言中我会尽量少用输出式参数。 12345int discount (int inputVal, int quantity, int yearToDate) &#123; if (inputVal &gt; 50) &#123; inputVal -= 2; &#125;&#125; 123456int discount (int inputVal, int quantity, int yearToDate) &#123; int result = inputVal; if (inputVal &gt; 50) &#123; result -= 2; &#125;&#125; 提炼类 (Extract Class) 某个类做了应该由两个类做的事。此时你需要考虑哪些部分可以分离出去，并将它们分离到一个单独的类中。 提炼类 移除中间人 (Remove Middle Man) 每当客户要使用受托类的新特性时，你就必须在服务端添加一个简单委托函数。随着受托类的特性(功能)越来越多，这一过程会让你痛苦不已。服务类完全变成了一个“中间人”，此时你就应该让客户直接调用受托类。 移除中间人 以字面常量取代魔法数 (Replace Magic Number with Symbolic Constant) 所谓魔法数(magic number)是指拥有特殊意义，却又不能明确表现出这种意义的数字。如果你需要在不同的地点引用同一个逻辑数，魔法数会让你烦恼不已，因为一旦这些数发生改变，你就必须在程序中找到所有魔法数，并将它们全部修改一遍，这简直就是一场噩梦。就算你不需要修改，要准确指出每个魔法数的用途，也会让你颇费脑筋。 123double potentialEnergy(double mass, double height) &#123; return mass * 9.81 * height;&#125; 1234double potentialEnergy(double mass, double height) &#123; return mass * GRAVITATIONAL_CONSTANT * height;&#125;static final double GRAVITATIONAL_CONSTANT = 9.81; 分解条件表达式 (Decompose Conditional) 程序之中，复杂的条件逻辑是最常导致复杂度上升的地点之一。你必须编写代码来检查不同的条件分支、根据不同的分支做不同的事，然后你很快就会得到一个相当长的函数。对于条件逻辑，将每个分支条件分解成新函数可以给你带来更多好处：可以突出条件逻辑，更清楚地表明每个分支的作用，并且突出每个分支的原因。 123if (date.before (SUMMER_START) || date.after(SUMMER_END)) charge = quantity * _winterRate + _winterServiceCharge;else charge = quantity * _summerRate; 123if (notSummer(date)) charge = winterCharge(quantity);else charge = summerCharge (quantity); 合并条件表达式 (Consolidate Conditional Expression) 之所以要合并条件代码，有两个重要原因。首先，合并后的条件代码会告诉你“实际上只有一次条件检查，只不过有多个并列条件需要检查而已”，从而使这一次检查的用意更清晰。其次，这项重构往往可以为你使用提炼函数(Extract Method)做好准备。将检查条件提炼成一个独立函数对于厘清代码意义非常有用，因为它把描述“做什么”的语句换成了“为什么这样做”。 12345double disabilityAmount() &#123; if (_seniority &lt; 2) return 0; if (_monthsDisabled &gt; 12) return 0; if (_isPartTime) return 0; // compute the disability amount 123double disabilityAmount() &#123; if (isNotEligableForDisability()) return 0; // compute the disability amount 合并重复的条件片段 (Consolidate Duplicate Conditional Fragments) 有时你会发现，一组条件表达式的所有分支都执行了相同的某段代码。如果是这样，你就应该将这段代码搬移到条件表达式外面。这样，代码才能更清楚地表明哪些东西随条件的变化而变化、哪些东西保持不变。 12345678if (isSpecialDeal()) &#123; total = price * 0.95; send();&#125;else &#123; total = price * 0.98; send();&#125; 12345if (isSpecialDeal()) total = price * 0.95;else total = price * 0.98;send(); 移除控制标记 (Remove Control Flag) 人们之所以会使用这样的控制标记，因为结构化编程原则告诉他们：每个子程序只能有一个入口和一个出口。我赞同“单一入口”原则（而且现代编程语言也强迫我们这样做），但是“单一出口”原则会让你在代码中加入讨厌的控制标记，大大降低条件表达式的可读性。这就是编程语言提供break语句和continue语句的原因：用它们跳出复杂的条件语句。去掉控制标记所产生的效果往往让你大吃一惊：条件语句真正的用途会清晰得多。 12345678910111213141516boolean checkSecurity(String[] people) &#123; boolean found = false; for (int i = 0; i &lt; people.length; i++) &#123; if (!found)&#123; if (people[i].equals(\"Don\")) &#123; sendAlert(); found = true; &#125; if (people[i].equals(\"John\")) &#123; sendAlert(); found = true; &#125; &#125; &#125; return found;&#125; 123456789101112131415boolean checkSecurity(String[] people) &#123; for (int i = 0; i &lt; people.length; i++) &#123; if (!found)&#123; if (people[i].equals(\"Don\")) &#123; sendAlert(); return true; &#125; if (people[i].equals(\"John\")) &#123; sendAlert(); return true; &#125; &#125; &#125; return false;&#125; 以卫语句取代嵌套条件表达式 (Replace Nested Conditional with Guard Clauses) 如果条件表达式的两条分支都是正常行为，就应该使用形如if…else…的条件表达式；如果某个条件极其罕见，就应该单独检查该条件，并在该条件为真时立刻从函数中返回。这样的单独检查常常被称为“卫语句”(guard clauses)。 这个方法的精髓是：给某一条分支以特别的重视。它告诉阅读者：这种情况很罕见，如果它真地发生了，请做一些必要的整理工作，然后退出。 “每个函数只能有一个入口和一个出口”的观念，根深蒂固于某些程序员的脑海里。现今的编程语言都会强制保证每个函数只有一个入口，至于“单一出口”规则，其实不是那么有用。保持代码清晰才是最关键的：如果单一出口能使这个函数更清晰易读，那么就使用单一出口；否则就不必这么做。 123456789101112double getPayAmount() &#123; double result; if (_isDead) result = deadAmount(); else &#123; if (_isSeparated) result = separatedAmount(); else &#123; if (_isRetired) result = retiredAmount(); else result = normalPayAmount(); &#125;; &#125; return result;&#125;; 123456double getPayAmount() &#123; if (_isDead) return deadAmount(); if (_isSeparated) return separatedAmount(); if (_isRetired) return retiredAmount(); return normalPayAmount();&#125;; 扩展阅读：关于如何重构嵌套条件表达式，可以阅读如何重构“箭头型”代码，这篇文章更深层次地讨论了这个问题。 将查询函数和修改函数分离 (Separate Query from Modifier) 下面是一条好规则：任何有返回值的函数，都不应该有看得到的副作用。 如果你遇到一个“既有返回值又有副作用”的函数，就应该试着将查询动作从修改动作中分割出来。 将查询函数和修改函数分离","tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://www.secondplayer.top/tags/读书笔记/"},{"name":"重构","slug":"重构","permalink":"http://www.secondplayer.top/tags/重构/"}]},{"title":"常见推荐系统介绍","date":"2017-09-07T16:07:37.000Z","path":"2017/09/08/recommendation-system-book/","text":"本文主要是对项亮的推荐系统实践部分章节进行了一些总结，先从什么是推荐系统开始讲起，然后介绍了评测推荐系统的指标和方法，最后介绍了常见的推荐系统算法。 《推荐系统实践》封面 什么是推荐系统随着信息技术和互联网的快速发展，人们逐渐从信息匮乏的时代走入了信息过载的时代。每天都有海量的信息被生产出来，用户如何从中找到自己感兴趣的内容变得越来越困难，内容生产者也在想方设法让自己生成的内容从海量信息中脱颖而出。为了解决信息过载的问题，历史上出现过的代表方案有分类目录和搜索引擎，这两者都要求用户明确知道自己需要的内容关键词。而推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣的内容。推荐系统通过发掘用户的行为，找到用户的个性化需求，从而将长尾商品准确地推荐给需要它的用户，帮助用户发现那些他们感兴趣但很难发现的商品。 推荐系统的应用在互联网的各类网站中都可以看到推荐系统的应用，尽管不同网站使用的技术不同，但总的来说几乎所有的推荐系统应用都是由前台的展示页面、后台的日志系统以及推荐算法系统构成。 电子商务：淘宝、京东、亚马逊 电影/视频：Netflix、YouTube、爱奇艺 音乐：Pandora、网易云音乐、豆瓣FM 社交网络：Facebook、Twitter、LinkedIn、新浪微博 个性化阅读：Digg、Flipboard、今日头条 基于位置的服务：Foursquare 个性化广告：Facebook Audience Network 推荐系统实验方法在推荐系统中，主要有三种评测推荐效果的实验方法：离线实验、用户调查、在线实验。 推荐系统评测指标 用户满意度：用户的主观感受，主要通过用户调查的方式获得，也可以间接从用户行为统计中得到。 预测准确度：度量一个推荐系统或推荐算法预测用户行为的能力。评分预测的预测准确度一般通过计算测试集和训练集的均方根误差(RMSE)和平均绝对误差(MAE)得到。TopN推荐的预测准确度一般通过计算测试集和训练集的准确率(precison)和召回率(recall)得到。 令rui是用户u对物品i的实际评分，r^ui是推荐算法给出的预测评分，T是测试集，那么：RMSE = sqrt(Σu,i∈T(rui-r^ui)2 / |T|)MAE = Σu,i∈T|rui-r^ui| / |T| 令R(u)是用户u在训练集上的推荐结果，T(u)是用户u在测试集上的行为结果，U是用户集合，那么： Precision = Σu∈U|R(u) ∩ T(u)| / Σu∈U|R(u)| Recall = Σu∈U|R(u) ∩ T(u)| / Σu∈U|T(u)| 覆盖率：描述一个推荐系统对物品长尾的发掘能力。 假设用户集合为U，物品集合为I，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，那么：Coverage = |∪u∈UR(u)| / |I| 多样性：为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域。 新颖性：是指给用户推荐那些他们以前没听说过的商品。 惊喜度(serendipity)：如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高。 信任度：提高信任度的方法是给出合理的推荐解释。 实时性：推荐系统需要实时地更新推荐列表来满足用户新的行为变化，并且需要能够将新加入系统的物品推荐给用户。 健壮性(robust)：衡量一个推荐系统抗击作弊的能力。 在众多指标中，作者认为：对于可以离线优化的指标，应该在给定覆盖率、多样性、新颖性等限制条件下，尽量优化预测准确度。 常见推荐系统算法推荐系统是联系用户和物品的媒介，而推荐联系用户和物品的方式主要有3种，如下图所示。 3种联系用户和物品的推荐系统 第一种方法，首先找到用户喜欢的物品，然后找到与这些物品相似的物品推荐给用户。基于这种方法可以给出如下的推荐解释：购买了该商品的用户也经常购买这些商品。这种方法通常被称为基于物品的协同过滤算法(item-based collaborative filtering)。第二种方法，首先找到和用户有相似兴趣的其他用户，然后推荐这些其他用户喜欢的物品。这种方法通常被称为基于用户的协同过滤算法(user-based collaborative filtering)。第三种方法，首先找到用户感兴趣的物品特征，然后推荐包含这些特征的物品。这种方法核心思想是通过隐含特征联系用户兴趣和物品，通常被称为隐语义模型算法(latent factor model)。 协同过滤算法个性化推荐系统的一个重要算法是基于用户行为分析，学术界一般将这种类型的算法称为协同过滤算法(collaborative filtering)。 顾名思义，协同过滤就是指用户可以齐心协力，通过不断地和网站互动，使自己的推荐列表能够不断过滤掉自己不感兴趣的物品，从而越来越满足自己的需求。 基于物品的协同过滤算法基于物品的协同过滤算法(以下简称ItemCF)，是目前业界应用最多的算法，最早由电子商务公司亚马逊提出。ItemCF算法给用户推荐那些和他们之前喜欢的物品相似的物品，它的主要步骤分为两步。 (1) 计算物品之间的相似度 (2) 根据物品的相似度和用户的历史行为给用户生成推荐列表 第一步计算相似度可用余弦相似度公式 令N(i)是喜欢物品i的用户集合，那么物品i和物品j的相似度可定义为： wij = |N(i) ∩ N(j)| / sqrt(|N(i)||N(j)|) 第二步计算用户对物品的兴趣，如下公式的含义是：和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。 令puj为用户u对物品j的兴趣，wji是物品j和物品i的相似度，rui是用户u对物品i的兴趣（对于隐反馈数据集，如果用户u对物品i有过行为，可简单令rui=1），S(j,K)是和物品j最相似的K个物品的集合，那么： puj = Σi∈N(u)∩S(j,K) wjirui 最后选取该用户兴趣值最高的N的物品作为推荐列表。 基于用户的协同过滤算法基于用户的协同过滤算法(以下简称UserCF)，是推荐系统中最古老的算法。UserCF算法先找到和他有相似兴趣的其他用户，然后把那些用户喜欢的、而他没有听说过的物品推荐给他，它的主要步骤分为两步。 (1) 找到和目标用户兴趣相似的用户集合 (2) 找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户 第一步计算用户的兴趣相似度可用余弦相似度公式 令N(u)是用户u曾经有过正反馈的物品集合，那么用户u和用户v的相似度可定义为： wuv = |N(u) ∩ N(v)| / sqrt(|N(u)||N(v)|) 第二步计算用户对物品的兴趣 令pui为用户u对物品i的兴趣，wuv是用户u和用户v的相似度，rvi是用户v对物品i的兴趣（对于隐反馈数据集，如果用户v对物品i有过行为，可简单令rvi=1），S(u,K)是和用户u兴趣最相似的K个用户的集合，那么： pui = Σv∈N(i)∩S(u,K) wuvrvi 最后选取该用户兴趣值最高的N的物品作为推荐列表。 隐语义模型隐语义模型算法(以下简称LFM)，是最近几年推荐系统领域最为热门的研究话题。LFM算法的核心思想是通过隐含特征联系用户兴趣和物品，它的主要步骤分为三步。 (1) 对物品进行分类 (2) 确定用户对哪些类的物品感兴趣以及感兴趣的程度 (3) 对于给定的类，确定物品在这个类的权重，并且选择性地推荐给用户 关于如何给物品分类，一个简单方案是由编辑来手动分类，但这样存在很强的主观性和较大的工作量。为了解决这个困难，研究人员提出可以从用户数据出发，基于隐含语义分析技术(latent variable analysis)自动找到哪些类，然后进行个性化推荐。隐含语义分析技术有很多著名的模型和方法，比如pLSA、LDA、隐含类别模型、隐含主题模型、矩阵分解等。 LFM通过如下公式计算用户u对物品i的兴趣： Preferenceui = Σk∈[1,K] pu,kqi,k 其中pu,k度量了用户u的兴趣和第k个隐类的关系，而qi,k度量了第k个隐类和物品i的关系。这两个参数的计算需要一点最优化理论或者机器学习的知识，这里不多作介绍。 三种算法的优缺点比较 LFM是一种基于机器学习的算法，有较好的理论基础。ItemCF/UserCF是基于邻域的方法，更多的是一种基于统计的方法，没有学习过程。 假设有M个用户和N个物品，选取F个隐类。UserCF需要存储用户的相似度矩阵，存储空间是O(M*M)。ItemCF需要存储物品的相似度矩阵，存储空间是O(N*N)。LFM需要的存储空间是O(F*(M+N))。如果用户数很多，UserCF将会占据很大的内存。如果物品数很多，ItemCF将会占据很大的内存。LFM存储空间最少，这在M和N很大时可以很好地节省离线计算的内存。 假设有M个用户和N个物品和K条用户对物品的行为记录。那么，UserCF计算用户表的时间复杂度是O(N*(K/N)2)，而ItemCF计算物品表的时间复杂度是O(M*(K/M)2)。而对于LFM，如果用F个隐类，迭代S次，那么它的时间复杂度是O(K*F*S)。在一般情况下，LFM的时间复杂度要稍微高于UserCF和ItemCF，主要是因为该算法需要多次迭代。 ItemCF算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果，但LFM无法提供这样的解释。 总结在互联网应用中可以看到大量推荐系统的应用，它主要解决了信息过载的问题，通过算法主动帮助用户找到自己感兴趣的内容。常见的推荐系统算法有三种，分别代表三种联系用户和物品的方式，它们是：基于物品的协同过滤算法(ItemCF)，基于用户的协同过滤算法(ItemCF)，隐语义模型算法(LFM)。三种方法各有优劣，需要根据实际场景选择合适的算法，通过不断优化指标找到最优算法。","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://www.secondplayer.top/tags/推荐系统/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://www.secondplayer.top/tags/读书笔记/"}]},{"title":"使用redis的有序集合实现排行榜功能","date":"2017-07-23T05:43:36.000Z","path":"2017/07/23/redis-sorted-set/","text":"排行榜是业务开发中常见的一个场景，如何设计一个好的数据结构能够满足高效实时的查询，下面我们结合一个实际例子来讨论一下。 场景选手报名参加活动，观众可以对选手进行投票，每个观众对同一名选手只能投一票，活动期间最多投四票。后台需要提供如下接口： 接口1：返回TOP 10的选手信息及投票数 接口2：返回活动总参与选手数及总投票数 接口3：对于每个选手，返回自己的投票数，排名，距离上一名差的票数 基于数据库的方案首先需要一张表存储投票记录，一次投票就是一条记录。这张表相当于投票明细，判断每人只投一张票以及最多投四张表都依赖对这张表的查询。如果直接对这张表做TOP 10的查询，则需要根据选手id做聚合查询，这样每次查询必然耗时。为了优化查询，可以增加另一张排行榜表，用一个定时任务每隔一段时间对原表做聚合查询，然后将结果写进排行榜表里，表里包含投票数及排名的字段，这样查询TOP 10和排名的时候直接查这张表。引入另一张表加快了性能，但牺牲了实时性，活动说明里需加上类似“榜单数据每10分钟同步一次”的话来告知用户。 基于redis的方案对于排行榜的需求，redis有一个数据结构非常适合做这件事，那就是有序集合(sorted set)。 redis的有序集合相关命令有序集合和集合一样可以存储字符串，另外有序集合的成员可以关联一个分数(score)，这个分数用于集合排序。下面以投票为例说明常见的命令，vote_activity是有序集合的key。12345678910111213141516171819202122232425262728#给Alice投票redis&gt; zincrby vote_activity 1 Alice&quot;1&quot; #给Bob投票redis&gt; zincrby vote_activity 1 Bob&quot;1&quot;#给Alice投票redis&gt; zincrby vote_activity 1 Alice&quot;2&quot;#查看Alice投票数redis&gt; zscore vote_activity Alice&quot;2&quot;#获取Alice排名(从高到低，zero-based)redis&gt; zrevrank vote_activity Alice(integer) 0#获取前10名(从高到低)redis&gt; zrevrange vote_activity 0 91) &quot;Alice&quot;2) &quot;Bob&quot;#获取前10名及对应的分数(从高到低)redis&gt; zrevrange vote_activity 0 9 withscores1) &quot;Alice&quot;2) &quot;2&quot;3) &quot;Bob&quot;4) &quot;1&quot;#获取总参与选手数redis&gt; zcard vote_activity(integer) 2 接口实现回到最开始的场景，大部分需求都已经得到满足，还剩下两个数据需要单独说一下。接口2中的总投票数没有直接的接口获得，一种方法是先用ZRANGE遍历所有的key，然后对score进行求和，另一种方法是对总票数单独用一个数据结构存储。接口3的距离上一名差的票数，先用ZREVRANK获取自己排名，然后用ZREVRANGE获取上一排名的分数，最后用自己的分数减去上一名的分数即可，代码示例如下：12345678def get_next_step(redis_key, member): next_step = None score = redis.zscore(redis_key, member) rank = redis.zrevrank(redis_key, member) if rank &gt; 0: next_member = redis.zrevrange(redis_key, rank - 1, rank - 1, withscores=True) next_step = next_member[0][1] - score return next_step 另外如果两个key的score相同，排序逻辑是按照key的字母序排序。在有些情况下这个可能不满足实际要求，因此需要按实际情况重新设计key。比如如果要求同分数情况下按时间排序，那么key最好加上时间戳前缀。 redis与数据库的同步redis通常是作为缓存层加速查询的，如果数据没有做持久化则有概率会丢失数据。一个方案是用定时任务定时同步redis与数据库的数据，数据库里存储着原始数据，通过计算数据库的数据和redis做对比，可以修正由于redis不稳定导致的数据不一致。这里需要注意的是在同步过程时redis的数据有可能还在增长，因此最好先读redis的数据，然后记下时间，查询指定时间段里的数据库的数据，最后再用ZINCRBY增量修正redis数据，而不是直接用ZADD覆盖redis数据。 总结redis的有序集合是一个非常高效的数据结构，可以替代数据库里一些很难实现的操作。它的一个典型应用场景就是排行榜，通过ZRANK可以快速得到用户的排名，通过ZRANGE可以快速得到TOP N的用户列表，它们的复杂度都是O(log(N))，用来替代数据库查询可以大大提升性能。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://www.secondplayer.top/tags/Redis/"}]},{"title":"使用redis实现分布式锁","date":"2017-07-16T05:40:10.000Z","path":"2017/07/16/redis-distribution-lock/","text":"背景在类似秒杀这样的并发场景下，为了确保同一时刻只能允许一个用户访问资源，需要利用加锁的机制控制资源的访问权。如果服务只在单台机器上运行，可以简单地用一个内存变量进行控制。而在多台机器的系统上，则需要用分布式锁的机制进行并发控制。基于redis的一些特性，利用redis可以既方便又高效地模拟锁的实现。 一个简单方案让我们先从一个简单的实现说起，这里用到了redis的两个命令，SETNX和EXPIRE。如果lock_key不存在，那么就设置lock_key的值为1，并且设置过期时间；如果lock_key存在，说明已经有人在使用这把锁，访问失败。12345def acquire_lock(lock_key, expire_timeout=60): if redis.setnx(lock_key, 1): redis.expire(lock_key, expire_timeout) return True return False 逻辑上看似乎没有问题，但是考虑一下异常情况：如果setnx设置成功，但expire由于某些原因（比如超时）操作失败，那么这把锁就永远存在了，也就是所谓的死锁，后面的人永远无法访问这个资源。 利用时间戳取值的方案为了解决死锁，我们可以利用setnx的value来做文章。上例中的我们设的value是1，其实并没有派上用场。因此可以考虑将value设为当前时间加上expire_timeout，当setnx设置失败后，我们去读lock_key的value，并且和当前时间作比对，如果当前时间大于value，那么资源理当被释放。代码示例如下：123456789def acquire_lock(lock_key, expire_timeout=60): expire_time = int(time.time()) + expire_timeout if redis.setnx(lock_key, expire_time): redis.expire(lock_key, expire_timeout) return True redis_value = redis.get(lock_key) if redis_value and int(time.time()) &gt; int(redis_value): redis.delete(lock_key) return False 然而仔细推敲下这段代码仍然能发现一些问题。第一，这个方案依赖时间，如果在分布式系统中的时间没有同步，则会对方案产生一定偏差。第二，假设C1和C2都没拿到锁，它们都去读value并对比时间，在竞态条件(race condition)下可能产生如下的时序：C1删除lock_key，C1获得锁，C2删除lock_key，C2获得锁。这样C1和C2同时拿到了锁，显然是不对的。 改进后的方案幸运的是，redis里还有一个指令可以帮助我们解决这个问题。GETSET指令在set新值的同时会返回老的值，这样的话我们可以检查返回的值，如果该值和之前读出来的值相同，那么这次操作有效，反之则无效。代码示例如下：123456789101112def acquire_lock(lock_key, expire_timeout=60): expire_time = int(time.time()) + expire_timeout if redis.setnx(lock_key, expire_time): redis.expire(lock_key, expire_timeout) return True redis_value = redis.get(lock_key) if redis_value and int(time.time()) &gt; int(redis_value): expire_time = int(time.time()) + expire_timeout old_value = redis.getset(lock_key, expire_time) if int(old_value) == int(redis_value): return True return False 这个方案基本可以满足要求，除了有一个小瑕疵，由于getset会去修改value，在竞态条件下可能会被修改多次导致timeout有细微的误差，但这个对结果影响不大。 最终方案以上方案实现起来略显繁琐，但从redis 2.6.12版本开始有一个更为简便的方法。我们可以使用SET指令的扩展 SET key value [EX seconds] [PX milliseconds] [NX|XX] ，这个指令相当于对SETNX和EXPIRES进行了合并，因而我们的算法可以简化为如下一行：123def acquire_lock(lock_key, expire_timeout=60): ret = redis.set(lock_key, int(time.time()), nx=True, ex=expire_timeout): return ret 总结在redis 2.6.12版本之后我们可以用一个简单的SET命令实现分布式锁，而在此版本之前则需要将SETNX和GETSET配合使用一个较为繁琐的方案。简化后的方案对于开发者来说当然是好事，但通过学习这一演变过程我们会对问题有更深刻的印象。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://www.secondplayer.top/tags/Redis/"}]},{"title":"使用Hexo搭建个人静态博客","date":"2016-06-12T06:42:23.000Z","path":"2016/06/12/hexo-blog-setup/","text":"最近有时间折腾了一下建一个个人博客，在对比了几家之后，最终决定用Hexo作为框架，GitHub作为部署平台搭建博客。VPS选用的是AWS，新用户可以免费使用1年的EC2，足够用来体验了。 申请VPSVPS指的是虚拟服务器，国内推荐用阿里云，国外推荐用Linode, Digital Ocean, AWS。我选择的是AWS，主要有几个原因，一是因为新用户可以试用免费1年，二是因为公司用的就是AWS，对其各项操作比较熟悉，最后一个原因是选择一个国外服务器可以自己搭建ShadowSocks科学上网。 申请域名域名申请服务商，国内有万网，美橙，国外有GoDaddy, NameCheap。我选择的是NameCheap，主要因为价格因素。你要问国内的那些更便宜为啥不选？呵呵国内的情况你懂的。 DNS解析DNS解析推荐DNSPOD，业界良心，服务免费且强大。域名绑定前记得先到NameCheap控制台设置DNS解析到DNSPOD提供的两个免费DNS解析服务器，具体参考这里。 Hexo安装及配置前面把主机和域名搞定了，现在开始在主机上搭建博客了。提到博客，一般都会选用经典的WordPress搭建。不过现在越来越多的个人博客都采用静态博客框架，典型的如Hexo， Jekyll， Octopress。从流行度和技术栈的角度来看，我倾向于选择Hexo。Hexo是一个用Node.js搭建的博客框架，简单强大易上手。静态文件用Markdown编写，Hexo会根据静态文件自动生成网页。 安装依赖环境 安装Git 1$ sudo apt-get install git 安装Node.js 通常用nvm(Node.js Version Manager)安装Node环境安装必要环境1$ sudo apt-get install build-essential libssl-dev 下载nvm1$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh 查看可用版本1$ nvm ls-remote 选取最新版本，这里我们安装v4.4.5，并将其设为默认123$ nvm install 4.4.5$ nvm alias default 4.4.5$ nvm use default 安装Hexo 我们通过npm分别安装hexo客户端和服务端 12$ npm install -g hexo-cli$ npm install -g hexo-server 生成文章 初始化hexo环境 我们把hexo_blog作为博客目录名，首先初始化hexo 123$ hexo init ~/hexo_blog$ cd ~/hexo_blog$ npm install 修改配置文件_config.yml 配置Site, URL, Directory, Writing等基本信息，详细参考这篇配置文档这里建议设置default_layout为draft，这样默认生成文章在Draft里，确认后再发布到Public。 发布文章 新建文章，以名称first_post为例 1$ hexo new first-post 编辑文章，文章都存放在source目录下 1$ vim ~/hexo_blog/source/_drafts/first-post.md 发布文章，这将会把文章从draft移到post目录 1$ hexo publish first-post 运行服务 启动服务器，默认起在4000端口，成功后访问http://localhost:4000 预览效果 1$ hexo server 部署博客我们需要选择一个静态文件的托管平台，首选GitHub，国内可以考虑Coding（最近收购了GitCafe）。 创建GitHub Repository 参考这个步骤，创建一个名为hexo_static的repo，注意设置为Public 修改配置文件_config.yml，注意替换$username 1234deploy: type: git repo: https://github.com/$username/hexo_static.git branch: master 安装hexo git插件 1$ npm install hexo-deployer-git --save 部署 12$ hexo generate$ hexo deploy 按照提示输入用户名和密码，一切步骤完成后，所有文件都已生成并提交到Git上了 自动化整个自动化的思路是：运行该脚本，生成博客静态文件，通过hexo deploy实现自动提交到Git，然后通过本地更新代码，对关联的空分支进行git push操作，触发post-receive钩子，从而将静态文件同步到/var/www/hexo目录，而该目录正是Nginx将80端口转发到本地的路径。 初始化空仓库 1$ git init --bare ~/hexo_bare 创建git hooks 1$ vim ~/hexo_bare/hooks/post-receive 这里我们用到了post-receive这个钩子，当一个本地仓库执行git push后会触发。post-receive具体内容为 123#!/bin/bashgit --work-tree=/var/www/hexo --git-dir=/home/$USER/hexo_bare checkout -f 将空仓库关联到主仓库 123$ git clone https://github.com/$username/hexo_static.git ~/hexo_static $ cd ~/hexo_static$ git remote add live ~/hexo_bare 创建自动化脚本 1$ vim ~/hexo_blog/hexo_git_deploy.sh 脚本内容为 1234567#!/bin/bashhexo cleanhexo generate hexo deploy( cd ~/hexo_static ; git pull ; git push live master) Nginx配置 创建/var/www/hexo目录，稍后会将Nginx的请求映射到该目录 123$ sudo mkdir -p /var/www/hexo$ sudo chown -R $USER:$USER /var/www/hexo$ sudo chmod -R 755 /var/www/hexo 编辑/etc/nginx/sites-available/default1$ sudo vim /etc/nginx/sites-available/default 配置Nginx将80端口的请求映射到/var/www/hexo目录下123456server &#123; listen 80 default_server; listen [::]:80 default_server ipv6only=on; root /var/www/hexo; index index.html index.htm;... 重启Nginx 1$ sudo service nginx restart 发布流程至此，我们可以总结下今后发布文章或更新博客的流程 1234$ hexo new my-post$ vim ~/hexo_blog/source/_draft/my-post.md$ hexo publish my-post$ hexo generate 接着运行hexo server，然后在http://localhost:4000 上预览效果，如果不满意则继续修改my-post.md（此时在_post目录下），重新生成文件（hexo generate），再预览直到可以发布为止 而最终对外发布，我们只需要敲下一行命令就完成了 1$ ~/hexo_blog/hexo_git_deploy.sh 其他 主题 默认hexo的主题是landscape，如果你想与众不同的话，可以用下别的主题或者自定义主题。官方收录的请点击这里，我选择的是indigo，主要看中的是他的Material Design风格 评论 常见的评论系统有Disqus，多说，友言等，我选择的是多说。接入非常简单，去网站上注册个账号，然后将示例代码插到网页中即可 统计 流量统计选择cnzz(现已被整合进Umeng+) 监控 可以接入监控宝 参考 Hexo Documentation How to Create a Blog with Hexo On Ubuntu 14.04","tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.secondplayer.top/tags/Hexo/"}]},{"title":"Hello World","date":"2016-06-10T16:00:00.000Z","path":"2016/06/11/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]