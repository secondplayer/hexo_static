[{"title":"机器学习笔记3: 广义线性模型","date":"2018-02-13T08:33:18.000Z","path":"2018/02/13/machine-learning-glm/","text":"牛顿方法之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。 我们先来看下如何用牛顿方法(Newton’s Method)求解θ使得f(θ)=0。如下图所示，首先我们选取一个初始点，比如说令θ=4.5，然后作出f(θ)在该点的切线，这条切线与x轴相交的点θ=2.8作为下一次迭代的点。下右图又一次重复了一轮迭代，f(θ)在θ=2.8处的切线与x轴相交于θ=1.8处，然后再次迭代到θ=1.3处。 以此类推，我们得到迭代规则如下： 牛顿方法可以找到θ使得f(θ)=0，那么如何把它应用到最大化l(θ)上呢？当l(θ)达到最大点时，其导数为0，因此问题转化为找到θ使得l’(θ)=0。所以，令f(θ)=l’(θ)，我们推导出迭代规则： 上式中的θ是参数为实数的情况，当θ为向量时，我们可以推导出更通用的公式： 其中∇θl(θ)是指l(θ)的梯度，H是一个n n的矩阵，被称为*海森矩阵(Hessian Matrix)。 和梯度下降法相比，牛顿方法收敛的速度更快，迭代的次数也更少。但是牛顿方法每次迭代的计算量更大，因为每次都要计算一个n阶矩阵的逆。总体而言，当n不是很大时牛顿方法计算的速度更快。当牛顿方法用来求解最大化对数似然函数l(θ)时，这个方法也被称为Fisher Scoring。 指数分布族到目前为止，我们分别学习了分类(classification)和回归(regression)两类问题。在回归问题里，我们假设p(y|x;θ)服从高斯分布N(0,σ2)；在分类问题里，我们假设p(y|x;θ)服从伯努利分布B(φ)。后面我们会看到，这两类问题可以被统一到一个更通用的模型，这个模型被称为广义线性模型(Generalized Linear Models, GLM)。在介绍GLM前，我们先引入一个概念：指数分布族(exponential family)。 指数分布族是指一类可以被表示为如下形式的概率分布： 其中η被称为分布的自然参数(natural parameter)，或者是标准参数(canonical parameter)；T(y)是充分统计量(sufficient statistic)，通常T(y)=y；a(η)是对数分割函数(log partition function)。e-a(η)通常起着归一化的作用，使得整个分布的总和/积分为1。 如果固定参数T, a, b，就定义了一个以η为参数的函数族。当η取不同的值，我们就得到一个不同的分布函数。 现在我们来证明高斯分布(Gaussian distribution)和伯努利分布(Bernoulli distribution)都属于指数分布族。 对于伯努利分布B(φ)，其y值为0或1，因而有p(y=1;φ)=φ; p(y=0;φ)=1-φ 。所以可推导p(y;φ)如下： 对比指数分布族的定义，可得η=log(φ/(1-φ))，进而可得φ=1/(1+e-η)，而这正是sigmoid函数的定义。同样对比其他参数，可得： 综上可得，伯努利分布属于指数分布族，且φ的形式与sigmoid函数一致。 接下来我们继续来看高斯分布N(μ,σ2)。回忆下之前推导线性回归的时候，σ2的值与θ和hθ(x)无关，因此为了简化证明，我们令σ2=1，所以可推导p(y;μ)如下： 对比指数分布族的定义，进而可得： 因而我们证明了高斯分布也属于指数分布族。事实上，大多数概率分布都属于指数分布族，我们列举一些如下： 多项式分布(Multinomial distribution)：对有k个离散结果的事件建模 泊松分布(Poisson distribution)：描述单位时间内独立事件发生次数的概率 伽马分布(Gamma distribution)与指数分布(Exponential distribution)：描述独立事件的时间间隔的概率 β分布(Beta distribution)：在(0,1)区间的连续概率分布 Dirichlet分布(Dirichlet distribution)：分布的分布(for distributions over probabilities) 广义线性模型介绍完指数分布族后，我们开始正式介绍广义线性模型(GLM)。对回归或者分类问题来说，我们都可以借助于广义线性模型进行预测。广义线性模型基于如下三个假设： 假设1: p(y|x;θ) 服从以η为参数的指数分布族中的某个分布 假设2: 给定x，我们的目标是预测T(y)的期望值，大多数情况下T(y)=y，所以假设函数可以写为h(x)=E[T(y)|x] 假设3: η与x是线性相关的，即η=θTx 依据这三个假设，我们可以推导出一个非常优雅的学习算法，也就是GLM。接下来我们分别看几个通过GLM推导出来的算法。 最小二乘法假设p(y|x;θ)服从高斯分布N(μ,σ2)，我们可以推导如下： 上式中第一个等号来自假设2，第二个等号是高斯分布的特性，第三个等号来自上一节中我们已经证明了η=μ，第四个等号来自假设3。 逻辑回归假设p(y|x;θ)服从伯努利分布B(φ)，我们可以推导如下： 上式中第一个等号来自假设2，第二个等号是伯努利分布的特性，第三个等号来自上一节中我们已经证明了φ=1/(1+e-η)，第四个等号来自假设3。 这里多介绍一些术语：将η与原始概率分布中的参数联系起来的函数g(即g(η)=E[T(y);η])称为标准响应函数(canonical response function)，它的逆函数g-1称为标准关联函数(canonical link function)。 Softmax回归接下来我们来看一个更复杂的模型。在分类问题上，我们不止预测0和1两个值，假设我们预测的值有k个，即y∈{1,2,…,k}。那么我们就不能再使用伯努利分布了，我们考虑用多项式分布(Multinomial distribution)建模。 我们用φ1, φ2, … ,φk表示每个结果出现的概率，即P(y=k)=φk。由于所有结果概率之和为1，所以实际上k个参数中有1个是多余的，即： 为了使多项式分布能表示成指数分布族的形式，我们定义T(y)如下： 和我们之前的例子不一样，T(y)这次不等于y，而是一个k-1维的向量。我们用(T(y))i表示T(y)的第i个元素。 接下来我们引入指示函数(indicator function)：1{·}。如果参数表达式为真，则指示函数取值为1；表达式为假，指示函数取值为0，即1{True} = 1, 1{False} = 0。基于上述定义，我们可以得到：(T(y))i = 1{y = i}，进一步可得： 现在我们可以证明多项式分布也属于指数分布族，证明如下： 由η的表达式，我们可以得到η和φ的对应关系： 这个从η和φ的映射函数被称为softmax函数(softmax function)。有了softmax函数并结合假设3，我们可以求出p(y|x;θ)为： 这个k分类问题的算法被称为softmax回归(softmax regression)，它是逻辑回归更一般化的形式。 最后我们可以求出假设函数： 如果要求解参数θ，我们可以先求出它的对数似然函数l(θ)，然后用梯度上升或牛顿方法进行迭代。 总结 梯度上升和牛顿方法都能用于求解最大化l(θ)的问题，区别是牛顿方法收敛速度更快，但是它每次迭代的计算量也更大，当数据规模不大时总体上性能更优 指数分布族描述了一大类我们常见的概率分布，高斯分布、伯努利分布、多项式分布等都属于指数分布族 广义线性模型(GLM)描述了一种更通用的学习模型，最小二乘法和逻辑回归都可以从GLM推导出来 k分类问题可以用softmax回归建模，逻辑回归可以看作是softmax回归的特例(k=2) 参考资料 斯坦福大学机器学习课CS229讲义 pdf 网易公开课：机器学习课程 双语字幕视频","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.secondplayer.top/tags/机器学习/"}]},{"title":"机器学习笔记2: 欠拟合与过拟合","date":"2018-01-29T14:55:48.000Z","path":"2018/01/29/machine-learning-underfitting-and-overfitting/","text":"线性回归的概率解释在解决线性回归问题时，我们为什么要使用最小二乘法作为代价函数？这个问题我们会通过概率统计来进行解释。 使用最小二乘法作为代价函数 假设对每个样本数据，输出值与预测值存在一定的误差ε(i)，误差可能来自未被建模的其他因素，也可能是随机的噪音。因而预测函数可写为 另外我们假设误差属于独立同分布(independently and identically distributed)，并且服从高斯分布N(0,σ2)，所以ε(i)的概率密度函数为 因此可推导出 P(y(i)|x(i);θ)表示：在θ为给定的参数的情况下，概率y(i)以x(i)为随机变量的概率分布，注意θ不是随机变量。 给定X(输入矩阵)和θ，Y(输出矩阵)的分布记为p(Y|X;θ)，这个概率的值我们定义为以θ为变量的似然函数(likelihood function) 由于每个误差值是独立分布的，所以 在θ作为参数的情况下，我们希望给定X时出现Y的概率是最大，因此问题变成最大化L(θ)。在求解最大化L(θ)的过程中，对L(θ)取对数将简化一些运算，因此我们最大化对数似然函数l(θ)： 上面的公式可以得出，最大化似然函数L(θ)等价于最小化代价函数J(θ)，这就是我们为什么取最小二乘法作为代价函数的原因。 局部加权线性回归如下左图显示了用线性函数y=θ0+θ1x拟合数据集的结果，由于数据集并不是一条直线，因此拟合效果不太理想。如果我们增加一个特征项x2，即用y=θ0+θ1x+θ2x2拟合数据集，那么得到的结果如中间所示。粗看起来，增加更多的特征项可以使拟合效果更好，然而事实上并非如此。如果我们把特征项增加到6项，即y= Σj∈[0,5]θjxj，我们得到的结果如右图所示。尽管这个曲线完美拟合整个数据集，但是我们很难说它能准确预测未知的新数据。我们把左图这种情况称为欠拟合(underfitting)，就是说模型没有很好地捕捉到数据特征，不能够很好地拟合数据；右图这种情况称为过拟合(overfitting)，就是说模型把数据学习得太彻底，以至于不能很好地预测新的数据。 欠拟合与过拟合 这里我们介绍一个新的方法称为局部加权线性回归(locally weighted linear regression)，它可以弥补普通线性回归模型欠拟合或者过拟合的问题。假设我们要预测x这个点对应的值，局部加权线性回归对x附近的每一个点赋予一定的权重，离x越近权重越大，离x越远权重越小。通过赋予权重，使得x附近的点对结果影响最大，离x很远的点对结果的影响可以忽略不计。因此代价函数表示如下，其中w(i)表示权重。 由上述对权重特性的描述，w(i)的图像应该是个钟形曲线。 通常我们定义w(i)的函数如下： 上式中的τ称为波长(bandwidth)，波长的大小取决了附近点的下降速率，参数根据对数据集的实验进行调整。 局部加权线性回归是一种非参数学习算法(non-parametric learning algorithm)，而之前我们学的普通线性回归是一种参数学习算法(parametric learning algorithm)。参数学习算法有固定的明确的参数，参数一旦确定，就不会改变了，我们不需要保留训练集中的训练样本。而非参数学习算法每进行一次预测，需要重新计算数据，因此需要保留训练数据。当训练数据较多时，非参数学习算法需要占用更多的存储空间。 逻辑回归现在我们开始讨论分类(classification)问题。分类问题和回归问题很类似，只不过预测的y值从连续值变成了离散值。我们先从最简单的二分类(binary classification)问题开始讨论，此时y值只有0和1两个取值。0被称为负类(negative class)，1被称为正类(positive class)，有时也会用符号-和+标记。给定x(i)，对应的y(i)值也被称为训练集的标签(label)。 一个二分类的例子是，通过给定肿瘤的大小(x(i))来预测是否为恶性肿瘤(y(i))。我们先用之前线性回归的方法求解这个问题，如下图所示，从结果上看线性回归的效果并不好。 直觉上看，hθ(x)的取值应该是介于0到1之间的。为了达到这一点，通常我们选取hθ(x)如下： 其中g(z)被称为逻辑函数(logistic function)或者sigmoid函数(sigmoid function)。它的图形如下所示： g(z)的值域在0到1之间，当z趋向正无穷时，g(z)趋向于1；当z趋向负无穷是，g(z)趋向于0。g(z)还有另外一个有用的特性，g(z)对z的导数可以用其自身来表示，具体推导如下： 那么对于这样一个逻辑回归模型，我们如何选取θ进行拟合呢？套用之前极大似然估计的思想，我们为这个模型赋予一些概率假设： 这两个式子可以简化成一个： 其似然函数L(θ)为： 对数似然函数l(θ)为： 如何最大化l(θ)呢？类似于在线性回归里求代价函数最小值是用的梯度下降法，我们可以用梯度上升法求函数的最大值。因此θ的每次迭代如下： 对l(θ)进行求导： 所以我们得到梯度上升法则： 这个迭代规则和线性回归的最小均方算法(LMS)看上去非常类似，但它们并不是同一个算法，因为现在的hθ(x)是一个非线性函数。然而它们都拥有相似的形式，这究竟是巧合还是有更深层次的原因呢？这个我们后面会讲到。 感知器学习算法最后我们再简短地介绍一个新的算法。之前我们选取sigmoid函数作为hθ(x)，如果我们换成另外一个函数： 然后用同样的迭代规则： 这样我们得到的算法称为感知器学习算法(perceptron learning algorithm)。在上世纪60年代，感知器(perceptron)被认为是神经网络组成单元的一个粗糙的模型，这个我们后续会详细展开。 总结 线性回归的概率解释：最大化似然函数等价于最小化代价函数，这就是我们为什么取最小二乘法作为代价函数的原因 为了避免普通线性回归欠拟合和过拟合的问题，可以采用局部加权线性回归方法，通过赋予权重来强化离x近的点的结果，弱化离x远的点的结果 局部加权线性回归是一种非参数学习算法，普通线性回归是一种参数学习算法 二分类问题通常取hθ(x)为sigmoid函数，其迭代规则与线性回归的规则形式相似 参考资料 Coursera机器学习课程讲义 Week 3 Lecture Notes 斯坦福大学机器学习课CS229讲义 pdf 网易公开课：机器学习课程 双语字幕视频","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.secondplayer.top/tags/机器学习/"}]},{"title":"机器学习笔记1: 线性回归","date":"2018-01-01T05:38:16.000Z","path":"2018/01/01/machine-learning-linear-regression/","text":"监督学习与非监督学习机器学习是指给定一些训练数据，使机器能够利用它们分析未知数据。任何机器学习问题都可以分为两类：监督学习(Supervised Learning)和非监督学习(Unsupervised Learning)。这两类的区别在于：监督学习的训练数据有特征有标签，而非监督学习的训练数据没有。 监督学习问题一般是指给定输入预测输出，根据输出值的不同可以分为两类：回归(regression)和分类(classification)。回归预测的是连续值，分类预测的是离散值。 举例来说，给定房子的面积来预测房价是一个回归问题，因为房价是个连续值。如果把它改成预测房价是否超过某个阈值，那么这是一个离散问题，因为输出是个“是”或“否”的离散值。同理，给定一个人的图片预测TA的年龄是个回归问题，预测TA的性别是个分类问题。 而非监督学习问题在给定输入时，不知道预测的结果长什么样子，我们是从一堆数据里推导出其中的结构。 非监督学习最常见的应用是聚类(clustering)。举例来说，给定《美国经济》的1000篇文章，按照不同主题进行自动分类。另一个非聚类的典型例子是鸡尾酒会效应，指的是在一个嘈杂的鸡尾酒会环境中谈话中，尽管周围噪音很多，你仍能分辨出朋友对你说话的声音。 线性回归让我们先从监督学习中最简单的一个问题开始，假设我们有一个数据集如下，我们假设房价受住房面积的影响。 住房面积(英尺2) 房价(1000$) 2104 400 1600 330 2400 369 1416 232 3000 540 … … 我们的目标是对给定数据集学习出一个函数h: x → y，使得对每个输入x，h(x)都能很好的预测出输出y。由于历史原因，我们把h称为假设函数(Hypothesis Function)。下图描述了这一过程： 假设函数 我们需要对假设函数进行建模，最简单的方式是将它视为线性函数，因而可表示成： 其中θi称之为参数(parameter)或者权重(weight)。为了简化表述，我们定义θ0=1，那么： 其中最右面等式中的θ和x都是向量表示，n是输入变量的个数（在这个例子中n=1）。 那么我们应该如何选取θ，使得h(x)和y的误差最小。为此我们定义代价函数(cost function)如下： 其中x(i)这种上标表示方式是指第i个训练集的输入数据，y(i)是第i个训练集的输出值，m是训练集的个数。 梯度下降算法引入了代价函数后，我们的目标变成了：选择合适的θ，使得J(θ)最小。在这方面我们主要介绍梯度下降算法(Gradient Descent)。这个算法的主要思想是先选取一个初始点θ0，然后不断改变θ的值使得J(θ)变小，直到J(θ)收敛到最小值。特别的，为了使J(θ)变得最小，我们选择下一个θ值时应该选择能使J(θ)下降最快的那个值，在数学上就是对J(θ)求导，具体来说下一个选取的θ值就是： 其中α是学习率(learning rate)，它会影响梯度下降的幅度。在每次迭代中，可以选取不同的α值。下图是梯度下降算法的图示，在选取初始点后，每次都按下降速率最快的方式寻找下一个点，直到找到最低点。 梯度下降算法图示 我们将J(θ)展开进行推导，由此得到： 因而迭代规则更新为： 这个规则被称为最小均方算法(Least Mean Squares，缩写为LMS)或者Widrow-Hoff算法。 这个算法在每次迭代时都要计算一遍训练集的数据，因而被称为批量梯度下降法(Batch Gradient Descent)。当训练集数据量很大时，计算速度将变得很慢。为了解决这个问题，我们可以在每次迭代时随机选取训练集数据的一部分来代替整体，这种方法称之为随机梯度下降法(Stochastic Gradient Descent)。随机梯度下降法由于只选取了部分样本数据，因此迭代过程会比较不稳定，虽然每次迭代不一定按着全体最优解靠近，但整体上趋于全体最优解。 正规方程梯度下降法求解的缺点是需要很多次迭代，是否存在更好的方法呢。正规方程(Normal Equation)就是一个不需要进行迭代就能求解的方法，其公式如下： 其中X和y定义如下，XT是矩阵X的转置。 这个公式证明需要大量线性代数的知识，详细证明可以查阅参考资料。下表给出了梯度下降和正规函数两个算法的对比。 梯度下降 正规函数 需要选择学习率α 不需要选择学习率α 需要很多次迭代 不需要迭代 O(kn2) O(n3)，需要计算XTX的逆矩阵 n很大时也能正常工作 n很大时计算很慢 在实践中，当n&gt;=10000时不适合用正规函数，推荐改用梯度下降算法。 另外正规方程还有一个问题，就是XTX可能是不可逆的。不可逆的可能原因是我们使用了冗余的特征(比如两个特征线性相关)或者使用了太多的特征(比如特征数超过了样本数)。解决方法是删除一些多余的特征。 总结 机器学习问题可以分为监督学习和非监督学习，区别在于训练数据是否有特征 监督学习问题根据预测值的不同分为两类：预测值是连续值的叫回归，预测值是离散值的叫分类 最简单的回归模型是线性回归，求解线性回归的两个方法是：梯度下降和正规方程 当训练数据量较大时(n&gt;=10000)时推荐用梯度下降，数据量较小时用正规函数 参考资料 Coursera机器学习课程讲义 1 2 斯坦福大学机器学习课CS229讲义 pdf 网易公开课：机器学习课程 双语字幕视频","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://www.secondplayer.top/tags/机器学习/"}]},{"title":"从SQLAlchemy的“缓存”问题说起","date":"2017-11-21T15:37:21.000Z","path":"2017/11/21/sqlalchemy-cache/","text":"问题描述最近在排查一个问题，为了方便说明，我们假设现在有如下一个API： 1234567891011@app.route(\"/sqlalchemy/test\", methods=['GET'])def sqlalchemy_test_api(): data = &#123;&#125; # 获取商品价格 product = Product.query.get(1) data['old_price'] = product.present_price # 休眠10秒，等待外部修改价格 time.sleep(10) product = Product.query.get(1) data['new_price'] = product.present_price return jsonify(status='ok', data=data) 这里我们的后台使用了Flask作为服务端框架，SQLAlchemy作为数据库ORM框架。Product是一张商品表的ORM模型，假设原来id=1的商品价格为10，在程序休眠的10秒内价格被修改为20，那么你觉得返回的结果是多少？ old_price显然是10，那么new_price呢？讲道理的话由于外部修改价格为20了，同时程序在sleep后立刻又query了一次，你可能觉得new_price应该是20。但结果并不是，真实测试的结果是10，给人感觉就像是SQLAlchemy“缓存”了上一次的结果。 另外在测试的过程还发现一个现象，虽然在第一次API调用时两个price都是10，但是在第二次调用API时，读到的price是20。也就是说，在一个新的API开始时，之前“缓存”的结果被清除了。 SQLAlchemy的session状态管理之前我们提出了一个猜测：第二次查询是否“缓存”了第一次查询。为了验证这个猜想，我们可以把SQLALCHEMY_ECHO这个配置项打开，这是个全局配置项，官方文档定义如下： 配置项 说明 SQLALCHEMY_ECHO If set to True SQLAlchemy will log all the statements issued to stderr which can be useful for debugging. 在这个配置项打开的情况下，我们可以看到查询语句输出到终端下。我们再次调用API，可以发现第一次查询会输出类似SELECT * FROM product WHERE id = 1的语句，而第二次查询则没有这样的输出。如此看来，SQLAlchemy确实缓存了上次的结果，在第二次查询的时候直接使用了上次的结果。 实际上，当执行第一句product = Product.query.get(1)时，product这个对象处于持久状态(persistent)了，我们可以通过一些工具看到ORM对象目前处于的状态。详细的状态列表可在官方文档中找到。 123456789&gt;&gt;&gt; from sqlalchemy import inspect&gt;&gt;&gt; insp = inspect(product)&gt;&gt;&gt; insp.persistentTrue&gt;&gt;&gt; product.__dict__&#123; 'id': 1, 'present_price': 10, '_sa_instance_state': &lt;sqlalchemy.orm.state.InstanceState object at 0x1106a3350&gt;,&#125; 为了清除该对象的缓存，程度从低到高有下面几种做法。expire会清除对象里缓存的数据，这样下次查询时会直接从数据库进行查询。refresh不仅清除对象里缓存的数据，还会立刻触发一次数据库查询更新数据。expire_all的效果和expire一样，只不过会清除session里所有对象的缓存。flush会把所有本地修改写入到数据库，但没有提交。commit不仅把所有本地修改写入到数据库，同时也提交了该事务。 12345db.session.expire(product)db.session.refresh(product)db.session.expire_all()db.session.flush()db.session.commit() 我们对这几种方法依次做实验，结果发现这5个操作都会让下次查询直接从数据库进行查询，但只有commit会读到最新的price。那这个又是什么原因呢，我们已经强制每次查询走数据库，为何还是读到“缓存”的数据。这个就要用数据库的事务隔离机制来解释了。 事务隔离在数据库系统中，事务隔离级别(isolation level)决定了数据在系统中的可见性。隔离级别从低到高分为四种：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可串行化(Serializable)。他们的区别如下表所示。 隔离级别 脏读 不可重复读 幻读 未提交读(RU) 可能 可能 可能 已提交读(RC) 不可能 可能 可能 可重复读(RR) 不可能 不可能 可能 可串行化 不可能 不可能 不可能 脏读(dirty read)是指一个事务可以读到其他事务还未提交的数据。不可重复读(non-repeatable read)是指在一个事务中同一行被读取了多次，可以读到不同的值。幻读(phantom read)是指在一个事务中执行同一个语句多次，读到的数据行发生了改变，即可能行数增加了或减少了。 前面提到的问题其实就涉及到不可重复读这个特性，即在一个事务中我们query了product.id=1的数据多次，但读到了重复的数据。对于MySQL来说，默认的事务隔离级别是RR，通过上表我们可知RR是可重复读的，因此可以解释这个现象。 事务A 事务B BEGIN; BEGIN; SELECT present_price FROM product WHERE id = 1; /* id=1的商品价格为10 */ UPDATE product SET present_price = 20 WHERE id = 1; /* 修改id=1的商品价格为20 */ COMMIT; SELECT present_price FROM product WHERE id = 1; /* 再次查询id=1的商品价格 */ COMMIT; 对于前面的问题，我们可以把两个事务的执行时序图画出来如上所示。因此为了使第二次查询得到正确的值，我们可以把隔离级别设为RC，或者在第二次查询前进行COMMIT新起一个事务。 Flask-SQLAlchemy的自动提交前面还遗留一个问题没有搞清楚：在一个新的API开始时，之前“缓存”的结果似乎被清除了。由于打开了SQLALCHEMY_ECHO配置项，我们可以观察到每次API结束的时候都会自动触发一次COMMIT，而正是这个自动提交清空了所有的“缓存”。通过查找源代码，我们发现是下面这段代码在起作用： 1234567@teardowndef shutdown_session(response_or_exc): if app.config['SQLALCHEMY_COMMIT_ON_TEARDOWN']: if response_or_exc is None: self.session.commit() self.session.remove() return response_or_exc 如果配置项SQLALCHEMY_COMMIT_ON_TEARDOWN为True，那么首先触发COMMIT，最后统一执行session.remove()操作，即释放连接并回滚事务操作。 有意思的是，这个配置项在Flask2.0版本的Changelog中被移除了。 Flask2.0 Changelog 关于删除的原因，作者在stackoverflow的一个帖子里进行了说明。这个帖子同时也解释了为什么在我们的生产环境中经常报这个错误：InvalidRequestError: This session is in &#39;prepared&#39; state; no further SQL can be emitted within this transaction.，而且只有重启才能解决问题。有兴趣的同学可以深入阅读一下。 总结在MySQL的同一个事务中，多次查询同一行的数据得到的结果是相同的，这里既有SQLAlchemy本身“缓存”结果的原因，也受到数据库隔离级别的影响。如果要强制读取最新的结果，最简单的办法就是在查询前手动COMMIT一次。根据这个原则，我们可以再仔细阅读下自己项目中的代码，看看会不会有一些隐藏的问题。","tags":[{"name":"Flask","slug":"Flask","permalink":"http://www.secondplayer.top/tags/Flask/"},{"name":"SQLAlchemy","slug":"SQLAlchemy","permalink":"http://www.secondplayer.top/tags/SQLAlchemy/"}]},{"title":"迁移博客到阿里云","date":"2017-10-31T15:00:38.000Z","path":"2017/10/31/migrate-to-aliyun-ecs/","text":"起因去年在AWS上搭建的博客已经过去一年多了，之前在使用Hexo搭建个人静态博客这篇文章中提到，使用AWS可以免费使用一年的VPS，然而一年到了后发现一个月要收费12.94美元，感觉实在性价比不高。听说阿里云在2016年开始进军海外业务，所以趁这次机会迁移过去。于是在官网上购买了美国西部（硅谷）节点的服务器，目前在双11活动期间处于优惠价，有兴趣的朋友可以趁现在入手试一下。 购买阿里云ECS ECS环境配置购买完服务器后就开始配置环境了。首先是登录服务器，默认是密码方式登录。然而每次输入密码实在是太麻烦了，建议使用密钥方式登录，在ECS后台-网络和安全-密钥对里创建一个新的密钥对，然后将其与你的实例绑定，之后就可以用私钥登录了。注意密钥对创建完成后一定要马上下载私钥，因为阿里云只给你一次下载私钥的机会，并且不要将私钥泄露给别人。 登录到服务器之后开始安装环境，在此之前需要检查一下服务器是否能访问外网。如果无法访问外网，需要到ECS后台-网络和安全-安全组里新建安全组，给安全组配置默认规则，默认规则的出方向即为允许访问任意ip的任意端口。这个安全组后面还会用到，如果你想开放一个自定义端口允许外网访问，也需要新建一个安全组并配置相应规则。 配置安全组规则 迁移博客一切准备就绪后开始迁移博客。由于hexo是静态博客，所以只需把相应的静态文件拷贝的新机器上即可。这里列一下遇到的坑以及一些升级改动。 全局安装hexo报错旧服务器上的node版本是v4.4.5，转眼一年过去了，最新版本是v8.4.0。在新版本下执行npm install hexo-cli -g安装hexo会有报错，解决办法详见官方issues，简而言之就是先执行一句npm config set unsafe-perm true再安装即可。 升级主题我的博客一直在使用这个Material Design风格的主题，名叫indigo。在一年内这个主题也有了较大的更新，升级之后界面变得更简洁了，优化了分享功能，增加了赞赏功能。升级的话也很简单，直接将代码更新到最新，按照文档更新配置即可。 评论系统切换旧博客使用的评论系统是多说，然而这家公司业务调整，已经关闭该系统了。知乎上有很多关于替代方案的讨论)，最终我选择了用gitment作为新博客的评论系统。这套评论系统最大的特点是基于GitHub Issues的评论系统，主要面向程序员群体。使用上也很方便，而且indigo主题已经支持gitment，所以只需简单配置几个参数就能使用了。 总结整个迁移步骤，主要在熟悉阿里云后台配置上花的时间最多。由于AWS是行业先行者，可以看得出阿里云的后台功能有点仿照AWS的意思，但可能是功能太多的缘故，给人感觉布局很拥挤。不管怎样，博客还是成功迁移了，在阿里云海外服务器上搭建科学上网工具也很流畅。 最后打个广告，如果有兴趣购买阿里云的相关产品可以使用这个推广链接，点击链接可以领取优惠券。","tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.secondplayer.top/tags/Hexo/"},{"name":"阿里云","slug":"阿里云","permalink":"http://www.secondplayer.top/tags/阿里云/"}]},{"title":"重构: 改善既有代码的设计","date":"2017-09-18T16:12:24.000Z","path":"2017/09/19/refactoring-book/","text":"重构这本书由著名的世界软件开发大师Martin Fowler编写，是软件开发领域的经典书籍。书中的部分内容在refactoring.com上也有提及。 重构: 改善既有代码的设计 什么是重构视上下文不同，重构有两个定义： 重构(名词)：对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本 重构(动词)：使用一系列重构手法，在不改变软件可观察行为的前提下，调整其结构 为什么要重构重构是个工具，它可以用于以下几个目的： 重构改进软件设计 重构使软件更容易理解 重构帮助找到bug 重构提高编程速度 何时重构不需要专门拨出时间进行重构，重构应该随时随地进行。你之所以重构，是因为你想做别的什么事，而重构可以帮助你把那些事做好。 事不过三，三则重构 添加功能时重构 修补错误时重构 复审代码时重构 何时不该重构 当既有代码实在太混乱，重构不如重写来得简单 当项目已接近最后期限，应该避免进行重构，因为已经没有时间了 代码的坏味道「如果尿布臭了，就换掉它」。代码的坏味道指出了重构的可能性。 重复代码 (Duplicated Code) 过长函数 (Long Method) 过大的类 (Large Class) 过长参数列 (Long Parameter List) 发散式变化 (Divergent Change) switch语句 (Switch Statements) 中间人 (Middle Man) 异曲同工的类 (Alternative Classes with Different Interfaces) 过多的注释 (Comments) … 构筑测试体系重构的基本技巧「小步前进，频繁测试」已经得到了多年的实践检验。因此如果你想进行重构，首要前提就是拥有一个可靠的测试体系。 常用重构方法提炼函数 (Extract Method) 当我看见一个过长的函数或者一段需要注释才能让人理解用途的代码，我就会将这段代码放进一个独立函数中 1234567void printOwing() &#123; printBanner(); //print details System.out.println (\"name: \" + _name); System.out.println (\"amount \" + amount);&#125; 123456789void printOwing() &#123; printBanner(); printDetails(amount);&#125;void printDetails (double amount) &#123; System.out.println (\"name: \" + _name); System.out.println (\"amount \" + amount);&#125; 引入解释性变量 (Introduce Explaining Variable) 表达式有可能非常复杂而难以阅读。这种情况下，临时变量可以帮助你将表达式分解为比较容易管理的形式。 123456if ((platform.toUpperCase().indexOf(\"MAC\") &gt; -1) &amp;&amp; (browser.toUpperCase().indexOf(\"IE\") &gt; -1) &amp;&amp; wasInitialized() &amp;&amp; resize &gt; 0)&#123; // do something&#125; 123456final boolean isMacOs = platform.toUpperCase().indexOf(\"MAC\") &gt; -1;final boolean isIEBrowser = browser.toUpperCase().indexOf(\"IE\") &gt; -1;final boolean wasResized = resize &gt; 0;if (isMacOs &amp;&amp; isIEBrowser &amp;&amp; wasInitialized() &amp;&amp; wasResized) &#123; // do something&#125; 分解临时变量 (Split Temporary Variable) 如果临时变量承担多个责任，它就应该被替换(分解)为多个临时变量，每个变量只承担一个责任。同一个临时变量承担两件不同的事情，会令代码阅读者糊涂。 1234double temp = 2 * (_height + _width);System.out.println (temp);temp = _height * _width;System.out.println (temp); 1234final double perimeter = 2 * (_height + _width);System.out.println (perimeter);final double area = _height * _width;System.out.println (area); 移除对参数的赋值 (Remove Assignments to Parameters) 我之所以不喜欢(对参数赋值)这样的做法，因为它降低了代码的清晰度，而且混淆了按值传递和按引用传递这两种参数传递方式。当然，面对那些使用「输出式参数」(output parameters)的语言，你不必遵循这条规则。不过在那些语言中我会尽量少用输出式参数。 12345int discount (int inputVal, int quantity, int yearToDate) &#123; if (inputVal &gt; 50) &#123; inputVal -= 2; &#125;&#125; 123456int discount (int inputVal, int quantity, int yearToDate) &#123; int result = inputVal; if (inputVal &gt; 50) &#123; result -= 2; &#125;&#125; 提炼类 (Extract Class) 某个类做了应该由两个类做的事。此时你需要考虑哪些部分可以分离出去，并将它们分离到一个单独的类中。 提炼类 移除中间人 (Remove Middle Man) 每当客户要使用受托类的新特性时，你就必须在服务端添加一个简单委托函数。随着受托类的特性(功能)越来越多，这一过程会让你痛苦不已。服务类完全变成了一个“中间人”，此时你就应该让客户直接调用受托类。 移除中间人 以字面常量取代魔法数 (Replace Magic Number with Symbolic Constant) 所谓魔法数(magic number)是指拥有特殊意义，却又不能明确表现出这种意义的数字。如果你需要在不同的地点引用同一个逻辑数，魔法数会让你烦恼不已，因为一旦这些数发生改变，你就必须在程序中找到所有魔法数，并将它们全部修改一遍，这简直就是一场噩梦。就算你不需要修改，要准确指出每个魔法数的用途，也会让你颇费脑筋。 123double potentialEnergy(double mass, double height) &#123; return mass * 9.81 * height;&#125; 1234double potentialEnergy(double mass, double height) &#123; return mass * GRAVITATIONAL_CONSTANT * height;&#125;static final double GRAVITATIONAL_CONSTANT = 9.81; 分解条件表达式 (Decompose Conditional) 程序之中，复杂的条件逻辑是最常导致复杂度上升的地点之一。你必须编写代码来检查不同的条件分支、根据不同的分支做不同的事，然后你很快就会得到一个相当长的函数。对于条件逻辑，将每个分支条件分解成新函数可以给你带来更多好处：可以突出条件逻辑，更清楚地表明每个分支的作用，并且突出每个分支的原因。 123if (date.before (SUMMER_START) || date.after(SUMMER_END)) charge = quantity * _winterRate + _winterServiceCharge;else charge = quantity * _summerRate; 123if (notSummer(date)) charge = winterCharge(quantity);else charge = summerCharge (quantity); 合并条件表达式 (Consolidate Conditional Expression) 之所以要合并条件代码，有两个重要原因。首先，合并后的条件代码会告诉你“实际上只有一次条件检查，只不过有多个并列条件需要检查而已”，从而使这一次检查的用意更清晰。其次，这项重构往往可以为你使用提炼函数(Extract Method)做好准备。将检查条件提炼成一个独立函数对于厘清代码意义非常有用，因为它把描述“做什么”的语句换成了“为什么这样做”。 12345double disabilityAmount() &#123; if (_seniority &lt; 2) return 0; if (_monthsDisabled &gt; 12) return 0; if (_isPartTime) return 0; // compute the disability amount 123double disabilityAmount() &#123; if (isNotEligableForDisability()) return 0; // compute the disability amount 合并重复的条件片段 (Consolidate Duplicate Conditional Fragments) 有时你会发现，一组条件表达式的所有分支都执行了相同的某段代码。如果是这样，你就应该将这段代码搬移到条件表达式外面。这样，代码才能更清楚地表明哪些东西随条件的变化而变化、哪些东西保持不变。 12345678if (isSpecialDeal()) &#123; total = price * 0.95; send();&#125;else &#123; total = price * 0.98; send();&#125; 12345if (isSpecialDeal()) total = price * 0.95;else total = price * 0.98;send(); 移除控制标记 (Remove Control Flag) 人们之所以会使用这样的控制标记，因为结构化编程原则告诉他们：每个子程序只能有一个入口和一个出口。我赞同“单一入口”原则（而且现代编程语言也强迫我们这样做），但是“单一出口”原则会让你在代码中加入讨厌的控制标记，大大降低条件表达式的可读性。这就是编程语言提供break语句和continue语句的原因：用它们跳出复杂的条件语句。去掉控制标记所产生的效果往往让你大吃一惊：条件语句真正的用途会清晰得多。 12345678910111213141516boolean checkSecurity(String[] people) &#123; boolean found = false; for (int i = 0; i &lt; people.length; i++) &#123; if (!found)&#123; if (people[i].equals(\"Don\")) &#123; sendAlert(); found = true; &#125; if (people[i].equals(\"John\")) &#123; sendAlert(); found = true; &#125; &#125; &#125; return found;&#125; 123456789101112131415boolean checkSecurity(String[] people) &#123; for (int i = 0; i &lt; people.length; i++) &#123; if (!found)&#123; if (people[i].equals(\"Don\")) &#123; sendAlert(); return true; &#125; if (people[i].equals(\"John\")) &#123; sendAlert(); return true; &#125; &#125; &#125; return false;&#125; 以卫语句取代嵌套条件表达式 (Replace Nested Conditional with Guard Clauses) 如果条件表达式的两条分支都是正常行为，就应该使用形如if…else…的条件表达式；如果某个条件极其罕见，就应该单独检查该条件，并在该条件为真时立刻从函数中返回。这样的单独检查常常被称为“卫语句”(guard clauses)。 这个方法的精髓是：给某一条分支以特别的重视。它告诉阅读者：这种情况很罕见，如果它真地发生了，请做一些必要的整理工作，然后退出。 “每个函数只能有一个入口和一个出口”的观念，根深蒂固于某些程序员的脑海里。现今的编程语言都会强制保证每个函数只有一个入口，至于“单一出口”规则，其实不是那么有用。保持代码清晰才是最关键的：如果单一出口能使这个函数更清晰易读，那么就使用单一出口；否则就不必这么做。 123456789101112double getPayAmount() &#123; double result; if (_isDead) result = deadAmount(); else &#123; if (_isSeparated) result = separatedAmount(); else &#123; if (_isRetired) result = retiredAmount(); else result = normalPayAmount(); &#125;; &#125; return result;&#125;; 123456double getPayAmount() &#123; if (_isDead) return deadAmount(); if (_isSeparated) return separatedAmount(); if (_isRetired) return retiredAmount(); return normalPayAmount();&#125;; 扩展阅读：关于如何重构嵌套条件表达式，可以阅读如何重构“箭头型”代码，这篇文章更深层次地讨论了这个问题。 将查询函数和修改函数分离 (Separate Query from Modifier) 下面是一条好规则：任何有返回值的函数，都不应该有看得到的副作用。 如果你遇到一个“既有返回值又有副作用”的函数，就应该试着将查询动作从修改动作中分割出来。 将查询函数和修改函数分离","tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://www.secondplayer.top/tags/读书笔记/"},{"name":"重构","slug":"重构","permalink":"http://www.secondplayer.top/tags/重构/"}]},{"title":"常见推荐系统介绍","date":"2017-09-07T16:07:37.000Z","path":"2017/09/08/recommendation-system-book/","text":"本文主要是对项亮的推荐系统实践部分章节进行了一些总结，先从什么是推荐系统开始讲起，然后介绍了评测推荐系统的指标和方法，最后介绍了常见的推荐系统算法。 《推荐系统实践》封面 什么是推荐系统随着信息技术和互联网的快速发展，人们逐渐从信息匮乏的时代走入了信息过载的时代。每天都有海量的信息被生产出来，用户如何从中找到自己感兴趣的内容变得越来越困难，内容生产者也在想方设法让自己生成的内容从海量信息中脱颖而出。为了解决信息过载的问题，历史上出现过的代表方案有分类目录和搜索引擎，这两者都要求用户明确知道自己需要的内容关键词。而推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣的内容。推荐系统通过发掘用户的行为，找到用户的个性化需求，从而将长尾商品准确地推荐给需要它的用户，帮助用户发现那些他们感兴趣但很难发现的商品。 推荐系统的应用在互联网的各类网站中都可以看到推荐系统的应用，尽管不同网站使用的技术不同，但总的来说几乎所有的推荐系统应用都是由前台的展示页面、后台的日志系统以及推荐算法系统构成。 电子商务：淘宝、京东、亚马逊 电影/视频：Netflix、YouTube、爱奇艺 音乐：Pandora、网易云音乐、豆瓣FM 社交网络：Facebook、Twitter、LinkedIn、新浪微博 个性化阅读：Digg、Flipboard、今日头条 基于位置的服务：Foursquare 个性化广告：Facebook Audience Network 推荐系统实验方法在推荐系统中，主要有三种评测推荐效果的实验方法：离线实验、用户调查、在线实验。 推荐系统评测指标 用户满意度：用户的主观感受，主要通过用户调查的方式获得，也可以间接从用户行为统计中得到。 预测准确度：度量一个推荐系统或推荐算法预测用户行为的能力。评分预测的预测准确度一般通过计算测试集和训练集的均方根误差(RMSE)和平均绝对误差(MAE)得到。TopN推荐的预测准确度一般通过计算测试集和训练集的准确率(precison)和召回率(recall)得到。 令rui是用户u对物品i的实际评分，r^ui是推荐算法给出的预测评分，T是测试集，那么：RMSE = sqrt(Σu,i∈T(rui-r^ui)2 / |T|)MAE = Σu,i∈T|rui-r^ui| / |T| 令R(u)是用户u在训练集上的推荐结果，T(u)是用户u在测试集上的行为结果，U是用户集合，那么： Precision = Σu∈U|R(u) ∩ T(u)| / Σu∈U|R(u)| Recall = Σu∈U|R(u) ∩ T(u)| / Σu∈U|T(u)| 覆盖率：描述一个推荐系统对物品长尾的发掘能力。 假设用户集合为U，物品集合为I，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，那么：Coverage = |∪u∈UR(u)| / |I| 多样性：为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域。 新颖性：是指给用户推荐那些他们以前没听说过的商品。 惊喜度(serendipity)：如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高。 信任度：提高信任度的方法是给出合理的推荐解释。 实时性：推荐系统需要实时地更新推荐列表来满足用户新的行为变化，并且需要能够将新加入系统的物品推荐给用户。 健壮性(robust)：衡量一个推荐系统抗击作弊的能力。 在众多指标中，作者认为：对于可以离线优化的指标，应该在给定覆盖率、多样性、新颖性等限制条件下，尽量优化预测准确度。 常见推荐系统算法推荐系统是联系用户和物品的媒介，而推荐联系用户和物品的方式主要有3种，如下图所示。 3种联系用户和物品的推荐系统 第一种方法，首先找到用户喜欢的物品，然后找到与这些物品相似的物品推荐给用户。基于这种方法可以给出如下的推荐解释：购买了该商品的用户也经常购买这些商品。这种方法通常被称为基于物品的协同过滤算法(item-based collaborative filtering)。第二种方法，首先找到和用户有相似兴趣的其他用户，然后推荐这些其他用户喜欢的物品。这种方法通常被称为基于用户的协同过滤算法(user-based collaborative filtering)。第三种方法，首先找到用户感兴趣的物品特征，然后推荐包含这些特征的物品。这种方法核心思想是通过隐含特征联系用户兴趣和物品，通常被称为隐语义模型算法(latent factor model)。 协同过滤算法个性化推荐系统的一个重要算法是基于用户行为分析，学术界一般将这种类型的算法称为协同过滤算法(collaborative filtering)。 顾名思义，协同过滤就是指用户可以齐心协力，通过不断地和网站互动，使自己的推荐列表能够不断过滤掉自己不感兴趣的物品，从而越来越满足自己的需求。 基于物品的协同过滤算法基于物品的协同过滤算法(以下简称ItemCF)，是目前业界应用最多的算法，最早由电子商务公司亚马逊提出。ItemCF算法给用户推荐那些和他们之前喜欢的物品相似的物品，它的主要步骤分为两步。 (1) 计算物品之间的相似度 (2) 根据物品的相似度和用户的历史行为给用户生成推荐列表 第一步计算相似度可用余弦相似度公式 令N(i)是喜欢物品i的用户集合，那么物品i和物品j的相似度可定义为： wij = |N(i) ∩ N(j)| / sqrt(|N(i)||N(j)|) 第二步计算用户对物品的兴趣，如下公式的含义是：和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。 令puj为用户u对物品j的兴趣，wji是物品j和物品i的相似度，rui是用户u对物品i的兴趣（对于隐反馈数据集，如果用户u对物品i有过行为，可简单令rui=1），S(j,K)是和物品j最相似的K个物品的集合，那么： puj = Σi∈N(u)∩S(j,K) wjirui 最后选取该用户兴趣值最高的N的物品作为推荐列表。 基于用户的协同过滤算法基于用户的协同过滤算法(以下简称UserCF)，是推荐系统中最古老的算法。UserCF算法先找到和他有相似兴趣的其他用户，然后把那些用户喜欢的、而他没有听说过的物品推荐给他，它的主要步骤分为两步。 (1) 找到和目标用户兴趣相似的用户集合 (2) 找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户 第一步计算用户的兴趣相似度可用余弦相似度公式 令N(u)是用户u曾经有过正反馈的物品集合，那么用户u和用户v的相似度可定义为： wuv = |N(u) ∩ N(v)| / sqrt(|N(u)||N(v)|) 第二步计算用户对物品的兴趣 令pui为用户u对物品i的兴趣，wuv是用户u和用户v的相似度，rvi是用户v对物品i的兴趣（对于隐反馈数据集，如果用户v对物品i有过行为，可简单令rvi=1），S(u,K)是和用户u兴趣最相似的K个用户的集合，那么： pui = Σv∈N(i)∩S(u,K) wuvrvi 最后选取该用户兴趣值最高的N的物品作为推荐列表。 隐语义模型隐语义模型算法(以下简称LFM)，是最近几年推荐系统领域最为热门的研究话题。LFM算法的核心思想是通过隐含特征联系用户兴趣和物品，它的主要步骤分为三步。 (1) 对物品进行分类 (2) 确定用户对哪些类的物品感兴趣以及感兴趣的程度 (3) 对于给定的类，确定物品在这个类的权重，并且选择性地推荐给用户 关于如何给物品分类，一个简单方案是由编辑来手动分类，但这样存在很强的主观性和较大的工作量。为了解决这个困难，研究人员提出可以从用户数据出发，基于隐含语义分析技术(latent variable analysis)自动找到哪些类，然后进行个性化推荐。隐含语义分析技术有很多著名的模型和方法，比如pLSA、LDA、隐含类别模型、隐含主题模型、矩阵分解等。 LFM通过如下公式计算用户u对物品i的兴趣： Preferenceui = Σk∈[1,K] pu,kqi,k 其中pu,k度量了用户u的兴趣和第k个隐类的关系，而qi,k度量了第k个隐类和物品i的关系。这两个参数的计算需要一点最优化理论或者机器学习的知识，这里不多作介绍。 三种算法的优缺点比较 LFM是一种基于机器学习的算法，有较好的理论基础。ItemCF/UserCF是基于邻域的方法，更多的是一种基于统计的方法，没有学习过程。 假设有M个用户和N个物品，选取F个隐类。UserCF需要存储用户的相似度矩阵，存储空间是O(M*M)。ItemCF需要存储物品的相似度矩阵，存储空间是O(N*N)。LFM需要的存储空间是O(F*(M+N))。如果用户数很多，UserCF将会占据很大的内存。如果物品数很多，ItemCF将会占据很大的内存。LFM存储空间最少，这在M和N很大时可以很好地节省离线计算的内存。 假设有M个用户和N个物品和K条用户对物品的行为记录。那么，UserCF计算用户表的时间复杂度是O(N*(K/N)2)，而ItemCF计算物品表的时间复杂度是O(M*(K/M)2)。而对于LFM，如果用F个隐类，迭代S次，那么它的时间复杂度是O(K*F*S)。在一般情况下，LFM的时间复杂度要稍微高于UserCF和ItemCF，主要是因为该算法需要多次迭代。 ItemCF算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果，但LFM无法提供这样的解释。 总结在互联网应用中可以看到大量推荐系统的应用，它主要解决了信息过载的问题，通过算法主动帮助用户找到自己感兴趣的内容。常见的推荐系统算法有三种，分别代表三种联系用户和物品的方式，它们是：基于物品的协同过滤算法(ItemCF)，基于用户的协同过滤算法(ItemCF)，隐语义模型算法(LFM)。三种方法各有优劣，需要根据实际场景选择合适的算法，通过不断优化指标找到最优算法。","tags":[{"name":"推荐系统","slug":"推荐系统","permalink":"http://www.secondplayer.top/tags/推荐系统/"},{"name":"读书笔记","slug":"读书笔记","permalink":"http://www.secondplayer.top/tags/读书笔记/"}]},{"title":"使用redis的有序集合实现排行榜功能","date":"2017-07-23T05:43:36.000Z","path":"2017/07/23/redis-sorted-set/","text":"排行榜是业务开发中常见的一个场景，如何设计一个好的数据结构能够满足高效实时的查询，下面我们结合一个实际例子来讨论一下。 场景选手报名参加活动，观众可以对选手进行投票，每个观众对同一名选手只能投一票，活动期间最多投四票。后台需要提供如下接口： 接口1：返回TOP 10的选手信息及投票数 接口2：返回活动总参与选手数及总投票数 接口3：对于每个选手，返回自己的投票数，排名，距离上一名差的票数 基于数据库的方案首先需要一张表存储投票记录，一次投票就是一条记录。这张表相当于投票明细，判断每人只投一张票以及最多投四张表都依赖对这张表的查询。如果直接对这张表做TOP 10的查询，则需要根据选手id做聚合查询，这样每次查询必然耗时。为了优化查询，可以增加另一张排行榜表，用一个定时任务每隔一段时间对原表做聚合查询，然后将结果写进排行榜表里，表里包含投票数及排名的字段，这样查询TOP 10和排名的时候直接查这张表。引入另一张表加快了性能，但牺牲了实时性，活动说明里需加上类似“榜单数据每10分钟同步一次”的话来告知用户。 基于redis的方案对于排行榜的需求，redis有一个数据结构非常适合做这件事，那就是有序集合(sorted set)。 redis的有序集合相关命令有序集合和集合一样可以存储字符串，另外有序集合的成员可以关联一个分数(score)，这个分数用于集合排序。下面以投票为例说明常见的命令，vote_activity是有序集合的key。12345678910111213141516171819202122232425262728#给Alice投票redis&gt; zincrby vote_activity 1 Alice&quot;1&quot; #给Bob投票redis&gt; zincrby vote_activity 1 Bob&quot;1&quot;#给Alice投票redis&gt; zincrby vote_activity 1 Alice&quot;2&quot;#查看Alice投票数redis&gt; zscore vote_activity Alice&quot;2&quot;#获取Alice排名(从高到低，zero-based)redis&gt; zrevrank vote_activity Alice(integer) 0#获取前10名(从高到低)redis&gt; zrevrange vote_activity 0 91) &quot;Alice&quot;2) &quot;Bob&quot;#获取前10名及对应的分数(从高到低)redis&gt; zrevrange vote_activity 0 9 withscores1) &quot;Alice&quot;2) &quot;2&quot;3) &quot;Bob&quot;4) &quot;1&quot;#获取总参与选手数redis&gt; zcard vote_activity(integer) 2 接口实现回到最开始的场景，大部分需求都已经得到满足，还剩下两个数据需要单独说一下。接口2中的总投票数没有直接的接口获得，一种方法是先用ZRANGE遍历所有的key，然后对score进行求和，另一种方法是对总票数单独用一个数据结构存储。接口3的距离上一名差的票数，先用ZREVRANK获取自己排名，然后用ZREVRANGE获取上一排名的分数，最后用自己的分数减去上一名的分数即可，代码示例如下：12345678def get_next_step(redis_key, member): next_step = None score = redis.zscore(redis_key, member) rank = redis.zrevrank(redis_key, member) if rank &gt; 0: next_member = redis.zrevrange(redis_key, rank - 1, rank - 1, withscores=True) next_step = next_member[0][1] - score return next_step 另外如果两个key的score相同，排序逻辑是按照key的字母序排序。在有些情况下这个可能不满足实际要求，因此需要按实际情况重新设计key。比如如果要求同分数情况下按时间排序，那么key最好加上时间戳前缀。 redis与数据库的同步redis通常是作为缓存层加速查询的，如果数据没有做持久化则有概率会丢失数据。一个方案是用定时任务定时同步redis与数据库的数据，数据库里存储着原始数据，通过计算数据库的数据和redis做对比，可以修正由于redis不稳定导致的数据不一致。这里需要注意的是在同步过程时redis的数据有可能还在增长，因此最好先读redis的数据，然后记下时间，查询指定时间段里的数据库的数据，最后再用ZINCRBY增量修正redis数据，而不是直接用ZADD覆盖redis数据。 总结redis的有序集合是一个非常高效的数据结构，可以替代数据库里一些很难实现的操作。它的一个典型应用场景就是排行榜，通过ZRANK可以快速得到用户的排名，通过ZRANGE可以快速得到TOP N的用户列表，它们的复杂度都是O(log(N))，用来替代数据库查询可以大大提升性能。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://www.secondplayer.top/tags/Redis/"}]},{"title":"使用redis实现分布式锁","date":"2017-07-16T05:40:10.000Z","path":"2017/07/16/redis-distribution-lock/","text":"背景在类似秒杀这样的并发场景下，为了确保同一时刻只能允许一个用户访问资源，需要利用加锁的机制控制资源的访问权。如果服务只在单台机器上运行，可以简单地用一个内存变量进行控制。而在多台机器的系统上，则需要用分布式锁的机制进行并发控制。基于redis的一些特性，利用redis可以既方便又高效地模拟锁的实现。 一个简单方案让我们先从一个简单的实现说起，这里用到了redis的两个命令，SETNX和EXPIRE。如果lock_key不存在，那么就设置lock_key的值为1，并且设置过期时间；如果lock_key存在，说明已经有人在使用这把锁，访问失败。12345def acquire_lock(lock_key, expire_timeout=60): if redis.setnx(lock_key, 1): redis.expire(lock_key, expire_timeout) return True return False 逻辑上看似乎没有问题，但是考虑一下异常情况：如果setnx设置成功，但expire由于某些原因（比如超时）操作失败，那么这把锁就永远存在了，也就是所谓的死锁，后面的人永远无法访问这个资源。 利用时间戳取值的方案为了解决死锁，我们可以利用setnx的value来做文章。上例中的我们设的value是1，其实并没有派上用场。因此可以考虑将value设为当前时间加上expire_timeout，当setnx设置失败后，我们去读lock_key的value，并且和当前时间作比对，如果当前时间大于value，那么资源理当被释放。代码示例如下：123456789def acquire_lock(lock_key, expire_timeout=60): expire_time = int(time.time()) + expire_timeout if redis.setnx(lock_key, expire_time): redis.expire(lock_key, expire_timeout) return True redis_value = redis.get(lock_key) if redis_value and int(time.time()) &gt; int(redis_value): redis.delete(lock_key) return False 然而仔细推敲下这段代码仍然能发现一些问题。第一，这个方案依赖时间，如果在分布式系统中的时间没有同步，则会对方案产生一定偏差。第二，假设C1和C2都没拿到锁，它们都去读value并对比时间，在竞态条件(race condition)下可能产生如下的时序：C1删除lock_key，C1获得锁，C2删除lock_key，C2获得锁。这样C1和C2同时拿到了锁，显然是不对的。 改进后的方案幸运的是，redis里还有一个指令可以帮助我们解决这个问题。GETSET指令在set新值的同时会返回老的值，这样的话我们可以检查返回的值，如果该值和之前读出来的值相同，那么这次操作有效，反之则无效。代码示例如下：123456789101112def acquire_lock(lock_key, expire_timeout=60): expire_time = int(time.time()) + expire_timeout if redis.setnx(lock_key, expire_time): redis.expire(lock_key, expire_timeout) return True redis_value = redis.get(lock_key) if redis_value and int(time.time()) &gt; int(redis_value): expire_time = int(time.time()) + expire_timeout old_value = redis.getset(lock_key, expire_time) if int(old_value) == int(redis_value): return True return False 这个方案基本可以满足要求，除了有一个小瑕疵，由于getset会去修改value，在竞态条件下可能会被修改多次导致timeout有细微的误差，但这个对结果影响不大。 最终方案以上方案实现起来略显繁琐，但从redis 2.6.12版本开始有一个更为简便的方法。我们可以使用SET指令的扩展 SET key value [EX seconds] [PX milliseconds] [NX|XX] ，这个指令相当于对SETNX和EXPIRES进行了合并，因而我们的算法可以简化为如下一行：123def acquire_lock(lock_key, expire_timeout=60): ret = redis.set(lock_key, int(time.time()), nx=True, ex=expire_timeout): return ret 总结在redis 2.6.12版本之后我们可以用一个简单的SET命令实现分布式锁，而在此版本之前则需要将SETNX和GETSET配合使用一个较为繁琐的方案。简化后的方案对于开发者来说当然是好事，但通过学习这一演变过程我们会对问题有更深刻的印象。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://www.secondplayer.top/tags/Redis/"}]},{"title":"使用Hexo搭建个人静态博客","date":"2016-06-12T06:42:23.000Z","path":"2016/06/12/hexo-blog-setup/","text":"最近有时间折腾了一下建一个个人博客，在对比了几家之后，最终决定用Hexo作为框架，GitHub作为部署平台搭建博客。VPS选用的是AWS，新用户可以免费使用1年的EC2，足够用来体验了。 申请VPSVPS指的是虚拟服务器，国内推荐用阿里云，国外推荐用Linode, Digital Ocean, AWS。我选择的是AWS，主要有几个原因，一是因为新用户可以试用免费1年，二是因为公司用的就是AWS，对其各项操作比较熟悉，最后一个原因是选择一个国外服务器可以自己搭建ShadowSocks科学上网。 申请域名域名申请服务商，国内有万网，美橙，国外有GoDaddy, NameCheap。我选择的是NameCheap，主要因为价格因素。你要问国内的那些更便宜为啥不选？呵呵国内的情况你懂的。 DNS解析DNS解析推荐DNSPOD，业界良心，服务免费且强大。域名绑定前记得先到NameCheap控制台设置DNS解析到DNSPOD提供的两个免费DNS解析服务器，具体参考这里。 Hexo安装及配置前面把主机和域名搞定了，现在开始在主机上搭建博客了。提到博客，一般都会选用经典的WordPress搭建。不过现在越来越多的个人博客都采用静态博客框架，典型的如Hexo， Jekyll， Octopress。从流行度和技术栈的角度来看，我倾向于选择Hexo。Hexo是一个用Node.js搭建的博客框架，简单强大易上手。静态文件用Markdown编写，Hexo会根据静态文件自动生成网页。 安装依赖环境 安装Git 1$ sudo apt-get install git 安装Node.js 通常用nvm(Node.js Version Manager)安装Node环境安装必要环境1$ sudo apt-get install build-essential libssl-dev 下载nvm1$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh 查看可用版本1$ nvm ls-remote 选取最新版本，这里我们安装v4.4.5，并将其设为默认123$ nvm install 4.4.5$ nvm alias default 4.4.5$ nvm use default 安装Hexo 我们通过npm分别安装hexo客户端和服务端 12$ npm install -g hexo-cli$ npm install -g hexo-server 生成文章 初始化hexo环境 我们把hexo_blog作为博客目录名，首先初始化hexo 123$ hexo init ~/hexo_blog$ cd ~/hexo_blog$ npm install 修改配置文件_config.yml 配置Site, URL, Directory, Writing等基本信息，详细参考这篇配置文档这里建议设置default_layout为draft，这样默认生成文章在Draft里，确认后再发布到Public。 发布文章 新建文章，以名称first_post为例 1$ hexo new first-post 编辑文章，文章都存放在source目录下 1$ vim ~/hexo_blog/source/_drafts/first-post.md 发布文章，这将会把文章从draft移到post目录 1$ hexo publish first-post 运行服务 启动服务器，默认起在4000端口，成功后访问http://localhost:4000 预览效果 1$ hexo server 部署博客我们需要选择一个静态文件的托管平台，首选GitHub，国内可以考虑Coding（最近收购了GitCafe）。 创建GitHub Repository 参考这个步骤，创建一个名为hexo_static的repo，注意设置为Public 修改配置文件_config.yml，注意替换$username 1234deploy: type: git repo: https://github.com/$username/hexo_static.git branch: master 安装hexo git插件 1$ npm install hexo-deployer-git --save 部署 12$ hexo generate$ hexo deploy 按照提示输入用户名和密码，一切步骤完成后，所有文件都已生成并提交到Git上了 自动化整个自动化的思路是：运行该脚本，生成博客静态文件，通过hexo deploy实现自动提交到Git，然后通过本地更新代码，对关联的空分支进行git push操作，触发post-receive钩子，从而将静态文件同步到/var/www/hexo目录，而该目录正是Nginx将80端口转发到本地的路径。 初始化空仓库 1$ git init --bare ~/hexo_bare 创建git hooks 1$ vim ~/hexo_bare/hooks/post-receive 这里我们用到了post-receive这个钩子，当一个本地仓库执行git push后会触发。post-receive具体内容为 123#!/bin/bashgit --work-tree=/var/www/hexo --git-dir=/home/$USER/hexo_bare checkout -f 将空仓库关联到主仓库 123$ git clone https://github.com/$username/hexo_static.git ~/hexo_static $ cd ~/hexo_static$ git remote add live ~/hexo_bare 创建自动化脚本 1$ vim ~/hexo_blog/hexo_git_deploy.sh 脚本内容为 1234567#!/bin/bashhexo cleanhexo generate hexo deploy( cd ~/hexo_static ; git pull ; git push live master) Nginx配置 创建/var/www/hexo目录，稍后会将Nginx的请求映射到该目录 123$ sudo mkdir -p /var/www/hexo$ sudo chown -R $USER:$USER /var/www/hexo$ sudo chmod -R 755 /var/www/hexo 编辑/etc/nginx/sites-available/default1$ sudo vim /etc/nginx/sites-available/default 配置Nginx将80端口的请求映射到/var/www/hexo目录下123456server &#123; listen 80 default_server; listen [::]:80 default_server ipv6only=on; root /var/www/hexo; index index.html index.htm;... 重启Nginx 1$ sudo service nginx restart 发布流程至此，我们可以总结下今后发布文章或更新博客的流程 1234$ hexo new my-post$ vim ~/hexo_blog/source/_draft/my-post.md$ hexo publish my-post$ hexo generate 接着运行hexo server，然后在http://localhost:4000 上预览效果，如果不满意则继续修改my-post.md（此时在_post目录下），重新生成文件（hexo generate），再预览直到可以发布为止 而最终对外发布，我们只需要敲下一行命令就完成了 1$ ~/hexo_blog/hexo_git_deploy.sh 其他 主题 默认hexo的主题是landscape，如果你想与众不同的话，可以用下别的主题或者自定义主题。官方收录的请点击这里，我选择的是indigo，主要看中的是他的Material Design风格 评论 常见的评论系统有Disqus，多说，友言等，我选择的是多说。接入非常简单，去网站上注册个账号，然后将示例代码插到网页中即可 统计 流量统计选择cnzz(现已被整合进Umeng+) 监控 可以接入监控宝 参考 Hexo Documentation How to Create a Blog with Hexo On Ubuntu 14.04","tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://www.secondplayer.top/tags/Hexo/"}]},{"title":"Hello World","date":"2016-06-10T16:00:00.000Z","path":"2016/06/11/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]