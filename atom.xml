<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SecondPlayer&#39;s Blog</title>
  <subtitle>我读书多，你别骗我</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.secondplayer.top/"/>
  <updated>2018-11-17T14:27:19.539Z</updated>
  <id>http://www.secondplayer.top/</id>
  
  <author>
    <name>secondplayer</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习笔记18: 微分动态规划</title>
    <link href="http://www.secondplayer.top/2018/11/17/machine-learning-ddp/"/>
    <id>http://www.secondplayer.top/2018/11/17/machine-learning-ddp/</id>
    <published>2018-11-17T14:27:19.000Z</published>
    <updated>2018-11-17T14:27:19.539Z</updated>
    
    <content type="html"><![CDATA[<p>上一节中我们介绍了一个特殊的MDP模型：线性二次型调节控制(LQR)。事实上很多问题都可以用LQR来解决，即使动态模型是非线性的。尽管LQR是一个非常漂亮的解决方案，但它还不够通用。我们以<strong>倒摆</strong>(inverted pendulum)问题为例来引出这一节的主题。倒摆问题的状态转换过程为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7412eae0ba706386.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中函数F取决于角度的余弦值，显然这是一个非线性函数。那么我们的问题是：如何将该系统线性化处理？</p>
<h1 id="动态模型的线性化"><a href="#动态模型的线性化" class="headerlink" title="动态模型的线性化"></a>动态模型的线性化</h1><p>假设在t时刻，系统大部分时间都处于状态s<sup>-</sup><sub>t</sub>，并且采取的行动在a<sup>-</sup><sub>t</sub>附近。对于倒摆问题来说，当系统到达最优解时，很显然系统采取的行动范围很小，且不会偏离竖直方向太远。</p>
<p>我们将会使用<strong>泰勒展开</strong>(Taylor expansion)的方法将动态模型线性化。先考虑一个简单的场景，当状态是一维的并且状态转换函数F与行动无关，我们可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f433ab853e954878.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>更一般地，当F是关于状态和行动的函数时，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e701e72ec8c2481f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所以s<sub>t + 1</sub>是关于s<sub>t</sub>和a<sub>t</sub>的线性函数，这是因为我们可以把上式改写成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-bcf4d442d506f3d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中κ是一个常数，A和B是矩阵。这种形式和我们之前在LQR中做的假设非常相似。</p>
<h1 id="微分动态规划"><a href="#微分动态规划" class="headerlink" title="微分动态规划"></a>微分动态规划</h1><p>当我们的目标是停留在某个状态s<sup>*</sup>附近时，上面介绍的方法可以很有效地解决。然而在某些情况下，目标会变得更为复杂。</p>
<p>假设我们的目标是遵循某条<strong>轨迹</strong>(trajectory)，那么就需要用到<strong>微分动态规划</strong>(Differential Dynamic Programming (DDP))的方法了。这个方法会将轨迹按照时间片进行离散化，并为每个时间片建立中间目标。DDP的主要步骤如下：</p>
<p><strong>步骤1</strong>: 选定<strong>标称轨迹</strong>(nominal trajectory)，这个是对我们想要追随的轨迹的近似。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-8ef24705f5279770.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p><strong>步骤2</strong>: 对每一个在s<sup>*</sup><sub>t</sub>附近的轨迹线性化，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-10dc2077f404367a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中s<sub>t</sub>和a<sub>t</sub>是我们当前的状态和行动。既然我们现在有了当前点的线性近似，我们可以利用上一小节的结论将下一个状态改写成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-746e8e4088a57b6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意：我们也可以对奖励函数R<sup>(t)</sup>利用泰勒展开作类似的推导：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7504c52932c80031.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中H<sub>xy</sub>指的是R关于x和y在(s<sub>t</sub><sup>*</sup>, a<sub>t</sub><sup>*</sup>)处的Hessian矩阵。这个式子也可以被写成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a7caa1e44a8e32f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中U<sub>t</sub>和W<sub>t</sub>是矩阵的形式。</p>
<p><strong>步骤3</strong>: 现在我们证明了这个问题已经被转化成了LQR问题了，因此我们只需要用LQR的方法求解出最优策略π<sub>t</sub>。</p>
<p><strong>步骤4</strong>: 现在我们有了最优策略π<sub>t</sub>，我们用它来产生新的轨迹：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-94bdb1519f54fd29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意当我们生成新的轨迹时，我们应该用F而不是近似函数来计算下一个状态，也就是说：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-85519346104d81d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>计算出下一个状态后，我们再回到步骤2，不断重复这几步，直到收敛到某个值结束。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>LQR只适用于状态转换函数是线性的场景，当状态转换函数是非线性时，我们可以使用泰勒展开的方法做线性近似</li>
<li>当大部分状态和行动在某个小的局部范围内，我们可以选择局部中心做线性近似</li>
<li>当状态转换函数遵循某条轨迹时，可以使用微分动态规划(DDP)算法，其思想是在状态转换函数的多个点上依次做线性近似</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes13.pdf" target="_blank" rel="external">LQR, DDP and LQG</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/3/5/M6SGF6VB4_M6SGL2R35.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一节中我们介绍了一个特殊的MDP模型：线性二次型调节控制(LQR)。事实上很多问题都可以用LQR来解决，即使动态模型是非线性的。尽管LQR是一个非常漂亮的解决方案，但它还不够通用。我们以&lt;strong&gt;倒摆&lt;/strong&gt;(inverted pendulum)问题为例来
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记17: 线性二次型调节控制</title>
    <link href="http://www.secondplayer.top/2018/10/28/machine-learning-lqr/"/>
    <id>http://www.secondplayer.top/2018/10/28/machine-learning-lqr/</id>
    <published>2018-10-28T12:40:06.000Z</published>
    <updated>2018-10-28T12:54:57.826Z</updated>
    
    <content type="html"><![CDATA[<h2 id="有限边界的MDP"><a href="#有限边界的MDP" class="headerlink" title="有限边界的MDP"></a>有限边界的MDP</h2><p>在前面两章关于强化学习的介绍中，我们定义了马尔可夫决策过程(MDP)以及价值迭代/策略迭代这两种用于求解MDP的算法。特别地，我们介绍了<strong>最优贝尔曼方程</strong>(optimal Bellman equation)，它定义了在最佳策略π<sup>*</sup>下的最佳价值函数V<sup>π<sup>*</sup></sup>：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2553b7a070a9952a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在计算出最佳价值函数后，我们可以用下式求出最佳策略π<sup>*</sup>：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9d5ca0b87c96c007.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在这一章中，我们会将此扩展到一个更通用的情况下：</p>
<p>(1). 我们需要将等式改写成同时满足离散和连续两种情况，因此我们将原来式子中求和的部分改写为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5ce6af1d0312e8e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这代表着我们取的是价值函数在下一个状态下的期望值。在离散的情况下，期望值可以改写成状态的求和；在连续的情况下，期望值可以改写成状态的积分。符号s′ ~ P<sub>sa</sub>表示下一个状态s′ 以P<sub>sa</sub>的概率取样得到。</p>
<p>(2). 我们假设奖励函数同时取决于状态和行动，简而言之就是，R: S × A → R。因此原来式子需要改写为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7dee06b0acb3be44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>(3). 原来的MDP是没有<strong>时间边界</strong>(time horizon)的，现在我们假设有时间边界T，并定义<strong>有限边界的MDP</strong>(finite horizon MDP)由如下五元组构成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3316384295e9724b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在这种设定下，整个过程的奖励定义为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d5974a1862ff6433.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个式子替代了之前的定义：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e21fa8dfaaefd0be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在有限边界的MDP中，折扣因子γ不再出现。回顾下在无限边界的情况下，引入γ是为了让在无限状态的奖励函数是有界的，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a376274f21df16ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>而在有限边界的情况下，整个奖励函数是有限个奖励函数之和，因此γ没有存在的必要了。</p>
<p>在有限边界的设定下，MDP的有些行为会有不一样的变化。最重要的变化是，最佳策略π<sup>*</sup>是<strong>非稳定的</strong>(non-stationary)，也就是说它是随着时间变化的，用数学的表达就是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-799eb97a5527108e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这里的上标t表明当前策略在t时刻。具体来说就是，首先我们处于初始状态s<sub>0</sub>，然后根据时刻0的策略采取行动a<sub>0</sub> := π<sup>(0)</sup>s<sub>0</sub>，随后状态按照P<sub>s<sub>0</sub>a<sub>0</sub></sub>的概率转移到下一个状态s<sub>1</sub>，然后根据时刻1的策略采取行动a<sub>1</sub> := π<sup>(1)</sup>s<sub>1</sub>，以此类推。</p>
<p>为什么说最佳策略π<sup>*</sup>是非稳定的呢？直觉上来说，我们只能采取有限的行动，我们使用的策略取决于当前的环境以及我们还剩下多少时间。我们可以考虑如下的场景：在地图中分别有两个奖励+1和+10。起初我们的策略是选择接近+10的奖励，但是随着时间的推移，由于我们没有足够的步数到达+10，这时就迫使我们采取到达离我们更近的+1奖励。</p>
<p>(4). 状态转换概率也随着时间而变化，即在时刻t下的状态转换概率为P<sup>(t)</sup><sub>s<sub>t</sub>a<sub>t</sub></sub>。同样，奖励函数R<sup>(t)</sup>也随着时间而变化。</p>
<p>将上述几点统一起来，有限边界的MDP由如下五元组构成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-da1883dd05b9c55c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>策略π在时刻t的价值函数定义和之前一样，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d99a7d185dd8ae71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>那么现在的问题是，在有限边界的设定下，我们如何求得最优价值函数呢？</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-943dab964099bfd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="最优价值函数" title="">
                </div>
                <div class="image-caption">最优价值函数</div>
            </figure>
<p>在标准的价值迭代算法中，我们在每次迭代中使用了贝尔曼方程(题外话：贝尔曼是动态规划的创始人，贝尔曼方程在动态规划领域密切相关)。而在有限边界的MDP问题中，我们也采取基于迭代的算法，为了更好的进行说明，我们先看如下两个观察：</p>
<p>(1). 在最后时刻T，最优价值函数显然为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d014dc2be9e95263.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>(2). 在其他时刻 0 ≤ t &lt; T，如果我们知道下一时刻的最优价值函数V<sup>*</sup><sub>t+1</sub>，那么我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f39f4b7cd37c48a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>有了上面两个观察，我们可以得到求解有限边界MDP的最优价值函数的算法：</p>
<p>(1). 根据等式(1)求得V<sup>*</sup><sub>T</sub><br>(2). 对每一个t = T - 1, …, 1, 0:<br>    根据等式(2)和V<sup>*</sup><sub>t + 1</sub>计算出V<sup>*</sup><sub>t</sub></p>
<h2 id="线性二次型调节控制"><a href="#线性二次型调节控制" class="headerlink" title="线性二次型调节控制"></a>线性二次型调节控制</h2><p>这一节我们介绍一个特殊的有限边界MDP的模型，它被称为<strong>线性二次型调节控制</strong>(Linear Quadratic Regulation (LQR))，这个模型广泛应用于<strong>机器人学</strong>(robotics)中。</p>
<p>首先我们明确下这个模型做了哪些假设。这个模型的状态和行动是连续的，并且：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c30f9b9e796efd3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>状态转换函数是线性的，且有噪音，表示如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-67174dbdd6000732.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中A<sub>t</sub> ∈ R<sup>n×n</sup>，B<sub>t</sub> ∈ R<sup>n×d</sup>是两个矩阵，w<sub>t</sub> ~ N(0, Σ<sub>t</sub>)是高斯分布的噪音。在后面的段落中我们会看到只要噪音的高斯分布的均值为0，噪音不会影响到最优价值的计算。</p>
<p>我们继续假设奖励函数是二次函数，表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2bee1366e899af11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中U<sub>t</sub> ∈ R<sup>n×n</sup>，W<sub>t</sub> ∈ R<sup>d×d</sup>是两个半正定矩阵，也就是说奖励函数的值永远是负数。</p>
<p>现在我们已经完整定义了LQR模型的所有假设了，下面是求解LQR算法的两个步骤：</p>
<p>第一步：假设我们不知道A, B, Σ这三个矩阵。为了预估这三个参数，我们可以参考在价值迭代那节中用模型学习的方法。首先我们执行任意策略，并收集数据。然后用线性回归的方法求得A和B，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-affdad28602b273c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>最后用高斯判别分析(GDA)模型中提到的方法求出Σ。</p>
<p>第二步：假设A, B, Σ三个参数已知，我们可以通过动态规划求出最优价值函数。换句话说就是，给定：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7ea8fd72683dc37f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>的情况下，我们想要计算V<sup>*</sup><sub>t</sub>，我们用上一小节介绍的动态规划算法求解，具体来说就是：</p>
<p>(1). 初始化</p>
<p>对于最后时刻T，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c893e01e0aca940e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>(2). 不断循环</p>
<p>令 t &lt; T且假设我们已知V<sup>*</sup><sub>t + 1</sub>。</p>
<p><strong>事实1：</strong>可以证明如果V<sup>*</sup><sub>t + 1</sub>是关于s<sub>t</sub>的二次函数，那么V<sup>*</sup><sub>t + 1</sub>也是一个二次函数。或者可以表述为存在某个矩阵Φ和某个标量Ψ使得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6d5237ff020f9e18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>当t = T时，我们有Φ<sub>t</sub> = -U<sub>T</sub>，Ψ<sub>t</sub> = 0。</p>
<p><strong>事实2：</strong>可以证明最优价值函数是关于状态的线性函数。</p>
<p>如果我们知道了V<sup>*</sup><sub>t + 1</sub>，就等价于知道了Φ<sub>t + 1</sub>和Ψ<sub>t + 1</sub>，所以我们只需要解释如何根据Φ<sub>t + 1</sub>和Ψ<sub>t + 1</sub>以及其他参数求出Φ<sub>t</sub>和Ψ<sub>t</sub>就行了。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5d5961f779d342b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中第二行用到了最优价值函数的定义，第三行是将假设中的定义进行了代入。注意表达式最后是关于a<sub>t</sub>的二次参数，因此可以求出最优行动a<sup>*</sup><sub>t</sub>为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-797fd06fc36b363f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9a281f714ccacef3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由此可见，最优策略是与s<sub>t</sub>线性相关的。基于a<sup>*</sup><sub>t</sub>我们可以求出Φ<sub>t</sub>和Ψ<sub>t</sub>的值，它们之间的关系也被称为<strong>离散Ricatti方程</strong>(Discrete Ricatti equations)。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-fcc7021ac599da82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p><strong>事实3：</strong>我们注意到Φ<sub>t</sub>的值与Ψ<sub>t</sub>和Σ<sub>t</sub>都无关。由于L<sub>t</sub>是关于A<sub>t</sub>，B<sub>t</sub>和Φ<sub>t + 1</sub>的函数，这表明最优策略也与噪音无关。</p>
<p>最后总结一下，LQR问题求解的算法步骤如下：</p>
<ol>
<li>如果需要的话，先估计参数A<sub>t</sub>, B<sub>t</sub>, Σ<sub>t</sub></li>
<li>初始化参数Φ<sub>T</sub> := -U<sub>T</sub>，Ψ<sub>T</sub> := 0</li>
<li>对每一个t = T - 1, …, 1, 0，利用Φ<sub>t + 1</sub>，Ψ<sub>t + 1</sub>和离散Ricatti方程求出Φ<sub>t</sub>，Ψ<sub>t</sub>。如果存在某个策略使得状态趋于0，那么迭代一定是收敛的。</li>
</ol>
<p>利用<strong>事实3</strong>，我们可以使这个算法运行地更快一些。由于最优策略不依赖于Ψ<sub>t</sub>，并且Φ<sub>t</sub>的更新只依赖于Φ<sub>t</sub>，因此迭代时只更新Φ<sub>t</sub>就足够了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>在原有MDP的基础上引入了时间边界的概念，这类问题被称为有限边界的MDP，在这种设定下策略和价值函数都是不稳定的，也就是说它们是随着时间变化的</li>
<li>线性二次型调节控制(LQR)是一个特殊的有限边界MDP模型，该模型广泛应用于机器人学中</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes13.pdf" target="_blank" rel="external">LQR, DDP and LQG</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/1/H/M6SGF6VB4_M6SGL3P1H.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;有限边界的MDP&quot;&gt;&lt;a href=&quot;#有限边界的MDP&quot; class=&quot;headerlink&quot; title=&quot;有限边界的MDP&quot;&gt;&lt;/a&gt;有限边界的MDP&lt;/h2&gt;&lt;p&gt;在前面两章关于强化学习的介绍中，我们定义了马尔可夫决策过程(MDP)以及价值迭代/策略迭代这
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记16: 马尔可夫决策过程(下)</title>
    <link href="http://www.secondplayer.top/2018/09/23/machine-learning-mdp-part-two/"/>
    <id>http://www.secondplayer.top/2018/09/23/machine-learning-mdp-part-two/</id>
    <published>2018-09-23T15:21:33.000Z</published>
    <updated>2018-09-23T15:33:23.943Z</updated>
    
    <content type="html"><![CDATA[<p>到目前为止，我们一直都在讨论有限状态下的MDP问题，现在我们来看下当状态数量是无限时如何求解MDP问题。</p>
<h2 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h2><p>也许求解无限状态下的MDP问题最简单的方法就是先将无限状态离散化成有限状态，然后再用之前介绍的价值迭代或者策略迭代算法了。</p>
<p>假设我们有两个状态s<sub>1</sub>和s<sub>2</sub>，我们可以用下图所示的网格来离散化这个状态空间。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b3497fc7dad29d84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>图中的每一个网格都代表独立的离散状态s<sup>*</sup>，因此我们可以把无限状态的MDP近似表示成(S<sup>*</sup>, A, {P<sub>s<sup>*</sup>a</sub>}, γ, R)，其中S<sup>*</sup>是所有离散状态的集合，{P<sub>s<sup>*</sup>a</sub>}是在状态s<sup>*</sup>采取行动a的概率分布。然后我们就可以用价值迭代或者策略迭代算法求出V<sup>*</sup>(s<sup>*</sup>)和π<sup>*</sup>(s<sup>*</sup>)。</p>
<p>离散化的方法可以在很多场景都有很好的应用，但是它也有两个明显的缺点。第一个缺点是离散化只是对连续状态的近似，有时会有很大的误差。</p>
<p>为了更好地理解这一点，考虑如下的监督学习问题：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e869bfdb25ad4170.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果我们用线性回归作拟合，那么拟合效果是很好的。但是如果我们用离散化的方法去作拟合，那么拟合效果就如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9af860dddd7e6296.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>离散化的方法无法精确表示光滑曲线，如果需要降低误差，那么需要将离散化的粒度变得更小。</p>
<p>离散化的第二个缺点被称为<strong>维度的诅咒</strong>(curse of dimensionality)。假设我们把n维状态空间离散化成k份，那么所有离散状态的总数是k<sup>n</sup>个。当n值变大时，所有离散状态的总数呈指数性增长。比如当n=10，k=100时，所有离散状态的总数是100<sup>10</sup> = 10<sup>20</sup>个，这个数字对于现在的计算机来说也是很难处理过来的。</p>
<p>作为一个经验法则，离散化通常对1维或2维状态的问题有较好的效果。如果处理得当，离散化也能很好处理4维状态。在极端情况下，离散化最多能处理到6维状态。一旦维数超过6，那么离散化将很难发挥出作用。</p>
<h2 id="价值函数近似"><a href="#价值函数近似" class="headerlink" title="价值函数近似"></a>价值函数近似</h2><p>现在我们介绍另一种求解无限状态下MDP问题的方法，这次我们来直接估计V<sup>*</sup>。这个方法叫做<strong>价值函数近似</strong>(value function approximation)，在强化学习问题中有着成功的应用。</p>
<h3 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h3><p>在价值函数近似算法中，我们需要训练一个<strong>模型</strong>(model)，也称为<strong>模拟器</strong>(simulator)。简单来说，模拟器就是一个黑箱，它的输入是任意状态s<sub>t</sub>和行动a<sub>t</sub>，输出是根据状态转换概率P<sub>s<sub>t</sub>a<sub>t</sub></sub>得到的下一个状态s<sub>t+1</sub>。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e8fd33bee4371fa1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们可以有多种方法获得这个模型。一种方法是通过物理模拟，比如我们可以通过物理定律和已知参数进行推导，或者使用现成的物理模拟软件进行建模。</p>
<p>另一种获得模型的方法从MDP的训练数据中进行学习。比如我们进行m次MDP的<strong>试验</strong>(trial)，每次试验进行T个时间序列步骤。这样我们就得到了如下m次试验数据：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7ee694793d4cb7b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们可以将s<sub>t+1</sub>看成是一个以s<sub>t</sub>和a<sub>t</sub>为参数的函数，然后通过某个学习算法求得该函数。</p>
<p>比如我们可以选择如下的线性模型：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5dbcd6f8e98048d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过线性回归的算法可以求得模型中的参数，也就是A和B两个矩阵。通过最大似然估计法，可以求得参数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f03858c6cc5c3635.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>求出A和B两个参数后，一种方法是建立一个<strong>确定</strong>(deterministic)模型，也就是通过等式(5)给定参数s<sub>t</sub>和a<sub>t</sub>来唯一确定s<sub>t+1</sub>。另一种方法是建立一个<strong>随机</strong>(stochastic)模型，也就是说s<sub>t+1</sub>是关于输入的一个随机函数，这个模型可以表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ca7372707f029258.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中ε<sub>t</sub>是噪音项，通常来说ε<sub>t</sub> ~ N(0, Σ)。</p>
<p>上面我们假设s<sub>t+1</sub>是关于当前状态和行动的线型函数，但在实际情况中，非线性函数也是有可能的。这时我们可以把模型表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-370a97ea91092d6d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中φ<sub>s</sub>和φ<sub>a</sub>是关于状态和行动的某个非线性函数。另外我们也可以使用非线性学习算法，比如局部加权线性回归算法来估计参数。上述方法在构建MDP的确定模型和随机模型中都适用。</p>
<h3 id="拟合的价值迭代"><a href="#拟合的价值迭代" class="headerlink" title="拟合的价值迭代"></a>拟合的价值迭代</h3><p>现在我们介绍<strong>拟合价值迭代</strong>(fitted value iteration)算法，它同样用于求解无限状态下的MDP问题。这里我们假设连续状态空间S = R<sup>n</sup>，行动空间A规模很小且是离散的。</p>
<p>回顾一下在价值迭代算法中，我们每次都是在更新：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-85236cea0b99221e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意，由于现在状态空间是连续的，所以这里用积分的方式来替代求和。</p>
<p>拟合价值迭代的中心思想是通过某个监督学习算法（这里我们用线性回归）来近似求出价值函数，其中价值函数是关于状态的线性或非线性函数，可以用下式表示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5fcc18af6d214177.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中φ是关于状态的某个映射函数。算法步骤描述如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3f36238000aa28d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>算法的每次循环中，首先每次取样出k个状态，然后计算出y<sup>(i)</sup>，这个值正是对V(s)的近似（等式7的右边）。最后通过应用监督学习算法（线性回归）使得V(s)与y<sup>(i)</sup>尽可能的接近。</p>
<p>和有限状态的价值迭代算法不同，拟合价值迭代并不能保证算法总是收敛的。然而在实际应用中，算法通常是收敛的。注意，如果我们使用上一小节介绍的确定性的模型，那么价值迭代算法可以通过令k=1的方式进行简化。</p>
<p>最后，拟合价值迭代算法输出的是V，这是对V<sup>*</sup>的近似。特别地，当系统处于某个状态s时，我们需要选择一个行动，这个行动a将会是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6a02b9736ad7fec9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>计算这个值的过程和拟合价值迭代算法的内层循环很相似。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>无限状态下MDP问题可以通过两个算法求解：离散化和价值函数近似</li>
<li>离散化通常对2维以下状态的问题有较好的效果，极端情况下最多适用于6维以下状态</li>
<li>价值函数近似又可分为两种：使用模型和拟合价值迭代，其思想在于通过某种方法（物理模拟或算法学习）求得价值函数的一个近似值</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf" target="_blank" rel="external">Reinforcement Learning and Control</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/N/6/M6SGF6VB4_M6SGKVGN6.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;到目前为止，我们一直都在讨论有限状态下的MDP问题，现在我们来看下当状态数量是无限时如何求解MDP问题。&lt;/p&gt;
&lt;h2 id=&quot;离散化&quot;&gt;&lt;a href=&quot;#离散化&quot; class=&quot;headerlink&quot; title=&quot;离散化&quot;&gt;&lt;/a&gt;离散化&lt;/h2&gt;&lt;p&gt;也许求解无限
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记15: 马尔可夫决策过程(上)</title>
    <link href="http://www.secondplayer.top/2018/09/01/machine-learning-mdp-part-one/"/>
    <id>http://www.secondplayer.top/2018/09/01/machine-learning-mdp-part-one/</id>
    <published>2018-09-01T12:32:29.000Z</published>
    <updated>2018-09-01T12:32:29.277Z</updated>
    
    <content type="html"><![CDATA[<p>这一节开始我们介绍<strong>强化学习</strong>(reinforcement learning)。在监督学习中，对于一个给定的输入x，我们可以明确知道输出y。而在很多<strong>序列决策</strong>(sequential decision making)问题中，我们无法确切知道结果。比如我们造了一个机器人并教它如何走路，我们一开始是不知道给它下达哪个“正确”的指令才能让它走起来。</p>
<p>在强化学习问题中，我们会为算法定义一个奖励函数，这个函数用于给学习者一个反馈，然后根据反馈的好坏决定下一步动作。比如在机器人的例子中，我们会给机器人一个正向的奖励如果它向前走了一步，并且会给机器人一个负向的奖励如果它向后走了一步或者跌倒了。强化学习算法需要找到一组最佳行动使得机器人获得最大的奖励。</p>
<p>强化学习算法在诸如直升机自动驾驶、机器人行走、手机网络路由、市场战略决策等领域有着广泛成功的应用。我们将会从<strong>马尔可夫决策过程</strong>(Markov decision processes (MDP))开始学起，它给出了强化学习问题形式化的定义。</p>
<h2 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2><p>马尔可夫决策过程由一个五元组构成，可以表示为(S, A, {P<sub>sa</sub>}, γ, R)，其中：</p>
<ul>
<li>S是状态集合。比如在直升机无人驾驶例子中，S就是直升机可能的位置和方向集合。</li>
<li>A是行动集合。比如在直升机无人驾驶例子中，A就是操控直升机可能的动作集合，比如上下左右等。</li>
<li>P<sub>sa</sub>是状态转换概率。P<sub>sa</sub>指的是在状态s采取行动a的概率分布。</li>
<li>γ ∈ [0, 1)是<strong>折扣因子</strong>(discount factor)。</li>
<li>R: S × A → R是<strong>奖励函数</strong>(reward function)。R表示在状态s采取行动a时能获得的奖励。奖励函数有时只和状态有关，这时我们写成R: S → R。</li>
</ul>
<p>MDP的决策过程如下：首先我们处于初始状态s<sub>0</sub>，然后从行动集合A中选择一个行动a<sub>0</sub>，这样我们的状态按照P<sub>s<sub>0</sub>a<sub>0</sub></sub>的概率转移到下一个状态s<sub>1</sub>。紧接着我们选择下一个行动a<sub>1</sub>，状态按照P<sub>s<sub>1</sub>a<sub>1</sub></sub>的概率转移到下一个状态s<sub>2</sub>。再接着我们选择下一个行动a<sub>2</sub>，以此类推。我们可以把这一过程用下图表示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-66c8b92399359d97.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在遍历所有的状态和行动后，整个过程的奖励为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-8f7d4bf98cfbdaee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果我们把奖励函数当作只和状态相关，那么可以简化为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-09ff03b14e30ddf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们后面会一直使用简化版的奖励函数R(s)，尽管把它扩展到R(s, a)并没有太大的难度。</p>
<p>我们强化学习的目标就是选择一组行动使得整个奖励函数的期望最大化：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1188cdc27a4801bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意奖励的值随着时刻t被γ<sup>t</sup>的因子衰减。因此为了获得最大的奖励，我们需要尽早获得正向的奖励或者推迟获得负向的奖励。在经济学领域中，R表示获得的金钱，γ可以被自然的解释为“利率”（可以解释为今天的美元比明天的值钱）。</p>
<p><strong>策略</strong>(policy)是指任意一个从状态到行动的映射函数π: S → A。当我们在状态s时执行某个策略π时，我们采取的行动a = π(s)。我们定义策略π的<strong>价值函数</strong>(value function)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-61a1229c60601841.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>V<sup>π</sup>(s)表示的是初始状态为s，执行策略为π时总奖励的期望值。</p>
<p>对于一个固定的策略π，其价值函数V<sup>π</sup>满足<strong>贝尔曼方程</strong>(Bellman equations)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e8fd5cf3af3d1c3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式表明了V<sup>π</sup>包含了两部分：第一部分是初始状态下的<strong>立即奖励</strong>(immediate reward)R(s)，第二部分是后续状态下的总奖励。我们更详细地考察下第二部分，这个总奖励可以被写成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-0d0aacb5f5b7c484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中s<sup>′</sup>是初始状态s之后的下一个状态，因此整个第二部分是指初始状态为s<sup>′</sup>下的后续所有奖励之和。</p>
<p>贝尔曼方程可以用来高效地求解V<sup>π</sup>(s)。特别的，对于有限状态的MDP(|S| &lt; ∞)，我们可以为每个状态s写出对应的方程，这样对于|S|个状态就有|S|个方程，通过求解这个方程组就可以就出每个V<sup>π</sup>(s)。</p>
<p>我们的目标是找到最大的价值函数，因此定义<strong>最优价值函数</strong>(optimal value function)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6b483a7f65e42e0e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对于最优价值函数，也有对应版本的贝尔曼方程：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-acf88212edbc49a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们同样定义最优策略π<sup>*</sup>: S → A为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-12b2d2cc17c85e12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>π<sup>*</sup>(s)就是使得等式(2)中获得最优价值函数的那个策略。</p>
<p>对于所有的状态s和所有的策略π，可以很容易地发现它们满足如下不等式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2de4633a04ca87eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意π<sup>*</sup>对于所有的状态s都成立，这意味着不管我们在MDP中选取哪个作为初始状态，最优策略的函数不变。</p>
<h2 id="价值迭代和策略迭代"><a href="#价值迭代和策略迭代" class="headerlink" title="价值迭代和策略迭代"></a>价值迭代和策略迭代</h2><p>现在我们介绍用于求解有限状态的MDP问题的两种算法，现在我们考虑的MDP问题必须是有限状态和有限行动的，即满足|S| &lt; ∞，|A| &lt; ∞。</p>
<p>求解MDP的第一个方法称为<strong>价值迭代</strong>(value iteration)，步骤如下：</p>
<blockquote>
<ol>
<li>对每一个状态s，初始化V(s) := 0</li>
<li>重复如下操作直到收敛：{<br> 对于每个状态s，更新V(s): <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-0d43a8250084666d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure> }</li>
</ol>
</blockquote>
<p>算法的第2步可以看作是不断地用贝尔曼等式去更新价值函数。第2步的循环可以有两种不同的更新方式。第一种方法，我们可以为每个状态s计算V(s)，然后一次性把新的V(s)值替换旧值，这种更新称为是<strong>同步</strong>(synchronous)的。这种情况下，这个算法可以认为是实现了一个<strong>贝尔曼算子</strong>(Bellman backup operator)，贝尔曼算子将当前价值函数映射到更接近最优值的一个估计。另一种更新方法称为<strong>异步</strong>(asynchronous)更新，这种情况下每次计算出V(s)后就立刻进行更新。</p>
<p>不管是同步还是异步的更新，价值迭代都会使得V不断收敛到V<sup>*</sup>，得到V<sup>*</sup>后我们通过等式(3)就能求出最佳策略。</p>
<p>求解MDP的第二个方法称为<strong>策略迭代</strong>(policy iteration)，步骤如下：</p>
<blockquote>
<ol>
<li>随机初始化一个策略π</li>
<li>重复如下操作直到收敛：{<br> (a). 令V := V<sup>π</sup><br> (b). 对于每个状态s，更新π(s): <figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ef01c545b1587b35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure> }</li>
</ol>
</blockquote>
<p>算法的第2步不断地根据当前策略求出价值函数，然后根据当前价值函数更新策略。其中第2步中的(a)可以用之前提到的求解方程组获得。</p>
<p>在算法经过一定次数的迭代后，V会不断收敛到V<sup>*</sup>，π会不断收敛到π<sup>*</sup>。</p>
<p>价值迭代和策略迭代都是用于求解MDP的常规算法，目前没有一个统一的定论说哪个更好。对于规模较小的MDP，策略迭代收敛更快。对于规模较大的MDP，由于求解方程组的开销太大，价值迭代性能更好。因此在实践中，通常选取的算法是价值迭代。</p>
<h2 id="MDP的模型学习"><a href="#MDP的模型学习" class="headerlink" title="MDP的模型学习"></a>MDP的模型学习</h2><p>到目前为止，我们都是在已知状态转换概率和奖励函数的情况下讨论MDP算法。但在实际情况中，通常状态集合、行动集合和折扣因子是已知的，但状态转换概率和奖励函数很可能是未知的，这个时候我们就需要从实际数据中预估它们。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c4395002cdb2407d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>比如上图中描述了若干轮MDP迭代的过程，其中s<sub>i</sub><sup>(j)</sup>表示第j轮实验的i时刻时的状态，a<sub>i</sub><sup>(j)</sup>是对应时刻的行动。在实际情况中，每次实验都需要运行到终止状态为止，或者运行相对比较多的次数为止。</p>
<p>在这若干次实验后，我们可以根据经验认为状态转换概率由下式给出：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-70f3cade109b58b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果这个比率是“0/0”，也就是没有出现状态s和行动a的情况下，我们可以简单地用1/|S|进行估计，也就是所有状态出现的平均值。</p>
<p>如果我们实验运行的次数足够多，分子和分母可以用所有实验中出现次数的累加值，这样算出来的比率更接近状态转换概率的真实情况。</p>
<p>类似的，如果奖励函数R未知，我们可以把R(s)的值用状态s下的平均奖励作近似估计。</p>
<p>在建立了模型参数后，我们就可以用价值迭代或者策略迭代求解MDP问题了。将两者结合起来，下面描述了在状态转换概率未知的情况下求解MDP问题的步骤：</p>
<blockquote>
<ol>
<li>随机初始化一个策略π</li>
<li>重复如下操作直到收敛：{<br> (a). 用策略π进行若干次MDP实验<br> (b). 根据实验结果，估算出P<sub>sa</sub>的值<br> (c). 根据估算出的P<sub>sa</sub>，执行价值迭代得到新的V<br> (d). 根据新得到的V，执行策略迭代中第2步中的(b)，从而更新π<br>}</li>
</ol>
</blockquote>
<p>我们发现对于这个特定的算法，有一个很简单的优化可以使得这个算法运行更快。在算法内层循环中的(c)步，价值迭代默认是把V初始化成0，如果将V初始化成上一轮迭代得到的V值，这样能获得一个更好的迭代起始点，因而收敛地更快。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>强化学习的一个常见模型是马尔可夫决策过程(MDP)，MDP由一个五元组构成，MDP的目标是找到最优价值函数</li>
<li>当MDP是有限状态的情况下，可以用价值迭代或策略迭代两种方法求解。对于规模较小的MDP，策略迭代收敛更快；对于规模较大的MDP，价值迭代性能更好。因此在实践中，通常选取的算法是价值迭代</li>
<li>当MDP的状态转换概率和奖励函数未知的情况下，可以进行多次实验并根据实验结果给出近似的预估</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes12.pdf" target="_blank" rel="external">Reinforcement Learning and Control</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/2/N/M6SGF6VB4_M6SGKSC2N.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节开始我们介绍&lt;strong&gt;强化学习&lt;/strong&gt;(reinforcement learning)。在监督学习中，对于一个给定的输入x，我们可以明确知道输出y。而在很多&lt;strong&gt;序列决策&lt;/strong&gt;(sequential decision making
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记14: 独立成分分析</title>
    <link href="http://www.secondplayer.top/2018/08/12/machine-learning-ica/"/>
    <id>http://www.secondplayer.top/2018/08/12/machine-learning-ica/</id>
    <published>2018-08-12T13:25:20.000Z</published>
    <updated>2018-08-12T13:25:20.686Z</updated>
    
    <content type="html"><![CDATA[<p>这一节的主题是<strong>独立成分分析</strong>(Independent Components Analysis, ICA)。和PCA的降维思路不同，ICA主要解决的是找到数据背后的“独立”成分。我们从一个鸡尾酒会问题开始说起。</p>
<p>在一个鸡尾酒会中假设有n个人同时说话，屋子里的n个麦克风记录着这n个人说话时叠加的声音。由于每个麦克风离人的距离不同，它所采集到的声音是不同的，但都是这n个人声音的一种组合。那么问题是根据这些采集到的声音，我们是否可以还原每个人说话的声音？</p>
<p>该问题可以形式化地描述为，从n个独立的数据源s ∈ R<sup>n</sup>中我们观察到了叠加数据x，其中x可以表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-52124338fde83c6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中A是一个未知的方阵，我们称为<strong>混合矩阵</strong>(mixing matrix)。我们观察到的数据是{x<sup>(i)</sup>; i=1,…,n}，我们的目标是求出源数据{s<sup>(i)</sup>; i=1,…,n}。</p>
<p>在鸡尾酒会问题中，s<sup>(i)</sup>是个n维向量，s<sub>j</sub><sup>(i)</sup>表示第j个人在i时刻发出的声音。同样，x<sup>(i)</sup>也是个n维向量，x<sub>j</sub><sup>(i)</sup>表示第j个麦克风在i时刻采集到的声音。</p>
<p>令W = A<sup>-1</sup>表示<strong>解析矩阵</strong>(unmixing matrix)，我们只需要求出W，然后根据s<sup>(i)</sup> = Wx<sup>(i)</sup>就能求出s<sup>(i)</sup>。为了简化描述，我们用w<sub>i</sub><sup>T</sup>表示W的第i行，因此W可表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-76ead6479cb277ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="ICA的不确定性"><a href="#ICA的不确定性" class="headerlink" title="ICA的不确定性"></a>ICA的不确定性</h2><p>如果我们对源数据和混合矩阵没有任何先验知识的话，在求混合矩阵的过程中会存在不确定性。</p>
<p>第一个不确定性来自于源数据的顺序。当源数据的顺序交换后，对混合矩阵的列之间进行对应的交换能够保证生成同样的数据。</p>
<p>第二个不确定性来自于对源数据的缩放。假设我们用2A来替代A，0.5s<sup>(i)</sup>替代s<sup>(i)</sup>，那么x<sup>(i)</sup> = 2A · 0.5s结果不变，因而s<sup>(i)</sup>和A的值不是唯一确定的。</p>
<p>第三个不确定性来自于源数据不能是高斯分布的。如果源数据s<sup>(i)</sup>是高斯分布的，我们可以证明混合矩阵A不是唯一的。证明如下：</p>
<p>令R是任意<strong>正交矩阵</strong>(orthogonal matrix)，因而RR<sup>T</sup> = I。令A’ = AR并将A替换成A’，那么x’ = A’s，并且x’也是高斯分布的，其均值为0，方差为E[x’(x’)<sup>T</sup>] = E[A’ss<sup>T</sup>(A’)<sup>T</sup>] = E[ARss<sup>T</sup>(AR)<sup>T</sup>] = ARR<sup>T</sup>A<sup>T</sup> = AA<sup>T</sup>。也就是说，无论是A还是A’，我们观察到的数据都是服从N(0, AA<sup>T</sup>)的高斯分布，因此我们没法区分混合矩阵究竟是A还是A’。</p>
<h2 id="密度函数和线性变换"><a href="#密度函数和线性变换" class="headerlink" title="密度函数和线性变换"></a>密度函数和线性变换</h2><p>在推导ICA算法之前，我们先讨论线性变换后的密度函数。</p>
<p>假设随机变量s的概率密度函数为p<sub>s</sub>(s)，另一个随机变量x = As，其中A是一个可逆的方阵，x的概率密度函数是p<sub>x</sub>，那么p<sub>x</sub>为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-679e14f93e7142c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中W = A<sup>-1</sup>。</p>
<h2 id="ICA算法"><a href="#ICA算法" class="headerlink" title="ICA算法"></a>ICA算法</h2><p>现在我们正式推导ICA算法，这个算法主要归功于Bell和Sejnowski，但我们这里使用最大似然估计法来进行解释。算法原始的版本用了一种更为复杂的方法，但对于目前我们理解ICA算法不是必要的。</p>
<p>假设每个源数据s<sup>(i)</sup>的概率密度函数是p<sub>s</sub>，那么它们的联合分布为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c060e554d795643c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这里我们假设每个源数据都是互相独立的。再根据上一节的公式，由于有x = As这样的线性变换，所以：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-08354345d4dff761.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>前面提过如果没有任何先验知识，W和s是无法唯一确定的，因此我们这里对s的概率密度函数做一定假设，同时根据前面讨论p<sub>s</sub>(s)不能是高斯分布的。我们知道密度函数可以通过对累计分布函数求导获得，而累计分布函数必须是在0到1之间单调递增的函数，因此我们可以选取sigmoid函数作为默认的累计分布函数。所以p<sub>s</sub>(s) = g’(s)，其中g(s) = 1 / (1 + e<sup>-s</sup>)。</p>
<p>确定了p<sub>s</sub>(s)后，W是模型里唯一需要求解的参数了。给定训练数据集{x<sup>(i)</sup>; i=1,…,n}，通过最大似然估计法可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c0dfe235db980491.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对l(W)求导并且根据∇<sub>W</sub>|W| = |W|(W<sup>-1</sup>)<sup>T</sup>的事实，我们可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2d6971c69dff2ada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中α是学习率。</p>
<p>算法收敛后即可得到参数W，然后通过s<sup>(i)</sup> = Wx<sup>(i)</sup>就能还原出源数据了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>独立成分分析(ICA)算法可以用于求解类似鸡尾酒会类型的问题，在已知叠加数据的情况下还原出源数据</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes11.pdf" target="_blank" rel="external">Independent Components Analysis</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/J/V/M6SGF6VB4_M6SGKINJV.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节的主题是&lt;strong&gt;独立成分分析&lt;/strong&gt;(Independent Components Analysis, ICA)。和PCA的降维思路不同，ICA主要解决的是找到数据背后的“独立”成分。我们从一个鸡尾酒会问题开始说起。&lt;/p&gt;
&lt;p&gt;在一个鸡尾酒会中假
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记13: 主成分分析</title>
    <link href="http://www.secondplayer.top/2018/07/29/machine-learning-pca/"/>
    <id>http://www.secondplayer.top/2018/07/29/machine-learning-pca/</id>
    <published>2018-07-29T15:13:41.000Z</published>
    <updated>2018-07-29T15:15:58.482Z</updated>
    
    <content type="html"><![CDATA[<p>上一节我们介绍了因子分析，该模型通过一系列变换可以将高维数据用低维数据来表示。因子分析基于的是概率模型，并且需要用到EM算法进行参数估计。</p>
<p>这一节我们介绍<strong>主成分分析</strong>(Principal Components Analysis, PCA)，这也是一种可以将高维数据映射到低维数据的方法，但是这种方法更加直接，计算方法也更为简单。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>假设我们有一个数据集{x<sup>(i)</sup>; i=1, …, m}表示m种不同汽车的特征，比如最大速度，转速等等。其中有两个特征x<sub>i</sub>和x<sub>j</sub>都表示汽车的最大速度，区别是一个以英里/小时作为单位，另一个以千米/小时作为单位。因此这两个特征是线性相关的，最多包含由于四舍五入导致的细微差别。因此数据的特征数可以缩减成n - 1维，那么我们如何自动检测并去除这一冗余的特征呢？</p>
<p>再假设我们有一个数据集表示对无线电遥控直升机的飞行员的调查，其中x<sub>1</sub><sup>(i)</sup>表示第i个飞行员的遥控技能，x<sub>2</sub><sup>(i)</sup>表示第i个飞行员有多热爱遥控直升机这件事。由于遥控直升机是一件非常难掌握的技能，所以只有非常热爱这件事情的人才能成为出色的飞行员。因此这两个特征是强相关的。如下图所示，x<sub>1</sub>和x<sub>2</sub>可以认为是在u<sub>1</sub>这条轴上并且伴有一些噪声的扰动，那么我们如何计算出u<sub>1</sub>呢？</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5fba0b26bbf8d248.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><p>PCA算法的第一步是对数据进行预处理，目的是将均值和方差归一化，具体步骤如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2e108d90b6fe8728.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对数据归一化后，我们应该如何计算u呢？考虑如下图的数据集，这些数据已经经过归一化处理了：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-236244490e464667.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>假设我们选取u为下图所示方向，图中圆点表示原始数据在u上的投影：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5fee9a37e3bf7cca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们可以看到投影后的数据有较大的方差，并且投影点离原点也较远。相反地，如果我们选取如下图所示的方向：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7a8941cead3c9fdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个图上投影后的数据的方差较小，并且投影点离原点较近。</p>
<p>我们希望选择前面两张图中的第一张图，并且是以一种自动的方式。将上述过程形式化，给定一个单位向量u和点x，x在u上的投影距离是x<sup>T</sup>u，由于我们希望将投影距离的方差最大化，所以目标是选择u使得下面的式子最大化：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d0b1721c2c921986.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式右边括号内的部分正是样本数据的协方差矩阵Σ，因此左边的式子就是样本数据的<strong>特征向量</strong>(eigenvector)。</p>
<p>因此，如果我们只需要映射到一维空间，那么我们应该选择Σ的特征向量。更一般地，如果我们希望映射到k维空间，那么我们应该选择Σ的前k个特征向量。</p>
<p>得到前k个特征向量后，x<sup>(i)</sup>可以被表示成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3d0f8f7349386915.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过如上变换，y<sup>(i)</sup>给出了x<sup>(i)</sup>的一个低维的近似表示。因此，PCA通常被认为是一种<strong>降维</strong>(dimensionality reduction)算法，向量u<sub>1</sub>, … u<sub>k</sub>被称为数据的前k个<strong>主成分</strong>(principal components)。</p>
<p>PCA有着非常广泛的应用。第一个应用是数据压缩，如果可以将很高维的数据降低到两三维，那么可以获得很高的压缩率。第二个应用是在监督学习算法中降低特征的个数，较少的个数不仅可以在计算上花费更少的时间，而且可以降低假设函数的复杂度。第三个应用是在图像处理中进行降噪，比如在人脸识别应用中，我们可以先使用PCA将图像降维成100*100维的向量，这个过程可以去除图像中冗余的特征，而且最大程度地保留了图像原有的信息。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>主成分分析是一种常见的降维算法，该算法不依赖任何概率模型，并且能最大程度地保留原始数据信息</li>
<li>主成分分析的步骤首先是对均值和方差进行归一化，然后求出样本数据的协方差矩阵的前k个特征向量，前k个特征向量也被称为数据的前k个主成分</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf" target="_blank" rel="external">Principal Components Analysis</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/M/E/M6SGF6VB4_M6SGKIEME.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一节我们介绍了因子分析，该模型通过一系列变换可以将高维数据用低维数据来表示。因子分析基于的是概率模型，并且需要用到EM算法进行参数估计。&lt;/p&gt;
&lt;p&gt;这一节我们介绍&lt;strong&gt;主成分分析&lt;/strong&gt;(Principal Components Analysis,
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记12: 因子分析</title>
    <link href="http://www.secondplayer.top/2018/07/15/machine-learning-factor-analysis/"/>
    <id>http://www.secondplayer.top/2018/07/15/machine-learning-factor-analysis/</id>
    <published>2018-07-15T13:38:47.000Z</published>
    <updated>2018-07-15T13:38:47.542Z</updated>
    
    <content type="html"><![CDATA[<p>上一节我们介绍了用EM算法求解混合高斯模型，但这个算法通常是在样本数足够多的情况下才成立，即满足样本数m远远大于特征数n。</p>
<p>如果n &gt;&gt; m，那么在模型计算参数的时候会遇到一些问题。计算均值和协方差函数这两个参数的公式为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c3330f8fe8df7b00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>当n &gt;&gt; m时，我们会发现Σ是奇异矩阵，这也就意味着Σ<sup>-1</sup>不存在，并且1/|Σ|<sup>1/2</sup> = 1/0，这几项在估计多元高斯分布的密度函数中都会用到，因此我们没法进行拟合。</p>
<p>更一般的，如果m没有在一定范围内大于n，那么用极大似然估计法估计参数的效果都很差。然而我们还是希望用多元高斯分布来估计样本，应该怎么办呢？</p>
<h2 id="限制协方差矩阵"><a href="#限制协方差矩阵" class="headerlink" title="限制协方差矩阵"></a>限制协方差矩阵</h2><p>如果我们没有足够的数据来估计协方差矩阵Σ，那么可以考虑给Σ做一些假设。比如我们可以假设Σ是对角矩阵，那么可以计算出：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-fc6c34ef31e8a798.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>回顾一下之前我们说过高斯密度函数的等高线是椭圆，如果Σ是对角矩阵，那么就意味着椭圆的主轴与坐标轴是平行的。</p>
<p>有时我们会对Σ做更强的假设，Σ不仅是对角矩阵，而且对角上每个元素的值都是相等的，我们可以写成：Σ = σ<sup>2</sup>I，其中 σ<sup>2</sup>是我们可以控制的参数，通过极大似然估计可以计算出：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-94d24e0919054319.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个模型对应的等高线是个圆形(在二维空间是圆，在三维空间是球)。</p>
<p>如果我们要估计出完整的Σ，需要满足m ≥ n + 1才能保证Σ是非奇异矩阵。而如果使用上面两种假设，只需要满足m ≥ 2 就能保证Σ是非奇异矩阵。</p>
<p>但是使用上面两种假设也是有明显缺点的，我们假设了特征之间是独立不相关的，这个假设太强了，我们还是希望能捕捉到特征之间的关系的。接下来我们介绍一种因子分析的模型，它使用了比对角矩阵更多的特征，同时保留了特征之间的关系，并且不需要计算一个完整的Σ。</p>
<h2 id="边缘与条件高斯分布"><a href="#边缘与条件高斯分布" class="headerlink" title="边缘与条件高斯分布"></a>边缘与条件高斯分布</h2><p>在引入因子分析模型之前，我们先介绍下如何在多元高斯分布下求解边缘与条件高斯分布。</p>
<p>假设我们有如下的随机变量：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-4ba05856e574d956.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中x<sub>1</sub> ∈ R<sup>r</sup>，x<sub>2</sub> ∈ R<sup>s</sup>，x ∈ R<sup>r + s</sup>。假设x服从高斯分布N(μ, Σ)，其中</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-dd5b0c8607c7f91d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的μ<sub>1</sub> ∈ R<sup>r</sup>，μ<sub>2</sub> ∈ R<sup>s</sup>，Σ<sub>11</sub> ∈ R<sup>r x r</sup>，Σ<sub>12</sub> ∈ R<sup>r x s</sup>，以此类推。注意由于协方差矩阵的对称性，Σ<sub>12</sub> = Σ<sub>21</sub><sup>T</sup>。</p>
<p>在我们的假设中，x<sub>1</sub>和x<sub>2</sub>的联合分布是多元高斯分布，那么x<sub>1</sub>的边缘分布是什么呢？不难证明，E[x<sub>1</sub>] = μ<sub>1</sub>，Cov(x<sub>1</sub>) = E[(x<sub>1</sub> - μ<sub>1</sub>)(x<sub>1</sub> - μ<sub>1</sub>)<sup>T</sup>] = Σ<sub>11</sub>。关于Cov(x<sub>1</sub>)的证明如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-256da9a43a4dd423.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>比对矩阵左上角部分就可得到结论。由此可见，多元高斯分布的边缘分布仍然是多元高斯分布，即x<sub>1</sub> ~ N(μ<sub>1</sub>, Σ<sub>11</sub>)。</p>
<p>接下来我们看条件分布应该如何求解。根据多元高斯分布的定义，可得x<sub>1</sub> | x<sub>2</sub> ~ N(μ<sub>1|2</sub>, Σ<sub>1|2</sub>)，其中：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6025ec6ff77d63d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在接下来的因子分析模型推导中，上面这些公式会非常有用。</p>
<h2 id="因子分析模型"><a href="#因子分析模型" class="headerlink" title="因子分析模型"></a>因子分析模型</h2><p>在<strong>因子分析</strong>(factor analysis)模型中，我们给出(x, z)的联合分布如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-da3e1bf125c34463.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中z ∈ R<sup>k</sup>是隐含随机变量，μ ∈ R<sup>n</sup>，变换矩阵Λ ∈ R<sup>n x k</sup>，对角矩阵Ψ ∈ R<sup>n x n</sup>，k通常选择为比n小的一个数。</p>
<p>上述过程可以理解为：首先在k维空间中按照多元高斯分布生成z<sup>(i)</sup>，然后通过μ + Λz<sup>(i)</sup>将z<sup>(i)</sup>映射到n维空间中，最后由于x<sup>(i)</sup>与上述模型之间存在误差，所以在模型基础上增加协方差矩阵Ψ的噪音，从而得到训练数据x<sup>(i)</sup>。</p>
<p>上述过程可以等价表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7902e4b310691711.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中ε和z是独立的</p>
<p>上述过程可以进一步表述为：高维样本点是通过低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以用低维数据来表示。</p>
<p>下面我们开始计算模型参数。由于x和z的联合分布符合多元高斯分布，所以可以表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-30dc8196a19bdf2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们需要计算出μ<sub>zx</sub>和Σ。首先由于z ~ N(0, I)，所以E[z] = 0，因此有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-66793e263ddee7d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>将两个结果结合起来，就有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ddd6b535086c3064.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>然后我们来计算Σ。我们很容易证明Σ<sub>zz</sub> = Cov(z) = I。另外，我们也可以推导出Σ<sub>zx</sub>：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c5f32675c5c0e152.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>同样我们也可以推导出Σ<sub>xx</sub>：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6f886a7aa2954e4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>综合上述结果，我们可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a9b15c5b7224ac63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由此，我们也能得到x的边缘分布是x ~ N(μ, ΛΛ<sup>T</sup> + Ψ)。因此对于样本{x<sup>(i)</sup>; i=1, …, m}，我们对其进行极大似然估计：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c47483f0d418dc07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>但是我们没有办法通过求导的方式获得各个参数，根据上一节的经验，我们需要借助EM算法进行求解。</p>
<h2 id="因子分析的EM算法"><a href="#因子分析的EM算法" class="headerlink" title="因子分析的EM算法"></a>因子分析的EM算法</h2><p>E步的推导比较简单。我们需要计算Q<sub>i</sub>(z<sup>(i)</sup>) = p(z<sup>(i)</sup>|x<sup>(i)</sup>; μ, Λ, Ψ)。根据之前条件分布的讨论，z<sup>(i)</sup>|x<sup>(i)</sup>; μ, Λ, Ψ ~ N(μ<sub>z<sup>(i)</sup>|x<sup>(i)</sup></sub>, Σ<sub>z<sup>(i)</sup>|x<sup>(i)</sup></sub>)，其中：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-461f8f209a702995.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>将其代入到Q<sub>i</sub>(z<sup>(i)</sup>)中，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5ce2a3b8adaa9784.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>接下来我们来看M步，我们需要最大化的目标函数是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-befe6dcd1e78ba82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们需要分别求出μ, Λ, Ψ。这三个参数的推导需要有一定数学技巧，这里就省略推导步骤，直接给出结果了。感兴趣的读者可以查阅讲义部分对Λ的推导。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-4c751a66917d1664.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5b925b7a932abf80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9692b191016ea633.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中Ψ是对角矩阵，只需将Φ上对角线上的元素放在Ψ对应位置上就得到了Ψ。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>当样本数m远远小于特征数n时，用EM算法求解混合高斯模型是不可行的，我们需要使用因子分析模型</li>
<li>因子分析模型的方法本质是：高维样本点是通过低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以用低维数据来表示</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes9.pdf" target="_blank" rel="external">Factor Analysis</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/L/3/M6SGF6VB4_M6SGKK6L3.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一节我们介绍了用EM算法求解混合高斯模型，但这个算法通常是在样本数足够多的情况下才成立，即满足样本数m远远大于特征数n。&lt;/p&gt;
&lt;p&gt;如果n &amp;gt;&amp;gt; m，那么在模型计算参数的时候会遇到一些问题。计算均值和协方差函数这两个参数的公式为：&lt;/p&gt;
&lt;figure 
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记11: K-Means算法和EM算法</title>
    <link href="http://www.secondplayer.top/2018/07/01/machine-learning-k-means-and-em-algorithm/"/>
    <id>http://www.secondplayer.top/2018/07/01/machine-learning-k-means-and-em-algorithm/</id>
    <published>2018-07-01T13:03:44.000Z</published>
    <updated>2018-07-01T13:03:44.178Z</updated>
    
    <content type="html"><![CDATA[<p>这一节开始我们讨论<strong>非监督学习</strong>(Unsupervised Learning)的算法。在监督学习算法中，训练数据既包含特征也包含标签，通常表示为{(x<sup>(1)</sup>, y<sup>(1)</sup>), …, (x<sup>(m)</sup>, y<sup>(m)</sup>)}；而在非监督学习中，训练数据只有特征没有标签，通常表示为{x<sup>(1)</sup>, …, x<sup>(m)</sup>}。</p>
<h2 id="K-Means算法"><a href="#K-Means算法" class="headerlink" title="K-Means算法"></a>K-Means算法</h2><p><strong>聚类</strong>(clustering)问题是最常见的非监督学习问题。对于一组无标签数组，可以用聚类算法将数据分成若干相似的“群组”，从而发掘数据中的隐藏结构。</p>
<p>K-Means算法是最常见的一种聚类算法，算法步骤描述如下：</p>
<blockquote>
<ol>
<li>随机选择k个<strong>中心点</strong>(centroids): μ<sub>1</sub>, μ<sub>2</sub>, …, μ<sub>k</sub></li>
<li>重复如下过程直到收敛：{<br> 对于每个样本i，令：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-897162e6574c92bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>对于每个中心点j，令：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-347a0b505b9f4c35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure></li>
</ol>
<p>}</p>
</blockquote>
<p>在上面的算法中，k表示我们想要聚类的个数，μ<sub>j</sub>表示目前我们对中心点的猜测。算法的第一步是初始化中心点，这个可以通过随机选择k个样本数据获得。算法的第二步是不断循环如下两个过程：首先将样本x<sup>(i)</sup>“指派”给离它距离最近的中心点μ<sub>j</sub>，然后将中心点μ<sub>j</sub>更新为所有被“指派”给它的样本距离的平均值。下图展示了算法的执行过程：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a7036ad9ff530a7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>图(a)表示原始的训练数据。图(b)表示随机选取了两个初始中心点，用记号x表示。图(c)表示将每个训练数据指派给离它最近的中心点，图中用不同颜色表示数据与中心点的关系。图(d)表示更新中心点的位置。图(e)和(f)表示又进行了一轮迭代。</p>
<p>K-Means算法一定可以保证收敛。特别的，我们定义<strong>失真函数</strong>(distortion function)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1bab771ef2b3be25.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此，J函数表示每个样本点离它对应中心点的平方和。可以证明，K-Means算法其实就是对J函数应用坐标下降算法。具体来说，K-Means算法内层循环的第一步是固定μ使c关于J求最小化，第二步是固定c使μ关于J求最小化。因此J是单调递减的，J的值必然是收敛的。</p>
<p>由于J函数不是凸函数，所以坐标下降法未必能得到全局最优解，也就是说K-Means算法可能会得到局部最优解。为了防止K-Means算法得出的结果是局部最优解，通常可以用不同的初始值多运行几次算法，然后取使得J函数最小的那个参数值。</p>
<h2 id="混合高斯模型"><a href="#混合高斯模型" class="headerlink" title="混合高斯模型"></a>混合高斯模型</h2><p>这一部分我们介绍使用混合高斯模型进行聚类的算法。回顾在高斯判别分析(GDA)模型中，我们假设x是连续的随机变量，p(y)服从伯努利分布，p(x|y)服从多元正态分布，通过极大似然法使得联合概率p(x, y)最大，从而求解出模型的参数。</p>
<p>而在非监督学习的设定下，我们没有y的数据，因此我们引入一个隐含(latent)随机变量z。我们建立联合分布p(x<sup>(i)</sup>, z<sup>(i)</sup>) = p(x<sup>(i)</sup>|z<sup>(i)</sup>)p(z<sup>(i)</sup>)，其中p(z<sup>(i)</sup>)服从参数为φ的多项式分布(p(z<sup>(i)</sup> = j) = φ<sub>j</sub>)，p(x<sup>(i)</sup>|z<sup>(i)</sup>)服从正态分布N(μ<sub>j</sub>, Σ<sub>j</sub>)。我们的模型首先让z<sup>(i)</sup>随机地从1到k中取值，然后x<sup>(i)</sup>是从k个高斯分布中选择第z<sup>(i)</sup>个分布，这样的模型就称之为<strong>混合高斯模型</strong>(Mixtures of Gaussians)。由于z<sup>(i)</sup>是不可观察的隐含变量，这使得我们的计算变得更为困难。</p>
<p>为了计算模型中φ, μ和Σ的值，我们写出似然函数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5a135317b94c4edb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果我们通过用求导的方式来求解参数，我们会发现这是不可能做到的。</p>
<p>由于z<sup>(i)</sup>是用来表示x<sup>(i)</sup>是来自k个高斯分布中的哪一个的，如果我们知道z<sup>(i)</sup>的取值的话，那么问题就变得简单了，我们将问题简化为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-095764ae8190dd82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对此似然函数最大化，我们求得各参数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-cf675464c0a13f2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此，如果z<sup>(i)</sup>是已知的，那么最大化似然函数的问题就很简单了，而且求解参数的结果和通过GDA求解得到的参数非常相似。</p>
<p>但是在我们实际问题中，z<sup>(i)</sup>是未知的，那我们应该怎么办呢？</p>
<p>我们可以采用EM算法。EM算法是一个迭代式的算法，它总共分为两步。在这个问题中，E步尝试猜测一个合理的z<sup>(i)</sup>值，M步按照上一步猜测的z<sup>(i)</sup>来更新模型的参数。在M步，由于我们假设E步猜测的z<sup>(i)</sup>是正确的，因此最大化似然函数的问题变得简单了。算法步骤描述如下：</p>
<blockquote>
<p>重复如下过程直到收敛：{<br>E步：对于每个i, j，我们令：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-255e206ece37ac55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>M步：按如下公式更新参数：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f3b199075a4eb62e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>}</p>
</blockquote>
<p>在E步中，我们计算的是在给定x<sup>(i)</sup>和其他参数情况下z<sup>(i)</sup>的后验概率，根据贝叶斯公式，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2b13f0b092d8799f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的p(x<sup>(i)</sup>|z<sup>(i)</sup> = j; μ,Σ)是通过平均值为μ<sub>j</sub>，协方差为Σ<sub>j</sub>的高斯分布在x<sup>(i)</sup>上的概率密度计算得出；p(z<sup>(i)</sup> = j;φ)的值由φ<sup>(j)</sup>计算得出。我们在E步中计算的w<sub>j</sub><sup>(i)</sup>可以认为是对z<sup>(i)</sup>的软猜测(软猜测是指猜测的结果是[0,1]之间的概率值，硬猜测是指0或1的取值)。</p>
<p>EM算法和K-Means算法的迭代步骤其实比较类似，不同的是K-Means算法中每次对c<sup>(i)</sup>的更新是硬猜测，而EM中每次对w<sup>(i)</sup>的更新是软猜测。和K-Means算法相同的是，EM算法也可能得到局部最优解，所以用不同的初始参数迭代会得到更好的结果。</p>
<p>这一部分我们用EM算法的思想讲了混合高斯模型问题，后面我们会讲更通用的EM算法，以及EM算法是如何保证算法的收敛性的。在讲解通用EM算法之前，我们先要介绍一个定理作为预备知识。</p>
<h2 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h2><p>令f是一个定义域为实数的函数。若f是<strong>凸函数</strong>(convex function)，则f’’(x) &gt;= 0。如果f的定义域是向量，那么凸函数的条件变为f的Hessian矩阵H是半正定矩阵(H &gt;= 0)。如果对于所有的x都有f’’(x) &gt; 0，那么我们说f是严格凸函数。对于定义域是向量的情况，如果H是正定矩阵(H &gt; 0)，那么我们说f是严格凸函数。</p>
<p><strong>Jensen不等式</strong>(Jensen’s inequality)可表述如下：</p>
<p>令f是凸函数，X是随机变量，那么有：E[f(X)] &gt;= f(EX)。如果f是严格凸函数，那么E[f(X)] = f(EX)当且仅当X = E[X]的概率为1(即X是常量)。</p>
<p>由于我们在书写期望的时候有时候会把括号省略掉，所以上式中的EX是E[X]的缩写。为了对这个公式有直观的理解，我们可以用下图来解释：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-07ec6d4d36961222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图中的曲线表示函数f，X是一个随机变量，它有一半的概率取a，有一半的概率取b，所以E[X]介于a和b的中间。另一方面我们可以看到y轴上f(a)，f(b)和f(EX)的值，而E[f(X)]介于f(a)和f(b)的中间。由于凸函数的特性，很容易看出一定有E[f(X)] &gt;= f(EX)。</p>
<p>事实上很多人会记不清这个不等式的方向，有了上面这个图的话可以帮助我们更好的记忆。</p>
<p>注意，如果f是<strong>凹函数</strong>(concave function)(即f’’(x) &lt;= 0或H &lt;= 0)，Jensen不等式也是成立的，只不过不等式的方向变了，即E[f(X)] &lt;= f(EX)。</p>
<h2 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h2><p>这一节我们介绍通用的EM算法。假设我们的训练集是m个互相独立的样本{x<sup>(1)</sup>, …, x<sup>(m)</sup>}，我们希望对p(x, z)进行建模，其似然函数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-595c774638978e3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>直接通过最大化似然函数求解参数是比较困难的。这里z<sup>(i)</sup>是隐含的随机变量，如果z<sup>(i)</sup>的值是已知的，那么问题将变得简单。</p>
<p>在这种情况下，EM算法给出了一个高效地求解最大化似然函数的方法。直接最大化l(θ)可能比较困难，那么我们的策略是不断地构造l的下界(E步)，然后最大化该下界(M步)。</p>
<p>对于每一个i，令Q<sub>i</sub>是z<sup>(i)</sup>的某个分布(Σ<sub>z</sub>Q<sub>i</sub>(z) = 1, Q<sub>i</sub>(z) &gt;= 0; 如果z<sup>(i)</sup>是连续的，那么Q<sub>i</sub>是概率密度函数)，引入Q<sub>i</sub>后，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-69d74bd5a897020b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中最后一步的不等式是由于Jensen不等式推导而来。具体来说由于f(x) = log(x)是凹函数，并且我们把</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3a98d75f5c21af10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这一项看成是[p(x<sup>(i)</sup>, z<sup>(i)</sup>; θ)/Q<sub>i</sub>(z<sup>(i)</sup>)]关于z<sup>(i)</sup>的期望，所以根据Jensen不等式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-0722a06e02040058.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中“z<sup>(i)</sup> ~ Q<sub>i</sub>”的下标表示这是关于z<sup>(i)</sup>的期望，而z<sup>(i)</sup>满足Q<sub>i</sub>的分布。综上我们可以从等式(2)推导出等式(3)。</p>
<p>现在对于任意分布Q<sub>i</sub>，等式(3)给出了l(θ)的下界。Q<sub>i</sub>可以有很多选择，而我们应该选择Q<sub>i</sub>使得l(θ)最接近下界，也就是说我们希望选择Q<sub>i</sub>使得Jensen不等式的等号成立。</p>
<p>回顾下使得Jensen不等式等号成立的条件，对照上式可得如下的随机变量应该是常数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-92113893e39d76ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中c代表某个与z<sup>(i)</sup>无关的常数，我们把它写成如下的形式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-44c1cd3861a1affb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>再加上我们已知Σ<sub>z</sub>Q<sub>i</sub>(z<sup>(i)</sup>) = 1，因此可推导出：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-da3ad8387ab286ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所以Q<sub>i</sub>是在给定x<sup>(i)</sup>和θ两个参数下的z<sup>(i)</sup>的后验概率。</p>
<p>现在我们可以给出EM算法的基本步骤：</p>
<blockquote>
<p>重复如下过程直到收敛：{<br>E步：对于每个i，令：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7ef5cb27848628b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>M步：按如下公式更新参数：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-93781c0ec95fec6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure><br>}</p>
</blockquote>
<p>算法中的E步给出了l(θ)的下界函数，M步是在最大化该下界。当算法收敛时我们便得到了最优解。那么这个算法为什么能收敛呢？这里我们简单证明一下。</p>
<p>假设θ<sup>(t)</sup>和θ<sup>(t+1)</sup>是EM算法连续两次迭代中的参数，而θ<sup>(t)</sup>是满足Jensen不等式成立的参数，所以：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-407da78ddc4ed7d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>而θ<sup>(t+1)</sup>是使得l(θ<sup>(t)</sup>)最大化的参数，所以有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-af40de5f5cbdf94b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此我们证明了l(θ<sup>(t+1)</sup>) &gt;= l(θ<sup>(t)</sup>)，所以EM算法是单调递增的，由此可证算法是收敛的。</p>
<p>另外，如果我们定义：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-99e0f39b74161b3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>那么EM算法可以视为将J函数作坐标上升的过程，其中在E步是固定θ使Q关于J求最大化，M步是固定Q使θ关于J求最大化。</p>
<h2 id="混合高斯模型重述"><a href="#混合高斯模型重述" class="headerlink" title="混合高斯模型重述"></a>混合高斯模型重述</h2><p>在给出了通用的EM算法定义后，我们重新推导一下混合高斯模型中的EM算法。前面我们只描述了算法，但没有证明为什么参数需要按照给定的公式更新。</p>
<p>证明E步是很简单的，根据EM算法中的推导，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7e405b2a7f244927.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>而在M步中，我们需要最大化的目标函数是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-72101acb30c630a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>首先固定其他参数，关于μ作目标函数最大化，我们先求出目标函数关于μ的导数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2227ded23389cb20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>将导数设为0，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-542d7801ad3a08e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这和我们之前给出的结论一致。</p>
<p>接下来我们再来求参数φ。将和φ有关的项进行合并，我们需要最大化的目标函数是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ffd530c2832e75a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>除此之外，我们还有一个约束：所有的φ<sub>j</sub>之和为1。为了最优化该问题，我们构建拉格朗日算子：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e1eea1252b2ccc00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中β是拉格朗日乘数，对L求导可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-4c443a1966fe8013.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>将导数设为0，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-093393989985a365.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>另外我们求解β，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e7ba3c6f241523d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>将β代入回φ<sub>j</sub>式中：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e591550f75197260.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此，我们推导出了参数φ。参数Σ的推导类似，这里就不做证明了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>聚类算法是最常见的无监督学习算法，而K-Means算法是最常见的聚类算法</li>
<li>Jensen不等式：令f是凸函数，X是随机变量，那么有：E[f(X)] &gt;= f(EX)。如果f是严格凸函数，那么E[f(X)] = f(EX)当且仅当X = E[X]的概率为1(即X是常量)；如果f是凹函数，那么Jensen不等式也成立，但不等式的方向相反</li>
<li>EM算法是一个迭代式的算法，分为两个步骤：E步和M步；EM算法也可以视为将目标函数作坐标上升的过程，每个步骤都是固定一个参数并估计另一个参数</li>
<li>EM算法和K-Means算法的迭代过程比较类似，不同的是K-Means算法中每次对参数的更新是硬猜测，而EM中每次对参数的更新是软猜测；相同的是，两个算法都可能得到局部最优解，采用不同的初始参数迭代会有利于得到全局最优解</li>
<li>混合高斯模型是对单一高斯模型的扩展，可以通过EM算法进行参数求解</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes7a.pdf" target="_blank" rel="external">The K-Means Algorithm</a> | <a href="http://cs229.stanford.edu/notes/cs229-notes7b.pdf" target="_blank" rel="external">Mixture of Gaussians</a> | <a href="http://cs229.stanford.edu/notes/cs229-notes8.pdf" target="_blank" rel="external">The EM Algorithm</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/O/T/M6SGF6VB4_M6SGKGMOT.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节开始我们讨论&lt;strong&gt;非监督学习&lt;/strong&gt;(Unsupervised Learning)的算法。在监督学习算法中，训练数据既包含特征也包含标签，通常表示为{(x&lt;sup&gt;(1)&lt;/sup&gt;, y&lt;sup&gt;(1)&lt;/sup&gt;), …, (x&lt;sup&gt;(m)
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记10: 应用机器学习的建议</title>
    <link href="http://www.secondplayer.top/2018/06/18/machine-learning-advice/"/>
    <id>http://www.secondplayer.top/2018/06/18/machine-learning-advice/</id>
    <published>2018-06-18T13:01:08.000Z</published>
    <updated>2018-06-18T13:01:08.869Z</updated>
    
    <content type="html"><![CDATA[<p>这一节我们主要讨论机器学习具体应用中的一些建议，大部分内容都不会涉及到数学，但却可能是最难理解的一部分。另外本文的部分内容可能有一些争议，部分内容也不适用于学术研究。</p>
<h2 id="机器学习算法的诊断"><a href="#机器学习算法的诊断" class="headerlink" title="机器学习算法的诊断"></a>机器学习算法的诊断</h2><p>我们来思考如下这个问题：在垃圾邮件分类问题中，我们从50000+的词汇表中选择了100个单词作为特征，然后选择<strong>贝叶斯逻辑回归</strong>(Bayesian logistic regression)模型，利用梯度下降法进行训练，但是训练得到的结果有20%的误差，这显然是不能接受的。</p>
<p>这个模型的目标函数如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7ee2511b9a9643ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>那么我们下一步应该如何优化呢？通常我们可以有如下多个改进方式：</p>
<ul>
<li>增加训练数据</li>
<li>尝试更小的特征集合</li>
<li>尝试更大的特征集合</li>
<li>尝试改变特征(比如使用邮件的标题和正文部分)</li>
<li>增加梯度下降的迭代次数</li>
<li>尝试用牛顿方法</li>
<li>改变正则化参数λ</li>
<li>尝试其他模型，比如SVM</li>
</ul>
<p>这些方法都可能有用，但是挨个进行实验非常耗时，如果碰巧解决问题也可能只是运气好。因此更好的方法是我们对算法进行诊断，并根据诊断结果进行针对性的改进。</p>
<p>第一个的诊断方法是判断算法是过拟合还是欠拟合。过拟合意味着高方差，欠拟合意味着高偏差。我们可以通过<strong>学习曲线</strong>(learning curve)来诊断模型到底是高方差还是高偏差的。</p>
<p>下图展示的是高偏差模型的学习曲线：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ede0c529feb19d7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>可以看出，当训练集个数增加时，测试误差逐步减少，但是测试误差和训练误差之间仍存在较大的间隔。因此增加训练数据可以改善模型。</p>
<p>下图展示的是高方差模型的学习曲线：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-708da7cc224f42ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>可以看出，测试误差和训练误差之间的间隔很小，但是它们的误差率与理想值相差很远。</p>
<p>第二个诊断方法是关于优化算法和优化目标。这里我们继续接着刚才的例子讨论：假设通过BLR(贝叶斯逻辑回归)算法在垃圾邮件上的误差是2%，在正常邮件上的误差也是2%(对于正常邮件来说，这个误差是不可接受的)，而通过SVM算法在垃圾邮件上的误差是10%，在正常邮件上的误差是0.01%(对于正常邮件来说，这个误差是可以接受的)，但是我们倾向于使用BLR算法，因为BLR的计算效率更高。我们接下来应该怎么办？</p>
<p>令SVM的参数为θ<sub>SVM</sub>，BLR的参数为θ<sub>BLR</sub>。其实我们真正在乎的是<strong>加权准确率</strong>(weighted accuracy)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-68ed373ec2605691.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在上面的例子中，我们有：a(θ<sub>SVM</sub>) &gt; a(θ<sub>BLR</sub>)</p>
<p>BLR的优化目标J(θ)是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-74861ce796d367f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所以我们接下来诊断的方法就是比较J(θ<sub>SVM</sub>)和J(θ<sub>BLR</sub>)的大小。</p>
<p>第一种情况：a(θ<sub>SVM</sub>) &gt; a(θ<sub>BLR</sub>)，J(θ<sub>SVM</sub>) &gt; J(θ<sub>BLR</sub>)。BLR的目标是最大化J(θ)，但是现在BLR没能做到这点，因此问题在于优化算法，有可能算法还没有达到收敛。</p>
<p>第二种情况：a(θ<sub>SVM</sub>) &gt; a(θ<sub>BLR</sub>)，J(θ<sub>SVM</sub>) &lt;= J(θ<sub>BLR</sub>)。BLR确实做到了最大化J(θ)，但是BLR的加权准确率不如SVM，因此问题在于优化目标，J(θ)并不能很好地代表a(θ)。</p>
<p>因此回过头来看之前列的改进方式，每个改进方式其实都是在优化某一方面，具体描述如下：</p>
<ul>
<li>增加训练数据：解决高方差</li>
<li>尝试更小的特征集合：解决高方差</li>
<li>尝试更大的特征集合：解决高偏差</li>
<li>尝试改变特征(比如使用邮件的标题和正文部分)：解决高偏差</li>
<li>增加梯度下降的迭代次数：解决优化算法问题</li>
<li>尝试用牛顿方法：解决优化算法问题</li>
<li>改变正则化参数λ：解决优化目标问题</li>
<li>尝试其他模型，比如SVM：解决优化目标问题</li>
</ul>
<p>通过对算法进行诊断，我们可以有针对性地提出解决方案，从而避免无意义地进行错误的尝试。</p>
<h2 id="误差分析和销蚀分析"><a href="#误差分析和销蚀分析" class="headerlink" title="误差分析和销蚀分析"></a>误差分析和销蚀分析</h2><p>很多机器学习应用中会包含多个组成部分，各个部分之间形成一个整体的“管道(pipeline)”。比如下图展示了一个从图像中进行面部识别的管道图：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ffb1a70e3a54308b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>那么我们如何知道究竟哪个组件(component)对整体的贡献率最大呢？</p>
<p>我们可以使用两种方法来分析这个问题。第一个方法是<strong>误差分析</strong>(error analysis)，误差分析是指从最基础的模型开始，依次增加一个组件，看每个组件对准确率的提升情况。比如对这个例子来说，我们可以分析如下：</p>
<table>
<thead>
<tr>
<th>组件</th>
<th style="text-align:right">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td>整体系统</td>
<td style="text-align:right">85%</td>
</tr>
<tr>
<td>预处理(去除背景)</td>
<td style="text-align:right">85.1%</td>
</tr>
<tr>
<td>面部识别</td>
<td style="text-align:right">91%</td>
</tr>
<tr>
<td>分割眼睛</td>
<td style="text-align:right">95%</td>
</tr>
<tr>
<td>分割鼻子</td>
<td style="text-align:right">96%</td>
</tr>
<tr>
<td>分割嘴巴</td>
<td style="text-align:right">97%</td>
</tr>
<tr>
<td>逻辑回归</td>
<td style="text-align:right">100%</td>
</tr>
</tbody>
</table>
<p>由此可见，面部识别和分割眼睛这两个组件对整体的贡献率较大。</p>
<p>第二个方法是<strong>销蚀分析</strong>(ablative analysis)。销蚀分析的步骤刚好相反，每次从完整系统中去除一个组件，看每个组件对准确率的降低情况。比如在一个垃圾邮件分类系统里，我们可以做如下分析：</p>
<table>
<thead>
<tr>
<th>组件</th>
<th style="text-align:right">准确率</th>
</tr>
</thead>
<tbody>
<tr>
<td>整体系统</td>
<td style="text-align:right">99.9%</td>
</tr>
<tr>
<td>拼写检查</td>
<td style="text-align:right">99.0%</td>
</tr>
<tr>
<td>发送方主机特征</td>
<td style="text-align:right">98.9%</td>
</tr>
<tr>
<td>邮件头部特征</td>
<td style="text-align:right">98.9%</td>
</tr>
<tr>
<td>邮件文本解析器特征</td>
<td style="text-align:right">95%</td>
</tr>
<tr>
<td>Javascript解析器</td>
<td style="text-align:right">94.5%</td>
</tr>
<tr>
<td>图像特征</td>
<td style="text-align:right">94%</td>
</tr>
</tbody>
</table>
<p>由此可见，邮件文本解析器特征极大地提升了准确率，而邮件头部特征则对准确率几乎没有什么帮助。</p>
<h2 id="如何上手一个机器学习问题"><a href="#如何上手一个机器学习问题" class="headerlink" title="如何上手一个机器学习问题"></a>如何上手一个机器学习问题</h2><p>通常我们面对一个机器学习问题，可以有如下两种思路：</p>
<p>思路1：<strong>仔细设计</strong>(Careful design)</p>
<ul>
<li>花较长的时间设计出最好的特征，收集到最好的数据，建造出最好的算法架构</li>
<li>实现这个算法并希望它可以成功</li>
<li>优点：可能找到更新，更优雅的学习算法；适合用于学术研究</li>
</ul>
<p>思路2：<strong>快速迭代</strong>(Build-and-fix)</p>
<ul>
<li>先快速做一个可用版本</li>
<li>对算法进行误差分析和诊断，找到优化的方向并进行优化</li>
<li>优点：可以更快地将算法应用到实际场景中</li>
</ul>
<p>另外还有一个建议：在项目的早期阶段，我们通常并不清楚系统每个部分的实现难度，因此我们需要避免进行<strong>过早优化</strong>(premature optimization)。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>通过对算法进行诊断，可以帮助我们确定优化方向，从而避免无意义地进行错误的尝试</li>
<li>误差分析和销蚀分析可以帮助我们确定一个系统中哪个组件的贡献率最大</li>
<li>应用机器学习的两种方法：仔细设计和快速迭代；仔细设计适合用于学术研究，但是要避免过早优化的风险</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/materials/ML-advice.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/L/M/M6SGF6VB4_M6SGKG5LM.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节我们主要讨论机器学习具体应用中的一些建议，大部分内容都不会涉及到数学，但却可能是最难理解的一部分。另外本文的部分内容可能有一些争议，部分内容也不适用于学术研究。&lt;/p&gt;
&lt;h2 id=&quot;机器学习算法的诊断&quot;&gt;&lt;a href=&quot;#机器学习算法的诊断&quot; class=&quot;he
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记9: 模型选择</title>
    <link href="http://www.secondplayer.top/2018/06/03/machine-learning-model-selection/"/>
    <id>http://www.secondplayer.top/2018/06/03/machine-learning-model-selection/</id>
    <published>2018-06-03T14:00:39.000Z</published>
    <updated>2018-06-03T14:04:55.199Z</updated>
    
    <content type="html"><![CDATA[<p>假设我们为了某个学习问题需要从若干模型中选出最优模型，比如在多项式回归模型h<sub>θ</sub>(x) = g(θ<sub>0</sub>+θ<sub>1</sub>x+…+θ<sub>k</sub>x<sup>k</sup>)中，我们应该如何选择合适的k值使得模型的偏差方差达成平衡？又比如在局部加权线性回归问题中，我们应该如何选择合适的波长参数τ使得模型的拟合效果最好呢？</p>
<p>将上述问题给出形式化定义：假设我们有一个有限的模型集合M = {M<sub>1</sub>, …, M<sub>d</sub>}，我们应该选择哪个模型M<sub>i</sub>作为最优模型呢？</p>
<h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>假设训练集用S表示，考虑到上一讲的经验风险最小化(ERM)，我们可能会得出这样的一种算法：</p>
<blockquote>
<ol>
<li>使用S训练每一个M<sub>i</sub>，获得对应的假设函数h<sub>i</sub></li>
<li>选择训练误差最小的假设函数</li>
</ol>
</blockquote>
<p>很遗憾，这个算法是不可行的。比如在多项式回归问题中，使用越高阶的多项式，拟合的效果越好，因而训练误差越小。但是在上一讲中我们也说明过这样的选择会有很大的方差，实际上是一种过拟合。</p>
<p>下面我们对这个算法进行改进，这个算法叫做<strong>简单交叉验证</strong>(hold-out cross validation)。该算法描述如下：</p>
<blockquote>
<ol>
<li>将训练集随机分为两部分，比如选择70%的数据作为S<sub>train</sub>，剩下的30%数据作为S<sub>cv</sub></li>
<li>使用S<sub>train</sub>训练每一个M<sub>i</sub>，获得对应的假设函数h<sub>i</sub></li>
<li>在S<sub>cv</sub>上测试每一个h<sub>i</sub>，计算相应的训练误差εˆ<sub>S<sub>cv</sub></sub>(h<sub>i</sub>)，并选择训练误差最小的假设函数</li>
</ol>
</blockquote>
<p>由于我们是在S<sub>train</sub>上训练的模型，并且在S<sub>cv</sub>上计算的训练误差，我们计算出的训练误差更接近于实际的泛化误差。通常用于测试的数据集S<sub>cv</sub>需要占到整个数据集的1/4到1/3之间，30%是典型值。</p>
<p>上述算法的第三步可以做一个改进，在选出最佳模型M<sub>i</sub>后，再在全部数据集上做一次训练。通常这样做是很有效的，因为训练数据越多，模型参数越准确。</p>
<p>简单交叉验证算法的缺点是它“浪费”了30%的训练数据，尤其当训练数据本身就很稀少的情况下，去除了30%后剩下的就更少了。</p>
<p>下面的算法优化了数据的利用率，这个算法叫做<strong>k重交叉验证</strong>(k-fold cross validation)。该算法描述如下：</p>
<blockquote>
<ol>
<li>将训练集随机分为k个不相交的子集，记为S<sub>1</sub>, …, S<sub>k</sub></li>
<li>对每一个M<sub>i</sub>，对每一个j=1, …, k，使用集合S<sub>1</sub>, …, S<sub>j-1</sub>, S<sub>j+1</sub>, …, S<sub>k</sub>（也就是从训练集中去除S<sub>j</sub>）进行训练，获得对应的假设函数h<sub>ij</sub>，并在S<sub>j</sub>上测试每一个h<sub>ij</sub>，计算相应的训练误差εˆ<sub>S<sub>j</sub></sub>(h<sub>ij</sub>)，并选择训练误差最小的假设函数<br>M<sub>i</sub>的近似泛化误差取所有εˆ<sub>S<sub>j</sub></sub>(h<sub>ij</sub>)的平均值</li>
<li>选择近似泛化误差最小的模型M<sub>i</sub>，然后使用全部训练集S再做一次训练，获得对应的假设函数h<sub>i</sub></li>
</ol>
</blockquote>
<p>上述算法通常取k = 10。当训练数据非常稀少时，我们可以选取k = m，这意味着每次只留下一份数据用于测试，用尽可能多的数据用于训练，这种方法也被称为<strong>留一交叉验证</strong>(leave-one-out cross validation)。上面两种算法有效地利用了数据集，但缺点是增加了更多计算的次数。</p>
<p>上面我们介绍了很多交叉验证的算法用于在多个模型中选择一个最佳模型，实际上也可以用于对单个模型进行评估。比如你发明了一个新的学习算法，可以通过交叉验证计算测试集的训练误差是否合理来评估算法预测的质量。</p>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>模型选择的一个特殊并且重要的场景是<strong>特征选择</strong>(feature selection)。假设一个监督学习问题中的特征数n非常大，远远大于训练样本数m，但是我们怀疑其中只有一部分特征和学习问题相关，那么我们如何从中选择出我们希望的特征呢？</p>
<p>给定n个特征，我们可以有2<sup>n</sup>种可能的模型(每个特征要么出现在模型中，要么不出现在模型中)。这样我们就将特征选择问题转化为规模为2<sup>n</sup>的模型选择问题。但是这样的计算量太大了，因此我们需要用一些启发式搜索方法进行特征选择。下面的这个算法称为<strong>前向搜索</strong>(forward search)：</p>
<blockquote>
<ol>
<li>初始化特征集合F为空</li>
<li>不断循环如下步骤，直到F的长度达到预设要求：<br> (a). 将i从1到n进行遍历，如果i不在F中，令F<sub>i</sub> = F ∪ {i}，然后用某种交叉验证算法计算F<sub>i</sub>的误差<br> (b). 选择误差最小的F<sub>i</sub>，并更新为F</li>
<li>输出最佳特征集合F</li>
</ol>
</blockquote>
<p>上述算法属于<strong>封装模型特征选择</strong>(wrapper model feature selection)。前向搜索是指每次循环都是从剩余未被选中的特征中选出一个最好的特征加到特征集合中。与之相对的还有<strong>后向搜索</strong>(backward search)，初始化特征集合F = {1, …, n}，每次循环都是从现有特征集合中选出一个最差的特征，然后将其从特征集合中移除，循环直到F的长度达到预设要求后才结束。</p>
<p>封装模型特征选择可以很好地工作，但是计算量比较大，时间复杂度达到了O(n<sup>2</sup>)。</p>
<p><strong>过滤特征选择</strong>(filter feature selection)算法是另一个启发式搜索方法，拥有较低的计算复杂度。算法的主要思想是，对每一个特征x<sub>i</sub>，计算其与y的信息量S(i)，S(i)越大说明x<sub>i</sub>与y的相关性越强。计算完所有的S(i)后，选择前k个拥有最大S(i)的特征即可。</p>
<p>在实际应用中，S(i)通常选择为x<sub>i</sub>和y的<strong>互信息</strong>(mutual information)MI(x<sub>i</sub>, y)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5d225c49cf13c7e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上述公式中的几个概率值都可以在训练集上计算得到。为了对这个公式有直观的感受，我们可以将互信息表示成<strong>KL散度</strong>(Kullback-Leibler (KL) divergence)的形式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-eea347b56c31c6b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果x<sub>i</sub>和y互相独立，那么p(x<sub>i</sub>, y) = p(x<sub>i</sub>)p(y)，那么这两个概率分布的KL散度为0，因此x<sub>i</sub>和y是不相关的，对应的S(i)较小。相反地，如果x<sub>i</sub>和y的相关性越大，那么KL散度越大，对应的S(i)也较大。</p>
<p>还有一个细节需要注意，计算完所有的S(i)后，应该如何选择k值？一个标准做法是使用交叉验证法来选择k值，不过这次的时间复杂度是线性的。</p>
<h2 id="贝叶斯统计与规则化"><a href="#贝叶斯统计与规则化" class="headerlink" title="贝叶斯统计与规则化"></a>贝叶斯统计与规则化</h2><p>这一节我们再介绍一种可以减少过拟合发生的方法。</p>
<p>回顾之前我们在逻辑回归中使用了<strong>最大似然估计</strong>(maximum likelihood (ML))法来拟合参数，其公式为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5bd6ffc8150e243f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在上式中，我们把θ视为一个未知的常数，这一观点被认为是<strong>频率学派</strong>(frequentist)的观点。在频率学派的观点下，θ不是随机的，只是一个未知的变量，我们的任务就是通过某些方法估计出θ的值。</p>
<p>另一种看待θ的观点来自<strong>贝叶斯学派</strong>(Bayesian)。贝叶斯学派认为θ是随机的未知的变量，因而首先为θ指定一个<strong>先验概率</strong>(prior distribution)p(θ)。给定一个训练集S = {x<sup>(i)</sup>, y<sup>(i)</sup>}<sub>i=1, …, m</sub>，我们可以计算出θ的<strong>后验概率</strong>(posterior distribution)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1f17e7dc8cd128fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的p(y<sup>(i)</sup>|x<sup>(i)</sup>, θ)的计算取决于我们使用的具体模型。比如在贝叶斯逻辑回归模型中，</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-25f75066b252e6d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b324f798778a020a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对于一个新的输入x，我们可以用下面的公式进行预测：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d7e725ca66571ada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果我们要求期望值的话，可以用如下公式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7856a3b61acf3604.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们可以把上面步骤理解为“全贝叶斯预测”，因为我们在求后验概率时是对θ所有的值都计算了一遍，遗憾的是通常我们很难计算出这样的后验概率，因为对所有θ(通常是高维的)求积分是非常困难的。</p>
<p>因此，在实际应用中，我们需要对后验概率作一个近似估计，通常用单个θ的取值来替代整个后验概率。<strong>最大后验概率估计</strong>(maximum a posteriori(MAP))方法就采用了这个思想，其估计公式为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7c922de4b1115389.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>可以发现，θ<sub>MAP</sub>和θ<sub>ML</sub>的公式非常相似，除了在最大后验概率估计公式后面多了一项θ的先验概率。</p>
<p>在实际应用中，我们通常让θ服从高斯分布N(0,τ<sup>2</sup>I)。确定了这样的先验概率后，通常θ<sub>MAP</sub>比θ<sub>ML</sub>的拟合效果更好，出现过拟合的概率更低。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>模型选择的常用方法是交叉验证，交叉验证具体又分为简单交叉验证、k重交叉验证和留一交叉验证等方法</li>
<li>特征选择的常用方法有封装模型特征选择和过滤特征选择。封装模型特征选择分为前向搜索和后向搜索，时间复杂度都为O(n<sup>2</sup>)；过滤特征选择的时间复杂度为O(n)</li>
<li>在参数拟合问题上有频率学派和贝叶斯学派两种观点。频率学派认为θ是固定的未知的变量，使用最大似然估计法；贝叶斯学派认为θ是随机的未知的变量，使用最大后验概率估计法；最大后验概率估计法拟合出来的参数通常拟合效果更好，出现过拟合的概率更低</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes5.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/U/O/M6SGF6VB4_M6SGJURUO.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;假设我们为了某个学习问题需要从若干模型中选出最优模型，比如在多项式回归模型h&lt;sub&gt;θ&lt;/sub&gt;(x) = g(θ&lt;sub&gt;0&lt;/sub&gt;+θ&lt;sub&gt;1&lt;/sub&gt;x+…+θ&lt;sub&gt;k&lt;/sub&gt;x&lt;sup&gt;k&lt;/sup&gt;)中，我们应该如何选择合适的k值使得模型的偏
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记8: 经验风险最小化</title>
    <link href="http://www.secondplayer.top/2018/05/27/machine-learning-erm/"/>
    <id>http://www.secondplayer.top/2018/05/27/machine-learning-erm/</id>
    <published>2018-05-27T12:20:49.000Z</published>
    <updated>2018-05-27T12:20:49.412Z</updated>
    
    <content type="html"><![CDATA[<h2 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h2><p>当讨论线性回归时，我们提到过欠拟合和过拟合的问题。如下图所示，左图用的是y=θ<sub>0</sub>+θ<sub>1</sub>x的线性模型进行拟合，而右图用了更为复杂的多项式模型y=θ<sub>0</sub>+θ<sub>1</sub>x+…+θ<sub>5</sub>x<sup>5</sup>进行拟合。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-86d3ce4a3204faa5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>使用五阶多项式模型进行拟合并不会得到一个很好的模型，因为即使在训练集上表现很好，我们不能保证在新的数据集上也能做出很好的预测。换句话说，从已知的训练集中学习的经验并不能推广到新数据集上，由此产生的误差就叫做这个假设函数的<strong>泛化误差</strong>(generalization error)。稍后我们会给出泛化误差更正式的定义。</p>
<p>上图最左边和最右边的两个图形都有很大的泛化误差，但这两种误差不太相同。左图的模型过于简单，无法准确描述训练集的数据，这种误差我们称为<strong>偏差</strong>(bias)，我们称这个模型是<strong>欠拟合</strong>(underfit)的。右图的模型过于复杂，对训练集的数据过于敏感，这种误差我们称为<strong>方差</strong>(variance)，我们称这个模型是<strong>过拟合</strong>(overfit)的。</p>
<p>通常我们需要在偏差和方差中进行权衡。如果我们的模型过于简单，并且参数较少，那么这个模型通常会有较大的偏差，但是方差较小。相反如果我们的模型过于复杂，并且参数较多，那么这个模型通常会有较大的方差，但是偏差较小。在上面这个例子里，使用二阶函数进行拟合效果最好，在偏差和方差中获得了较好的权衡。</p>
<h2 id="经验风险最小化"><a href="#经验风险最小化" class="headerlink" title="经验风险最小化"></a>经验风险最小化</h2><p>这一节我们正式给出泛化误差的定义以及如何在偏差和方差中进行权衡。在此之前，我们先介绍两个简单但很有用的引理。</p>
<p><strong>联合边界</strong>(The union bound)引理：令A<sub>1</sub>, A<sub>2</sub>, …, A<sub>k</sub>为k个不同的随机事件，那么：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-42c0f8628fdae398.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在概率论里，联合边界通常被作为一个公理，因此也无需被证明，我们可以在直观上理解：k个随机事件中任何一个事件发生的概率至多是k个随机事件出现的概率总和。</p>
<p><strong>Hoeffding不等式</strong>(Hoeffding inequality)引理：令Z<sub>1</sub>, Z<sub>2</sub>, …, Z<sub>m</sub>为m个服从伯努利分布B(φ)的独立同分布事件，即P(Z<sub>i</sub> = 1) = φ，P(Z<sub>i</sub> = 0) = 1 - φ，令φ<sup>^</sup>=(1/m)Σ<sub>i∈[1,m]</sub>Z<sub>i</sub>为这些随机变量的平均值，对于任意的γ &gt; 0，有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7255aab010d7bd91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个引理告诉我们，如果用m次随机变量的平均值φ<sup>^</sup>作为对φ的估计，那么m越大，两者的误差就越小。这个引理和我们在概率论中学习的大数定理是符合的。</p>
<p>基于上面两个引理，我们可以推导出学习理论中最重要的一些结论。</p>
<p>为了方便讨论，我们将问题限制在二元分类问题上，推导出的结论同样也适用于其他分类问题。</p>
<p>假设我们有一个个数为m的训练集S = {(x<sup>(i)</sup>, y<sup>(i)</sup>); i = 1, …, m}，训练集的每个数据(x<sup>(i)</sup>, y<sup>(i)</sup>)都是服从概率分布为D的独立同分布。对于某个假设函数h，定义其<strong>训练误差</strong>(training error)，或者说<strong>经验风险</strong>(empirical risk)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7bfa5bd1702de3a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>训练误差描述的是假设函数h误判结果的比例。</p>
<p>另外，我们定义<strong>泛化误差</strong>(generalization error)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-0b9a281b3d1b7e1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>泛化误差描述的是假设函数h误判结果的概率。</p>
<p>考虑线性回归的例子，令假设函数h<sub>θ</sub>(x) = 1{θ<sup>T</sup>x &gt;= 0}，那么我们如何选取参数θ使得假设函数最优呢？一个方法就是使经验风险最小，即选取：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-36b054d2c6931e26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们把这个方法称为<strong>经验风险最小化</strong>(empirical risk minimization(ERM))，并把通过ERM产生的最优假设函数记为h<sup>^</sup>。</p>
<p>为了便于讨论，我们抛弃掉假设函数的参数化过程。定义<strong>假设函数类</strong>(hypothesis class)H是所有可能的假设函数的集合。比如对于线性分类，H就是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9352c7996f88975d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>现在ERM可以表示成如下的最优化问题：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-57c892dbf36edfaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="有限假设空间"><a href="#有限假设空间" class="headerlink" title="有限假设空间"></a>有限假设空间</h2><p>首先我们考虑假设空间有限的情况，即H = {h<sub>1</sub>, …, h<sub>k</sub>}，H里包含k个假设函数，我们需要从k个假设函数中挑选出经验风险最小的那个函数。</p>
<p>我们的证明分两部分。首先证明可以用泛化误差来近似替代训练误差，其次证明泛化误差是有上边界的。</p>
<p>对于某个假设函数h<sub>i</sub> ∈ H，考虑一个概率分布为D的伯努利分布随机事件Z = 1{h<sub>i</sub>(x) != y}，Z的泛化误差是其分布的期望值，而Z的训练误差是所有事件的平均值，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b81bcbc53b3b0589.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>根据Hoeffding不等式，可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-bea6335218358bb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这就证明了对于某个假设函数h<sub>i</sub>，只要m越大，泛化误差就越接近训练误差。接下来我们还要证明，对所有的假设函数h ∈ H，上述特性都成立。令事件A<sub>i</sub>为|ε(h<sub>i</sub>) - ε(h<sup>^</sup><sub>i</sub>)| &gt; γ，我们已经证明了P(A<sub>i</sub>) &lt;= 2exp(-2γ<sup>2</sup>m)。根据联合边界引理，我们可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b14db3867526082a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果我们把两边同时减去1，可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-52373eb77a35d65c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>也就是说，对所有的h ∈ H，泛化误差与训练误差相差γ的概率至少是1 - 2kexp(-2γ<sup>2</sup>m)，这个结论也被称为<strong>一致收敛</strong>(uniform convergence)。因此当m足够大时，我们可以用泛化误差来估计训练误差。</p>
<p>在刚才的讨论中，我们固定参数m和γ，给出P(|ε(h<sub>i</sub>) - ε(h<sup>^</sup><sub>i</sub>)| &gt; γ)的边界。这里我们感兴趣的是三个变量，m，γ以及一个误差的概率(后面用δ表示)。我们可以固定其中两个变量来求解另一个变量的边界。</p>
<p>如果给定γ和δ &gt; 0，m需要取多大来保证上式成立？用δ = 2kexp(-2γ<sup>2</sup>m)代入求解，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-97812272367ca1de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个式子告诉我们为了满足误差的概率成立，m需要达到的最小值，此时m也被称为<strong>样本的复杂度</strong>(sample complexity)。这个式子也告诉我们，为了满足条件，m只和k成<strong>对数相关</strong>(logarithmic)。</p>
<p>类似的，固定m和δ来求解γ，我们可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7b53c22b94e6a0b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>下面我们来证明泛化误差是有上边界的。定义h<sup>*</sup> = arg min<sub>h∈H</sub>ε(h)是假设空间H里的最优假设函数。对于H里的任意假设函数h<sup>^</sup>，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-dbe13d617dee0fec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的第一步和第三步都是利用了一致收敛的结论，因此我们证明了某个假设函数h的泛化误差比最优假设函数的泛化函数最多相差2γ。</p>
<p>将前面几个结论整合到一起可以形成如下定理：</p>
<p>令|H| = k，对于任意的固定参数m和δ，如果泛化误差与训练误差一致收敛的概率至少为1 - δ，那么有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ea50f0777c88f448.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个式子可以用来量化我们之前讨论的偏差方差权衡问题。当我们选择一个更大的假设空间时，等式右边第一项将会变小(因为有可能寻找到更优的假设函数)，而等式右边第二项将会变大(因为k增大了)，因而我们可以将第一项看成偏差，第二项看成方差。</p>
<p>这个定理还有一个推论：</p>
<p>令|H| = k，对于任意的固定参数γ和δ，为了使ε(h<sup>^</sup>) &lt;=  arg min<sub>h∈H</sub>ε(h) + 2γ的概率至少为1 - δ，那么m需满足：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-612ac633bdedca57.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="无限假设空间"><a href="#无限假设空间" class="headerlink" title="无限假设空间"></a>无限假设空间</h2><p>我们已经在有限假设空间的情况下证明了一些有用的定理了，那么在无限假设空间中是否有相似的定理呢？</p>
<p>在给出结论前，我们需要先介绍VC维的知识。</p>
<p>给定一个数据集S = {x<sup>(1)</sup>, … x<sup>(d)</sup>}，数据集的每个点x<sup>(i)</sup> ∈ X，如果H可以对S进行任意标记(<em>H can realize any labeling on S</em>)，那么我们称H可以<strong>打散</strong>(shatter)S。也就是说，对任意标记 {y<sup>(1)</sup>, … y<sup>(d)</sup>}，都存在h ∈ H使得h(x<sup>(i)</sup>) = y<sup>(i)</sup>, i = 1, …, d。</p>
<p>给定一个假设空间类H，我们定义H的<strong>VC维</strong>(Vapnik-Chervonenkis dimension)，记为VC(H)，是H可以打散的最大样本集合个数。如果H可以打散任意大的样本集合，那么VC(H) = ∞。</p>
<p>举例来说，考虑如下三个点的样本集合：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c20f76ae49de12b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果H为二维线性分类器的集合，即h(x) = 1{θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub>x<sub>2</sub> &gt;= 0}，那么H是可以打散这个集合的。具体来说对于如下八种可能的情况，都可以找到一个“零训练误差”的线性分类器，如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-92e2d06ea28eb380.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>可以证明上述假设空间类H不能打散包含4个点的样本集合，因此H最多能打散的集合大小是3，即VC(H) = 3。</p>
<p>需要说明的是，如果这些点的排列方式在一条直线上，那么H是无法打散S的。这种情况如下图所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-dd8ac938d3884978.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此我们说VC(H) = d，只需证明H可以打散大小为d的某一个集合S即可。</p>
<p>介绍完VC维后，我们可以给出如下定理：</p>
<p>给定一个假设空间类H，令d = VC(H)，如果一致收敛的概率至少为1 - δ，那么对任意的h ∈ H，有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1aed4028419a881b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>也就有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-ad191bbe4db08b78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这就证明了，如果H的VC维是有限的，那么一致收敛就是有效的，只要m越大，泛化误差与训练误差越接近。</p>
<p>这个定理同样还有一个推论：</p>
<p>对所有的h∈H，为了使ε(h<sup>^</sup>) &lt;=  arg min<sub>h∈H</sub>ε(h) + 2γ的概率至少为1 - δ，那么m需满足：m = O<sub>γ,δ</sub>(d)</p>
<p>这个推论告诉我们，训练集的数据大小与H的VC维成正比。而对大多数假设函数类，VC维的大小近似与参数的个数成正比。将上述两个结论结合起来，我们可以得出：训练集的数据大小通常近似与参数的个数成正比。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>如果模型比较简单并且参数比较少，那么这个模型通常会有较大的偏差和较小的方差；如果模型比较复杂并且参数比较多，那么这个模型通常会有较大的方差和较小的偏差</li>
<li>训练误差描述了假设函数误判结果的比例，泛化误差描述了假设函数误判结果的概率；当数据集足够大时，可以用泛化误差来估计训练误差</li>
<li>在假设空间里选择一个使训练误差最小的假设函数，这个过程叫做经验风险最小化(ERM)</li>
<li>如果假设空间有限，样本复杂度与假设空间大小的对数成正比(m = O<sub>γ,δ</sub>(log k))；如果假设空间无限，样本复杂度与假设空间的VC维成正比(m = O<sub>γ,δ</sub>(d))</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes4.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="http://open.163.com/movie/2008/1/F/H/M6SGF6VB4_M6SGJV3FH.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;偏差与方差&quot;&gt;&lt;a href=&quot;#偏差与方差&quot; class=&quot;headerlink&quot; title=&quot;偏差与方差&quot;&gt;&lt;/a&gt;偏差与方差&lt;/h2&gt;&lt;p&gt;当讨论线性回归时，我们提到过欠拟合和过拟合的问题。如下图所示，左图用的是y=θ&lt;sub&gt;0&lt;/sub&gt;+θ&lt;sub&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>HTTP/2 新特性浅析</title>
    <link href="http://www.secondplayer.top/2018/05/06/http-2-introduction/"/>
    <id>http://www.secondplayer.top/2018/05/06/http-2-introduction/</id>
    <published>2018-05-06T14:27:12.000Z</published>
    <updated>2018-05-06T14:27:12.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HTTP发展简史"><a href="#HTTP发展简史" class="headerlink" title="HTTP发展简史"></a>HTTP发展简史</h2><h3 id="HTTP-0-9"><a href="#HTTP-0-9" class="headerlink" title="HTTP/0.9"></a>HTTP/0.9</h3><ul>
<li>1991年发布</li>
<li>只支持GET命令</li>
<li>请求及返回值都是ASCII码</li>
<li>请求以换行符(CRLF)结束</li>
<li>返回值只支持HTML格式</li>
<li>服务器返回值之后立刻关闭连接</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">GET /index.html</div></pre></td></tr></table></figure>
<h3 id="HTTP-1-0"><a href="#HTTP-1-0" class="headerlink" title="HTTP/1.0"></a>HTTP/1.0</h3><ul>
<li>1996年发布</li>
<li>除了GET命令，还增加了POST和HEAD命令</li>
<li>服务器支持返回任意格式</li>
<li>请求和返回值除了包含数据部分，还增加头部信息(HTTP header)</li>
<li>返回值增加状态码(status code)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">GET / HTTP/1.0</div><div class="line">User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)</div><div class="line">Accept: */*</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">HTTP/1.0 200 OK </div><div class="line">Content-Type: text/plain</div><div class="line">Content-Length: 137582</div><div class="line">Expires: Thu, 05 Dec 1997 16:00:00 GMT</div><div class="line">Last-Modified: Wed, 5 August 1996 15:55:28 GMT</div><div class="line">Server: Apache 0.84</div><div class="line"></div><div class="line">&lt;html&gt;</div><div class="line">  &lt;body&gt;Hello World&lt;/body&gt;</div><div class="line">&lt;/html&gt;</div></pre></td></tr></table></figure>
<h3 id="HTTP-1-1"><a href="#HTTP-1-1" class="headerlink" title="HTTP/1.1"></a>HTTP/1.1</h3><ul>
<li>1997年发布</li>
<li>支持持久连接(Connection: keep-alive)，即TCP连接默认不关闭，可以被多个请求复用</li>
<li>支持管道机制(pipelining)，即一个TCP连接内可以同时发起多个请求</li>
<li>支持分块传输编码(chunked transfer encoding)</li>
<li>支持断点续传(Accept-Ranges)</li>
<li>支持更多命令，如PUT, PATCH, OPTIONS, DELETE</li>
<li>尽管支持复用TCP连接，但仍然会产生队头阻塞(Head-of-line blocking)问题</li>
</ul>
<h3 id="HTTP-2"><a href="#HTTP-2" class="headerlink" title="HTTP/2"></a>HTTP/2</h3><ul>
<li>2015年发布</li>
<li>2009年Google自行研发的SPDY在Chrome上验证成功后，被当作是HTTP/2的基础</li>
<li>完全采用二进制协议</li>
<li>支持多路复用(multiplexing)</li>
<li>支持头部压缩(header compression)</li>
<li>支持服务器推送(server push)</li>
</ul>
<h2 id="二进制协议"><a href="#二进制协议" class="headerlink" title="二进制协议"></a>二进制协议</h2><p>HTTP/2之前的协议都是基于ASCII码，好处是可读性好，容易上手。其缺点是可选的空格以及多变的终止符给识别帧造成了一些困难。采用二进制协议可以使得帧的识别更简单，并且传输信息更高效。其缺点是不便于调试，这就需要我们使用相应的工具来理解二进制的内容。</p>
<p>HTTP/2完全采用二进制协议，头信息和数据体都是二进制的，统称为<strong>帧</strong>(frame)。下图展示了同一个请求在HTTP/1.1和HTTP/2的对应关系，可见请求在HTTP/2中分为了两部分：头部帧和数据帧。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://hpbn.co/assets/diagrams/ae09920e853bee0b21be83f8e770ba01.svg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>一个帧的基本格式如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://hpbn.co/assets/diagrams/2a9b83c6b43e961a41a847c3227dcad2.svg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所有帧都由一个9字节的header和可变长的payload组成，各字段定义如下：</p>
<ul>
<li>Length: 表示payload的长度，payload的长度默认不能超过2<sup>14</sup> (16,384) ，除非修改<a href="https://httpwg.org/specs/rfc7540.html#SETTINGS_MAX_FRAME_SIZE" target="_blank" rel="external">SETTINGS_MAX_FRAME_SIZE</a>为更大的值</li>
<li>Type: 表示帧的类型，比如0x0表示数据帧，0x1表示头部帧，类型不在规范里定义的帧将会被丢弃</li>
<li>Flags: 对于不同类型的帧，这个字段有不同的含义，比如在数据帧里，0x1表示这个frame是流的最后一帧(END_STREAM)</li>
<li>R: 保留位，这一位必须置为0并且需要被忽略</li>
<li>Stream Identifier: 流ID，客户端发起的流ID必须是奇数的，服务端发起的流ID必须是偶数的</li>
</ul>
<h2 id="多路复用"><a href="#多路复用" class="headerlink" title="多路复用"></a>多路复用</h2><p>为了说明什么是多路复用，我们先需要明确下面几个概念：</p>
<ul>
<li><strong>流</strong>(stream): 已建立的连接内的双向字节流，可以承载一条或多条消息</li>
<li><strong>消息</strong>(message): 与逻辑请求或响应消息对应的完整的一系列帧</li>
<li><strong>帧</strong>(frame): HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://hpbn.co/assets/diagrams/8e6931bb40fc26c511ad15645e7b6113.svg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这些概念的关系总结如下：</p>
<ul>
<li>所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流</li>
<li>每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息</li>
<li>每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧</li>
<li>帧是最小的通信单位，承载着特定类型的数据，例如 HTTP Header、消息负载等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装</li>
</ul>
<p>在HTTP/1.1中，如果客户端为了提高性能想要在一个TCP连接内同时发起多个请求，每个请求必须按顺序被服务器依次响应，如果某一个请求特别耗时，那么后面的请求将会被一直阻塞。</p>
<p>而在HTTP/2中，如果在一个TCP连接内同时发起多个请求，每个消息可以被拆成互不依赖的帧并且各帧之间交错发送，然后在另一端重新把帧组装起来。这个特性就叫做多路复用。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://hpbn.co/assets/diagrams/47ba5b32e42cf5a06c3741d29ef9b94a.svg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图展示了同一个连接内并行的多个数据流。客户端正在向服务器传输一个数据帧(数据流5)，与此同时，服务器正向客户端交错发送数据流1和数据流3的一系列帧。因此，一个连接上同时有三个并行数据流。</p>
<h2 id="头部压缩"><a href="#头部压缩" class="headerlink" title="头部压缩"></a>头部压缩</h2><p>每个HTTP请求时都会承载一组表头。在HTTP/1.x中表头是以纯文本形式传输，通常需要500~800字节的开销，如果有cookie的话甚至会达到上千字节。为了减少这种开销并且提升性能，HTTP/2使用了<a href="https://httpwg.org/specs/rfc7541.html" target="_blank" rel="external">HPACK</a>算法进行压缩，具体来说包含了如下两种简单并强大的技术：</p>
<ul>
<li>头部字段使用静态Huffman编码，如果编码后使得字符反而变长了，那么不采用Huffman编码</li>
<li>客户端和服务器同时维护并更新一个索引表，如果传输的值在索引表里，那么使用索引值作为传输的值。索引表分为静态表和动态表两部分，静态表在规范里定义，包含了一些常用的字段，动态表初始为空，在连接过程中动态更新</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a5bb0efd97c33117.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如上图所示，最左边是原始的请求头，第一行的<code>:method GET</code>通过查找<a href="https://httpwg.org/specs/rfc7541.html#static.table.definition" target="_blank" rel="external">静态索引表</a>得到索引值为2，所以HPACK算法将其编码为2。最后第二行的<code>user-agent Mozilla/5.0 ...</code>不在静态索引表里，但在动态索引表里查到索引值为62，所以HPACK算法将其编码为62。最后一行的两个字段均未在索引表里查到，所以分别对其进行Huffman编码。</p>
<h2 id="服务器推送"><a href="#服务器推送" class="headerlink" title="服务器推送"></a>服务器推送</h2><p>HTTP/2新增的另一个强大的功能是允许服务器除了可以响应客户端请求，还可以向客户端推送额外的资源。</p>
<p>通常当我们请求一个网页时，客户端解析HTML源码，发现有js或css等其他静态资源，然后再发起请求下载静态资源。而实际上，当客户端请求网页后，服务器完全可以预判客户端接下来要请求相关的静态资源，那为什么不让服务器提前推送这些资源，从而减少额外的延迟时间呢？HTTP/2为此提出了服务器推送机制，服务器端可以通过发起<a href="https://httpwg.org/specs/rfc7540.html#PUSH_PROMISE" target="_blank" rel="external">PUSH_PROMISE</a>帧告知客户端，客户端收到服务器想要推送资源的意图后，可以决定是否接收推送。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://hpbn.co/assets/diagrams/d759887277b266a42c526643285dd244.svg" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>事实上，如果你在网页中内联CSS或Javascript，那么你已经体验过服务器推送了。使用HTTP/2，我们不仅可以获得相同效果，还可以获得更多的性能优势：</p>
<ul>
<li>客户端可以缓存推送资源</li>
<li>推送资源可以被不同页面重用</li>
<li>推送资源可以与其他资源复用</li>
<li>推送资源可以由服务器设定优先级</li>
<li>推送资源可以被客户端拒绝(非强制推送)</li>
</ul>
<p>服务器推送功能虽然很强大，但在实际使用中还需要考虑一些问题。第一个问题是如果客户端已经有缓存了，那么推送资源就是一种浪费。一种解决方法是只在用户第一次访问的时候推送资源。第二个问题是目前我们一般把静态资源放在CDN上，目前大部分CDN还不支持服务器推送，那么CDN和服务器推送到底哪个效果更好，这个可能还需要一些测试数据来做评判。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>Hypertext Transfer Protocol version 2 - <a href="https://httpwg.github.io/specs/rfc7540.html" target="_blank" rel="external">RFC7540</a></li>
<li>HPACK - Header Compression for HTTP/2 - <a href="https://httpwg.github.io/specs/rfc7541.html" target="_blank" rel="external">RFC7541</a></li>
<li>An Excerpt from <em>High Performance Browser Networking</em>: <a href="https://hpbn.co/http2/" target="_blank" rel="external">HTTP/2</a></li>
<li><a href="https://http2.github.io/" target="_blank" rel="external">Homepage for HTTP/2</a></li>
<li><a href="https://developers.google.com/web/fundamentals/performance/http2" target="_blank" rel="external">Introduction to HTTP/2</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2016/08/http.html" target="_blank" rel="external">HTTP 协议入门</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2018/03/http2_server_push.html" target="_blank" rel="external">HTTP/2 服务器推送（Server Push）教程</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;HTTP发展简史&quot;&gt;&lt;a href=&quot;#HTTP发展简史&quot; class=&quot;headerlink&quot; title=&quot;HTTP发展简史&quot;&gt;&lt;/a&gt;HTTP发展简史&lt;/h2&gt;&lt;h3 id=&quot;HTTP-0-9&quot;&gt;&lt;a href=&quot;#HTTP-0-9&quot; class=&quot;head
    
    </summary>
    
    
      <category term="HTTP/2" scheme="http://www.secondplayer.top/tags/HTTP-2/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记7: 支持向量机(下)</title>
    <link href="http://www.secondplayer.top/2018/04/30/machine-learning-svm-part-two/"/>
    <id>http://www.secondplayer.top/2018/04/30/machine-learning-svm-part-two/</id>
    <published>2018-04-30T12:39:28.000Z</published>
    <updated>2018-04-30T12:46:01.889Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章中我们已经根据拉格朗日对偶性推导出了SVM最优化公式。而在这一篇文章中，我们将会从SVM最优化公式中引出<strong>核函数</strong>(kernels)的概念，由此给出在高维空间下更高效应用SVM的算法，然后利用正则化解决线性不可分与异常点的问题，最后介绍用于高效实现SVM的<strong>序列最小优化</strong>(sequential minimal optimization)算法。</p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>在线性回归的问题中，我们曾举过预测房价的例子，输入特征x是住房面积。假设我们为了提高预测准确性，希望用x<sup>2</sup>，x<sup>3</sup>作为特征来建模。为了区别这两类变量，我们把原始的输入变量称为<strong>属性</strong>(attribute)，对原始变量映射后的项叫做<strong>特征</strong>(feature)。定义φ为<strong>特征映射</strong>(feature mapping)函数，在这个例子中，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-c13233913129da02.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>为了应用SVM算法，我们需要将算法中出现x的地方替换成φ(x)。由于算法可以被完全写成向量内积的形式<x, z="">，这意味着我们可以将其替换为&lt;φ(x), φ(z)&gt;。给定一个特征映射函数，我们定义<strong>核函数</strong>(kernels)为：</x,></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-05259e57b49c6150.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此，在算法中我们可以把<x, z="">都替换成K(x, z)。给定φ，我们通过求φ(x)和φ(z)的内积来计算K(x, z)。有趣的是，即使φ(x)可能因为维度较高导致计算起来比较耗时，而计算K(x, z)并不是很耗时。在这种情况下，通过在算法中引入K(x, z)，可以使得SVM算法的计算量大大减少。</x,></p>
<p>我们来举个例子看一下，假设</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6c74ef39e35acdba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们可以计算出：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a2e6188faa3f6b14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对比K(x, z)的定义，可得到特征映射函数φ为(当n=3时)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f80f518523d72047.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>可见计算φ(x)的时间复杂度是O(n<sup>2</sup>)，而计算K(x, z)的时间复杂度是O(n)。</p>
<p>再考虑一个例子，假设</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a0b575380c3efeb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对比K(x, z)的定义，可得到特征映射函数φ为(当n=3时)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-21c0bf270d1412e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>推广到更一般的形式，假设K(x, z) = (x<sup>T</sup>z + c)<sup>d</sup>，计算φ(x)的时间复杂度是O(n<sup>d</sup>)，而计算K(x, z)的时间复杂度仍旧是O(n)。当维度较高时，核函数的优势更加明显。</p>
<p>另一个常用的核函数是<strong>高斯核</strong>(Gaussian kernel)，其特征映射函数φ可以映射到无限维。高斯核函数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-8c4c2471c88f0967.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们接下来的一个问题就是给定一个函数K，它是否能成为一个合法的核函数，也就是说是否存在一个映射函数φ使得K(x, z) = &lt;φ(x), φ(z)&gt;? </p>
<p>假设K是一个合法的核函数，对于一个包含有限个点的集合{x<sup>(1)</sup>, x<sup>(2)</sup>, …, x<sup>(m)</sup>}，定义<strong>核矩阵</strong>(Kernel matrix)K，矩阵的每个元素K<sub>ij</sub> = K(x<sup>(i)</sup>, x<sup>(j)</sup>)。注意由于核函数和核矩阵的关系密切，我们使用了相同的符号K来表示它们。</p>
<p>当K是合法的核函数时，可证明K<sub>ij</sub> = K<sub>ji</sub>，因此K是对称矩阵。此外，定义φ<sub>k</sub>(x)表示向量φ(x)的第k个元素，我们也能证明：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-893a75241ac69a7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>综上我们可得出结论，如果K是合法的核函数，那么对应的核矩阵K是<strong>对称半正定</strong>(symmetric positive semidefinite)矩阵。这个结论反过来也成立，即“K是合法的核函数”是“核矩阵K是对称半正定矩阵”的充分必要条件，这个结论被称为<strong>Mercer定理</strong>(Mercer Theorem)。</p>
<p>核函数在机器学习中有广泛的应用。比如在数字识别问题中，我们需要根据一张图片(16*16像素)识别出数字(0-9)。如果把每个像素作为特征值，那么会有256个特征值，使用核函数(K(x, z) = (x<sup>T</sup>z)<sup>d</sup>或者高斯核)后可以使SVM的性能大大提升。</p>
<h2 id="正则化与线性不可分的情况"><a href="#正则化与线性不可分的情况" class="headerlink" title="正则化与线性不可分的情况"></a>正则化与线性不可分的情况</h2><p>到目前为止，我们在推导SVM过程中都是基于“数据是线性可分的”这个假设。尽管用函数φ将特征映射到高维可以增加数据线性可分的可能性，但这个假设不能保证总是成立。此外，还有一种情况是如果数据里有<strong>异常点</strong>(outlier)，那么得到的超平面可能并不是我们想要的结果。比如左下图显示了一个最优超平面，右下图里增加了一个异常点使得最优超平面的间隔变小了，影响了分类器的性能。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-eab251efa23c1d18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>为了使SVM算法在线性不可分的情况下正常工作，并且对异常点不那么敏感，我们采用<strong>l1正则化</strong>(l1 regularization)方法修改优化目标为： </p>
<p><img src="https://upload-images.jianshu.io/upload_images/2245716-b66c3dfc0b3f73cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>通过增加ξ<sub>i</sub>项，我们允许函数间隔小于1，并且当函数间隔小于1的时候，我们在目标函数增加Cξ<sub>i</sub>的代价。</p>
<p>同样，我们对该问题构造拉格朗日算子：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-95bfcba57166ba50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中α<sub>i</sub>和r<sub>i</sub>是拉格朗日乘数。我们不详细展开推导了，最后我们可以得到对偶问题为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-47692cc3a15b82a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过求解该最大化问题可以得到α<sup>*</sup>，然后代入回原始问题可得到其他参数。另外由于采用了l1正则化，原来α<sub>i</sub> &gt;= 0的限制变成了 0 &lt;= α<sub>i</sub> &lt;= C。此外，该问题的KKT对偶互补条件为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a95b5e7847f4eb83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>最后我们还剩下一个问题没有讲，那就是如何求解最大化W(α)的方法，这个是我们下面要介绍的内容。</p>
<h2 id="坐标上升算法"><a href="#坐标上升算法" class="headerlink" title="坐标上升算法"></a>坐标上升算法</h2><p>首先我们考虑没有任何约束条件的优化问题：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a77dc740f51cc305.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中W是以α为参数的函数。之前在优化问题中我们介绍过梯度下降法和牛顿方法，在这个问题里我们使用<strong>坐标上升</strong>(coordinate ascent)算法。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-138f74c6b2f226df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>算法的核心思想是，每次固定一个参数α<sub>i</sub>，然后使W关于α<sub>i</sub>作优化调整。在这个算法里，我们依次选取α<sub>1</sub>到α<sub>m</sub>作优化，然后不断循环直到结果收敛为止。这个算法的一个优化方向是，调整α<sub>i</sub>参数选取的顺序，每次选择使得W(α)增加幅度最大的α<sub>i</sub>作为下一个参数。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f90a8e77cb13a78d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图是一个优化二次函数的等高线示意图。坐标初始点在(-2, 2)，可以看到每次优化只在一个维度方向上进行优化。</p>
<h2 id="序列最小优化算法"><a href="#序列最小优化算法" class="headerlink" title="序列最小优化算法"></a>序列最小优化算法</h2><p>回到我们SVM的对偶问题上来，我们要求解的优化问题为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-17ff5cd08b11019d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>利用坐标上升算法的思想，我们每次固定一个参数α<sub>i</sub>进行优化是否可行？答案是不可行，假设我们固定α<sub>1</sub>，根据约束条件(19)，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f3afdff51d8224b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>两边都乘以y<sup>(1)</sup>，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b4d05b184ebd2b41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所以α<sub>1</sub>受到其余α<sub>i</sub>参数的控制，当α<sub>2</sub>, …, α<sub>m</sub>不变时，α<sub>1</sub>也不会改变。</p>
<p>因此如果我们要使用坐标上升算法的话，需要一次更新两个参数。具体来说就是，每次选取两个参数α<sub>i</sub>和α<sub>j</sub>进行更新，使W关于α<sub>i</sub>和α<sub>j</sub>作优化调整，然后不断循环直到结果收敛为止。</p>
<p>这个算法被称为<strong>序列最小优化</strong>(sequential minimal optimization, SMO)算法，最早由<a href="https://en.wikipedia.org/wiki/John_Platt_%28computer_scientist%29" target="_blank" rel="external">John Platt</a>提出，相关论文可以在<a href="https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/" target="_blank" rel="external">这里</a>找到。</p>
<p>为了判断算法的收敛性，我们可以检查KKT条件是否满足tol参数，具体方法在John Platt关于SMO的论文里有阐述。</p>
<p>SMO算法之所以高效就在于每次更新两个参数α<sub>i</sub>和α<sub>j</sub>的操作本身就很高效，我们具体来描述一下。</p>
<p>根据等式(19)，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9a09217189f988c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>既然等式右边的参数是固定的，我们就用一个常量ζ来表示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-bb384100c09a9ee0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们可以把α<sub>1</sub>和α<sub>2</sub>的约束条件画成下面的图。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f7f5b44e54e98008.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>根据约束条件(18)，α<sub>1</sub>和α<sub>2</sub>必须限制在[0,C]×[0,C]的方块内。另外α<sub>1</sub>和α<sub>2</sub>必须满足图中直线的约束。α<sub>2</sub>要满足L ≤ α<sub>2</sub> ≤ H的条件，其中L, H是α<sub>2</sub>的上下边界。</p>
<p>根据等式(20)，我们把α<sub>1</sub>写成是关于α<sub>2</sub>的函数:</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-fd39699dbc777c6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>那么目标函数W可以表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-991e01e70893e605.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>把α<sub>3</sub>, …, α<sub>m</sub>视为常量，可以看出W是关于α<sub>2</sub>的二次函数。如果我们不考虑限制条件(18)，那么通过求导就可以求出使这个二次函数最大化的参数，记为α<sub>2</sub><sup>new, unclipped</sup>。再把限制条件(18)考虑进来，我们可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-46f5629d2ed03e87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>最后，根据α<sub>2</sub><sup>new</sup>的值以及等式(20)，可以求得α<sub>1</sub><sup>new</sup>的值。</p>
<p>关于这个算法还有其他一些细节，但我们不会在这里一一阐述，有兴趣的读者可以自行查看Platt的论文。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>引入核函数的概念后，通过把数据映射到高维空间，有概率可解决原始空间中线性不可分的问题，并且可以使SVM算法更高效</li>
<li>Mercer定理：“一个核函数是合法的”是“核函数对应的核矩阵是对称半正定矩阵”的充分必要条件</li>
<li>为了使SVM算法在线性不可分的情况下正常工作，并且对异常点不那么敏感，可以采用l1正则化的方法</li>
<li>John Platt提出的SMO算法，可以高效求解SVM的对偶问题，其基本思想是利用了坐标上升算法</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/9/3/M6SGF6VB4_M6SGJVA93.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章中我们已经根据拉格朗日对偶性推导出了SVM最优化公式。而在这一篇文章中，我们将会从SVM最优化公式中引出&lt;strong&gt;核函数&lt;/strong&gt;(kernels)的概念，由此给出在高维空间下更高效应用SVM的算法，然后利用正则化解决线性不可分与异常点的问题，最后介
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记6: 支持向量机(上)</title>
    <link href="http://www.secondplayer.top/2018/04/15/machine-learning-svm-part-one/"/>
    <id>http://www.secondplayer.top/2018/04/15/machine-learning-svm-part-one/</id>
    <published>2018-04-15T13:06:29.000Z</published>
    <updated>2018-04-15T13:06:29.143Z</updated>
    
    <content type="html"><![CDATA[<p>在接下来的两篇文章里，我们会着重介绍<strong>支持向量机</strong>(Support Vector Machine)算法，以下简称为SVM。SVM可以称得上是监督学习里最优秀的算法了，在诸如文本分类，图像识别，生物序列分析等领域有很多的应用。</p>
<p>本篇文章首先介绍<strong>间隔</strong>(margin)的概念，然后给出<strong>最优间隔分类器</strong>(optimal margin classifier)的定义，同时将该问题转化为一个<strong>凸优化</strong>(convex optimization)问题，随后补充介绍<strong>拉格朗日对偶性</strong>(Lagrange duality)的知识，并由此推导出最优间隔分类器的对偶问题。</p>
<h2 id="符号表示"><a href="#符号表示" class="headerlink" title="符号表示"></a>符号表示</h2><p>为了使SVM的描述更简单，我们需要对之前的符号表示做一些修改。对于一个二元线性分类器，特征向量是x，输出分类是y。之前y的取值是0和1，现在我们把取值修改为-1和1。之前假设函数h(x)是以θ作为参数，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3e172ceb42d3b16f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>现在我们使用w和b作为参数，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9c449af3fcad64a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中，如果z &gt;= 0，那么g(z) = 1，否则g(z) = -1。用w和b表示可以显式地让我们把<strong>截距</strong>(intercept)b从其他参数中分离出来。对比两种h(x)的定义，b相当于原来的θ<sub>0</sub>，w相当于剩余的参数[θ<sub>1</sub>, θ<sub>2</sub>, …, θ<sub>n</sub>]<sup>T</sup>。</p>
<p>注意，由于g(z)的定义变了，现在我们的分类器可以直接输出1或者-1，而不是像逻辑回归那样先求出y = 1的概率，再根据概率预测输出。</p>
<h2 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h2><p>下图是一个线性分类器的图示，图中显示了训练集的两个分类数据(分别用圈和叉表示)，以及一个<strong>决策边界</strong>(decision boundary)将两类数据分割开。图中的直线是w<sup>T</sup>x + b = 0，也被称为<strong>超平面</strong>(hyperplane)。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f4820c61986462a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>图中有三个点A、B和C，其中A离超平面最远，C离超平面最近，因而我们认为A比C有更大的可能性属于叉的分类。那么，我们应该确定这个超平面呢？从直觉上看，这个超平面应该使每个点离超平面的间隔都尽可能大。</p>
<p>为了确定这个超平面，我们需要正式给出<strong>间隔</strong>(margin)的定义。对于训练数据(x<sup>(i)</sup>, y<sup>(i)</sup>)，我们定义它与超平面(w, b)的<strong>函数间隔</strong>(functional margin)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-8d78fb6349ab6fe6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果y<sup>(i)</sup> = 1，那么w<sup>T</sup>x + b必须是正数，并且w<sup>T</sup>x + b越大，函数间隔越大；相反地，如果y<sup>(i)</sup> = -1，那么w<sup>T</sup>x + b必须是负数，并且w<sup>T</sup>x + b越小，函数间隔越大。因而函数间隔越大，分类预测的可信度越高。</p>
<p>有了单个训练数据的函数间隔的定义，我们可以定义整个训练数据集的函数间隔是所有训练数据的函数间隔的最小值，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2dad8fa20f77ad3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>函数间隔虽然可以表示分类预测的可信度，但它有一个缺点：如果我们成比例地增大w和b(比如把w和b替换成2w和2b)，那么函数间隔的值也会成比例地增大，然而超平面本身并没有改变，因此这样是无意义的。直觉上看，我们需要对参数作一些约束，比如限制||w|| = 1，而这就引出了<strong>几何间隔</strong>(geometric margin)的定义。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1fe09c83c5576937.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>几何间隔实际上就是数据点到超平面的垂直距离，比如上图中点A的几何间隔就是A到超平面的距离AB，根据平面几何的知识可以求出数据点(x<sup>(i)</sup>, y<sup>(i)</sup>)到超平面(w, b)的距离为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b9590ee128858740.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式就是几何间隔的定义。可以看到，如果||w|| = 1，那么几何间隔就等于函数间隔。</p>
<p>另外如果我们任意缩放w和b，那么几何间隔并不会随之改变。</p>
<p>同样地，整个训练数据集的几何间隔是所有训练数据的几何间隔的最小值，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1713e46b91727d50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>一般地，函数间隔和几何间隔有如下关系：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-afef56dc7fce6076.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="最优间隔分类器的定义"><a href="#最优间隔分类器的定义" class="headerlink" title="最优间隔分类器的定义"></a>最优间隔分类器的定义</h2><p>上面我们讨论到，寻找最优超平面的条件应该使每个点离超平面的间隔都尽可能大，因而最优超平面也被称为<strong>最优间隔分类器</strong>(optimal margin classifier)。并且由于“任意缩放w和b，几何间隔并不会随之改变”，上述讨论中的间隔应该使用几何间隔，所以最优间隔分类器问题可以定义为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f186d4f00c9d6996.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个优化问题解决起来比较棘手，因为目标函数是非凸函数，我们没有办法用现成的软件可以解决这个问题。再次考虑“任意缩放w和b，几何间隔并不会随之改变”这个结论，我们可以对该问题增加一个约束：函数间隔等于1，并且最大化1 / ||w||等价于最小化||w||<sup>2</sup> ，那么问题被转化为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-e78d188fd8437fce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这样我们就把原始问题转化成了一个凸优化问题，而这个问题可以用现成的<strong>二次规划</strong>(quadratic programming)软件来求解。</p>
<h2 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h2><p>这一部分我们介绍<strong>拉格朗日对偶性</strong>(Lagrange duality)，其核心思想是通过拉格朗日对偶性可以将原始问题转化为对偶问题，而对偶问题通常比较容易求解。</p>
<p>考虑如下优化问题：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-082937072d74c9a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>即最小化函数f(w)，并且满足l个<strong>等式约束</strong>(equality constraint)条件h<sub>i</sub>(w) = 0。定义<strong>拉格朗日算子</strong>(Lagrangian)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-b4adc2919d108800.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>即等于原始目标函数加上约束函数的线性组合，其中β<sub>i</sub>是<strong>拉格朗日乘数</strong>(Lagrange multipliers)。分别求L关于w和β的偏导数即可求出原始问题的解：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2b84533d305ce1c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上述的优化问题只包含等式约束，而更广义的优化问题还包含<strong>不等式约束</strong>(equality constraint)。下面我们定义这个广义优化问题，也称为<strong>原始优化问题</strong>(primal optimization problem)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-cb6506241c838643.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>即最小化函数f(w)，并且满足l个等式约束条件h<sub>i</sub>(w) = 0和k个不等式约束条件g<sub>i</sub>(w) &lt;= 0。定义<strong>广义拉格朗日算子</strong>(generalized Lagrangian)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-abaa7c9f666cb2ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中α<sub>i</sub>和β<sub>i</sub>是拉格朗日乘数。定义：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-20a7c25ac380eff0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>下标p表示<strong>原始</strong>(primal)问题，可以证明：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-cbe10b87264ff19d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此当w满足原始约束条件时，原问题等价为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-cc2718c024ec2aa4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>为了后面表述方便，定义p<sup>*</sup>是原始问题的最优解：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-853c6cb2ca1ab982.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们再考虑另一个稍微不同的问题，定义：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-88cec11ed0f8721a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>下标D表示<strong>对偶</strong>(dual)问题，注意在原始问题中是关于参数α和β求最优解，而对偶问题是关于参数w求最优解。定义<strong>对偶优化问题</strong>(dual optimization problem)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-358247d3eed39b28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>和原始优化问题相比，对偶优化问题把max和min的顺序交换了一下。同样为了表述方便，定义d<sup>*</sup>是对偶问题的最优解：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-503d89909cb9d8bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>原始问题与对偶问题的关系如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9ae1ec2ac89afc71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>事实上一个函数的max min值总是小于等于它的min max值，这里我们不作证明。而在某些情况下，我们有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-cc349d6a203c2170.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这种情况下求解原始问题等价于求解对偶问题。下面我们不加证明地给出使得p<sup>*</sup> = d<sup>*</sup>成立的条件。</p>
<p>假设f和g<sub>i</sub>是凸函数，h<sub>i</sub>是<strong>仿射</strong>(affine)函数(即存在a<sub>i</sub>和b<sub>i</sub>使得h<sub>i</sub>(w) = a<sub>i</sub><sup>T</sup>w + b<sub>i</sub>)，并且g<sub>i</sub>是<strong>严格可执行的</strong>(strictly feasible)(即存在w使得对于所有g<sub>i</sub>(w)<0都成立)。在上述条件下，存在w<sup>*，α<sup>*</sup>，β<sup>*</sup>，其中w<sup>*</sup>是原始问题的解，α<sup>*</sup>，β<sup>*</sup>是对偶问题的解，并且满足：</0都成立)。在上述条件下，存在w<sup></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-39aee51676b87679.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>此外，w<sup>*</sup>，α<sup>*</sup>，β<sup>*</sup>同时也满足如下条件：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1d60b7baad3c970c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这些条件被称为<strong>KKT条件</strong>(Karush-Kuhn-Tucker (KKT) conditions)。相反地，如果w<sup>*</sup>，α<sup>*</sup>，β<sup>*</sup>满足KKT条件，那么它们能成为原始问题和对偶问题的解。</p>
<p>KKT条件中等式(5)也被称为<strong>KKT对偶互补条件</strong>(KKT dual complementarity condition)。这个条件表明如果α<sub>i</sub><sup>*</sup> &gt; 0，那么g<sub>i</sub>(w<sup>*</sup>) = 0。</p>
<h2 id="最优间隔分类器的推导"><a href="#最优间隔分类器的推导" class="headerlink" title="最优间隔分类器的推导"></a>最优间隔分类器的推导</h2><p>我们再次回到最优间隔分类器这个问题：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-adfc47913af3972b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>把约束条件写成如下形式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f869279692954eb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这里我们只有一个不等式约束条件。根据KKT对偶互补条件，当α<sub>i</sub> &gt; 0时，有g<sub>i</sub>(w<sup>*</sup>) = 0，即函数间隔等于1。考虑下图实例，其中实线表示这个训练集的最优间隔超平面。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-29bdd36f2b787d03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>训练集上拥有最小间隔的点也是最接近超平面的点，这样的点在上图虚线处总共有三个。因此这三个点对应的α<sub>i</sub>是非零的，而这三个点被称为该问题的<strong>支持向量</strong>(support vectors)。事实上，支持向量的数量在整个训练集的占比非常小。</p>
<p>接下来我们对原问题构造拉格朗日算子：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1f8e437f50f53617.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意，由于我们只有一个约束条件，所以这里只有α<sub>i</sub>一个乘数。</p>
<p>接着我们开始求解对偶问题，通过求L关于w的偏导数，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d8beda2601306e18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-259fe7c3baeafd9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过求L关于b的偏导数，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-d0bcafe9808ebb07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>将等式(9)代回到等式(8)中并利用等式(10)的结论，可以得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-77cbfbaa207ba317.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此对偶问题可以表述为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a22a26d4f45d667f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>可以证明该优化问题是满足KKT条件的，所以求解对偶问题等价于求解原始问题。通过求解该最大化问题可以得到α<sup>*</sup>，然后将其代入等式(9)可得w<sup>*</sup>，最后再代入回原始问题可得到b<sup>*</sup>，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9ac81b78ee861da1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>求解完模型各参数后，对于一个新特征x，我们需要计算w<sup>T</sup>x + b的值，如果值大于0，那么可以预测y = 1。根据等式(9)可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-efae530aa6b5fc57.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>从上式可以看出，当α<sub>i</sub>确定后，我们只需要逐个计算x和每个训练集的<strong>内积</strong>(inner product)。另外当训练集的点是支持向量时才有α<sub>i</sub>不等于0，并且支持向量只占训练集很少一部分，所以式子里的大多数项都是0，我们只需要计算x和支持向量的内积就可以了。</p>
<p>通过将问题表示成特征向量的内积形式，其实是为了引出核函数的概念，这个概念可以启发我们在高维空间下更高效应用SVM的算法，而这是我们下一章的内容。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>支持向量机(SVM)可以称得上是监督学习里最优秀的算法了，在诸如文本分类，图像识别，生物序列分析等领域有很多的应用</li>
<li>间隔描述的是点到超平面的距离，寻找一个超平面使得每个点的间隔最大，这个问题被称为最优间隔分类器</li>
<li>最优间隔分类器可以转化为凸优化问题，而这个问题可以用现成的二次规划软件来求解</li>
<li>当满足一定条件下，原始问题和对偶问题是等价的，通过求解最优间隔分类器的对偶问题，我们可以推导出SVM最优化公式</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/C/6/M6SGF6VB4_M6SGJVMC6.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在接下来的两篇文章里，我们会着重介绍&lt;strong&gt;支持向量机&lt;/strong&gt;(Support Vector Machine)算法，以下简称为SVM。SVM可以称得上是监督学习里最优秀的算法了，在诸如文本分类，图像识别，生物序列分析等领域有很多的应用。&lt;/p&gt;
&lt;p&gt;本篇
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记5: 朴素贝叶斯算法</title>
    <link href="http://www.secondplayer.top/2018/03/26/machine-learning-naive-bayes/"/>
    <id>http://www.secondplayer.top/2018/03/26/machine-learning-naive-bayes/</id>
    <published>2018-03-26T14:22:44.000Z</published>
    <updated>2018-03-26T14:22:44.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="朴素贝叶斯模型"><a href="#朴素贝叶斯模型" class="headerlink" title="朴素贝叶斯模型"></a>朴素贝叶斯模型</h2><p>在上节介绍的GDA方法中，输入特征x是连续型随机变量。现在我们介绍一个算法用于处理x是离散值的情况。</p>
<p>我们以邮件分类为例来介绍这个算法，邮件分类问题是<strong>文本分类</strong>(text classification)问题的一个子集。这里我们只考虑把邮件分为两类：<strong>垃圾邮件</strong>(spam email)和<strong>非垃圾邮件</strong>(non-spam email)。</p>
<p>我们把邮件中所有出现的单词的集合称为<strong>词汇表</strong>(vocabulary)。首先我们构造特征向量x<sub>i</sub>，x<sub>i</sub>的长度等于词汇表里单词的个数。如果词汇表中第i个单词出现在某封邮件中，那么x<sub>i</sub>=1，否则x<sub>i</sub>=0，比如：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-278ecdae24dc4b38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图表明这封邮件里包含a和buy这两个单词，但不包含aardvark，aardwolf，zygmurgy这三个单词。</p>
<p>构建完特征向量后，我们需要对p(x|y)进行建模。假设词汇表里有50000个单词，那么x是50000维的向量且每个元素在0和1内取值，这样可能的结果就有2<sup>50000</sup>种，如果用多项式分布建模的话，就会有2<sup>50000</sup> - 1个参数，这显然是个天文数字。</p>
<p>为了简化问题，我们就需要做一些假设。我们假设给定y的情况下，x<sub>i</sub>之间是条件独立的，这个假设称为<strong>朴素贝叶斯假设</strong>(Naive Bayes assumption)。举例来说，如果y=1表示垃圾邮件，buy是第2087个单词，price是第39831个单词，那么在已知该邮件是垃圾邮件的情况下，“buy出现在该邮件中”和“price出现在该邮件中”这两件事是互不相关的。用形式化的方法表述就是，p(x<sub>2087</sub>|y)=p(x<sub>2087</sub>|y, x<sub>39831</sub>)。注意，我们不是说x<sub>2087</sub>和x<sub>39831</sub>是互相独立的，而是说在给定y的情况下x<sub>2087</sub>和x<sub>39831</sub>是条件独立的。</p>
<p>因此我们可以作如下推导：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-f1084f541149a0e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中第一个等号基于条件概率链式法则，第二个等号基于朴素贝叶斯假设。需要说明的是，尽管朴素贝叶斯假设是个比较强的假设，但在实际问题中表现的效果很好。</p>
<p>我们的模型由φ<sub>i|y=1</sub> = p(x<sub>i</sub>=1|y=1)，φ<sub>i|y=0</sub> = p(x<sub>i</sub>=1|y=0)和φ<sub>y</sub> = p(y=1)这三个参数决定，其似然函数为</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2ff1092ca38b24a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过最大化似然函数，可以求得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-fb20c63b766fc5f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的∧表示逻辑上的<strong>与</strong>(and)。这个公式可以从直观上进行解释，比如φ<sub>i|y=1</sub>就是第j个单词出现在垃圾邮件中的次数除以垃圾邮件的总个数。</p>
<p>所有参数确定后，对一个新特征x作预测，我们可以计算出：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2c7bad7158e0d879.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>根据p(y=1|x)是否大于0.5来判断新邮件是否是垃圾邮件。</p>
<p>最后，尽管上面我们的x取值只是0或1，但实际上可以把它扩展到多个离散值的情况。另外即使x是连续取值的，我们可以通过<strong>离散化</strong>(discretize)，即按一定的区间将连续值映射到离散值，然后应用朴素贝叶斯算法。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-619370e241323f05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>比如对于房价预测问题，我们可以按照上表把住房面积按一定区间离散化，如果住房面积是890，那么对应的特征值x<sub>i</sub>就是3。一般来说，如果连续型随机变量不能用多元正态分布建模(不能使用GDA)，那么将其离散化并采用朴素贝叶斯建模是一个更好的算法。</p>
<h2 id="拉普拉斯平滑处理"><a href="#拉普拉斯平滑处理" class="headerlink" title="拉普拉斯平滑处理"></a>拉普拉斯平滑处理</h2><p>朴素贝叶斯算法对大多数问题都有很好的表现，但是我们还需要对其作一些修正使得它在文本分类问题中表现地更出色。</p>
<p>假设NIPS是词汇表里的第35000个单词，但是这个单词从未在训练数据中出现过，因此：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-51c57c8579d7d45d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所以预测一个包含单词NIPS的邮件是否为垃圾邮件，我们计算得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-8e145de435df88e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这是由于每一项乘积里都有p(x<sub>35000</sub>|y) = 0，因此分子分母都为0，这使得我们无法进行计算。</p>
<p>这个问题从广义上来讲就是，仅仅因为一个事件没有在训练集中出现就预测它的概率为0不是一个好主意。对于φ<sub>i</sub> = p(z=i)，之前根据最大似然估计的结果为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-70d0bc98a4a71744.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>为了避免某些φ<sub>j</sub>等于0，我们可以使用<strong>拉普拉斯平滑处理</strong>(Laplace smoothing)，修正参数如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-15e185fea6373944.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们在原参数基础上，分子上加了1，分母上加了k。注意修正之后，所有<br>φ<sub>j</sub>之和仍然为1(j从1到k取值)。并且所有的φ<sub>j</sub>都不为0，解决了之前的问题。</p>
<p>将拉普拉斯平滑处理代入到朴素贝叶斯算法，我们得到修正后的参数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-327545f82e7d02fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="文本分类的事件模型"><a href="#文本分类的事件模型" class="headerlink" title="文本分类的事件模型"></a>文本分类的事件模型</h2><p>朴素贝叶斯算法在很多文本分类问题中都表现地不错，但还有个与之相关的算法表现地更出色。</p>
<p>在文本分类的特定领域，朴素贝叶斯算法使用的是<strong>多元伯努利事件模型</strong>(multi-variate Bernoulli event model)。在该模型中，我们假设下一封邮件的发送方是随机的发送者(可能是垃圾邮件制造者或者是正常发件人)，然后发送方遍历整个字典，然后决定是否将单词i写到邮件中，每个单词i写入的概率p(x<sub>i</sub>=1|y) = φ<sub>i|y</sub>互相独立。因此，这封邮件出现的概率为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3f1148076dcf76dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>我们再介绍另一个模型，称之为<strong>多项式事件模型</strong>(multinomial event model)。这个模型引入了一套不同的符号和特征，x<sub>i</sub>表示邮件里第i个单词的在字典中的位置，x<sub>i</sub>在1到|V|中取值，|V|是词汇表(字典)的大小。一封由n个单词组成的邮件由长度为n的向量表示(x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>)。比如，某封邮件的开头为”A NIPS …”，那么x<sub>1</sub>=1(a是字典里第1个单词)，x<sub>2</sub>=35000(NIPS是字典里第35000个单词)。</p>
<p>在多项式事件模型中，我们仍随机选择发送者(和多元伯努利事件模型一样，概率是p(y))，然后发送者根据多项式分布决定第一个单词x<sub>1</sub>出来(概率是p(x<sub>1</sub>|y))，再以同样的方式决定后续的单词x<sub>2</sub>, …, x<sub>n</sub>，直到选出n个单词构成这封邮件。因此，在该模型下邮件出现的概率为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1bc6671c7f7aa694.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>注意这个公式和在多元伯努利事件模型下推导出来的公式非常类似，但实际上这里的每一项都表示不同的含义，尤其是x<sub>i</sub>|y现在服从多项式分布，而不是伯努利分布。</p>
<p>该模型的参数和之前一样，它们是 φ<sub>k|y=1</sub> = p(x<sub>j</sub>=k|y=1)，φ<sub>k|y=0</sub> = p(x<sub>j</sub>=k|y=0)和φ<sub>y</sub> = p(y)。注意对于任意的j，p(x<sub>j</sub>|y)的值都是一样的，也就是说单词在邮件中出现的位置与概率无关。</p>
<p>给定训练数据集(x<sup>(i)</sup>, y<sup>(i)</sup>)，其似然函数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-69fc6ef13bb476d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过最大化似然函数，可以求得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-84badcd78deb2d72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>应用拉普拉斯平滑，我们在分子上加1，在分母上加|V|，修正后的参数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-8e6d3f95572763ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>尽管朴素贝叶斯算法不是最好的分类算法，但它的效果却是惊人地好。由于它的简单和易于实现的特性，我们通常把它作为首选试验的算法。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>在分类问题中常用的分类算法是朴素贝叶斯算法，模型需要满足朴素贝叶斯假设，即给定y的情况下，x<sub>i</sub>之间是条件独立的</li>
<li>如果x是连续型随机变量，可以通过离散化后应用朴素贝叶斯算法；对于某些不能使用GDA建模的模型，该方法是一个更好的算法</li>
<li>为了解决朴素贝叶斯算法中某些参数为0导致预测失效，可以采用拉普拉斯平滑对参数修正</li>
<li>文本分类的两种事件模型：多元伯努利事件模型和多项式事件模型，后者通常效果更好</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes2.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 双语字幕视频 <a href="https://open.163.com/movie/2008/1/A/R/M6SGF6VB4_M6SGHMFAR.html" target="_blank" rel="external">上</a> <a href="https://open.163.com/movie/2008/1/7/H/M6SGF6VB4_M6SGJVV7H.html" target="_blank" rel="external">下</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;朴素贝叶斯模型&quot;&gt;&lt;a href=&quot;#朴素贝叶斯模型&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯模型&quot;&gt;&lt;/a&gt;朴素贝叶斯模型&lt;/h2&gt;&lt;p&gt;在上节介绍的GDA方法中，输入特征x是连续型随机变量。现在我们介绍一个算法用于处理x是离散值的情况。
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记4: 生成学习算法</title>
    <link href="http://www.secondplayer.top/2018/03/11/machine-learning-gla/"/>
    <id>http://www.secondplayer.top/2018/03/11/machine-learning-gla/</id>
    <published>2018-03-11T12:40:01.000Z</published>
    <updated>2018-03-11T12:40:01.527Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生成学习算法"><a href="#生成学习算法" class="headerlink" title="生成学习算法"></a>生成学习算法</h2><p>目前为止，我们主要讨论的学习算法基于p(y|x;θ)进行建模，即给定x的情况下y的条件分布。比如在逻辑回归里我们基于p(y|x;θ)推导出h<sub>θ</sub>(x)=g(θ<sup>T</sup>x)，其中g(z)是sigmoid函数。这次我们来介绍另一种类型的学习算法。</p>
<p>考虑这样一个分类问题，根据动物的某些特征用来区分该动物究竟是大象(y=1)还是狗(y=0)。之前的回归算法可能通过梯度上升算法求出一条直线，也就是<strong>决策边界</strong>(decision boundary)，来区分大象和狗。对于一个新的动物，看它落到直线的哪一边就能做出相应的预测。</p>
<p>另一种方法是，我们挑选出大象的数据，单独对大象进行建模；对狗也一样，单独对狗进行建模。对于一个新的动物，分别对大象的模型和狗的模型进行匹配，看哪个匹配得更像，进而做出相应的预测。</p>
<p>像逻辑回归这样对p(y|x)进行建模的算法称为<strong>判别学习算法</strong>(discriminative learning algorithms)。而这次我们介绍的对p(x|y)进行建模的算法称为<strong>生成学习算法</strong>(generative learning algorithms)。拿刚才的分类问题举例，如果y=1表示动物是大象，y=0表示动物是狗，那么p(x|y=0)就表示对狗的特征进行建模，p(x|y=1)就表示对大象的特征进行建模。</p>
<p>p(x|y)和p(y|x)的关系可以用<strong>贝叶斯规则</strong>(Bayes rule)描述：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6fa92a7f38e9a854.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由于p(x)是一个与y无关的值，所以为了让p(y|x)取最大值，可以忽略分母的值，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5e5bd43fb3f05a1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="多元正态分布"><a href="#多元正态分布" class="headerlink" title="多元正态分布"></a>多元正态分布</h2><p>在介绍下面的算法之前，我们先简单讨论一些<strong>多元正态分布</strong>(multivariate normal distribution)的知识。</p>
<p>多元正态分布，又称多元高斯分布，是一元高斯分布的在向量形式的推广。对一个<strong>均值</strong>(mean vector)为μ，<strong>协方差矩阵</strong>(covariance matrix)为Σ的n维多元正态分布，其概率密度函数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-eee56b9690a0df1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果一个随机变量X服从多元正态分布N(μ,Σ)，它的期望值由μ决定，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-7d2ce8ef7ffa740f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>协方差矩阵Cov(X)=E[(X-E(X))(X-E(X))<sup>T</sup>]=Σ。下面我们用几张图说明参数μ和Σ对分布的影响。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-6223798e1c84a852.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图最左面的图形表示一个μ=0，Σ=I(2*2的单位矩阵)的<strong>标准正态分布</strong>(standard normal distribution)。中间的图形表示的是μ=0，Σ=0.6I的正态分布。右边的图形表示的是μ=0，Σ=2I的正态分布。由此可见，Σ越大，图形更“扩散(spread-out)”，Σ越小，图形更“压缩(compressed)”。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-3fa75081ce7e496b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图的三个图形表示μ=0，Σ分别如下所示的正态分布：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-2c0f43a8942458c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由此可见，增加Σ非对角线(off-diagonal)上的值，图形向45度角方向上变得更“压缩(compressed)”了。</p>
<p>最后我们看下参数μ对图形的影响：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-5d96aff37fa449f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图的分布参数Σ=I，μ的值分别如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-afe9303dc322041a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>综上，μ的值决定了中心的位置，Σ的值决定了分布的幅度。</p>
<h2 id="高斯判别分析"><a href="#高斯判别分析" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h2><p>假设在我们的分类问题中，x是连续的随机变量，p(y)服从伯努利分布，p(x|y)服从多元正态分布，这样的模型称为<strong>高斯判别分析</strong>(Gaussian Discriminant Analysis, GDA)模型。具体来说：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-a3124533f0dee6c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>它们对应的概率分布为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-9478c2a38a87aad0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这里我们模型的参数有φ，Σ，μ<sub>0</sub>，μ<sub>1</sub>。注意，尽管两个多元正态分布有不同的均值μ<sub>0</sub>和μ<sub>1</sub>，但它们有相同的协方差矩阵Σ。这个模型的对数似然函数为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-56b810a9b3c71e3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通过最大化l，我们可以求得各参数如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-1208740e490781a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>下面我们用图形来更直观地理解一下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-0ab498c4e9033e61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上图给出了训练数据，以及两个高斯分布的等高线图。两个分布的形状相似(因为有相同的协方差矩阵Σ)，但位置不同(因为均值μ不同)。图中也给出了一个直线，表示p(y=1|x) = 0.5时的决策边界。在边界的一边我们认为y=1是概率最大的，而另一边我们认为y=0是概率最大的。</p>
<h2 id="高斯判别分析与逻辑回归的关系"><a href="#高斯判别分析与逻辑回归的关系" class="headerlink" title="高斯判别分析与逻辑回归的关系"></a>高斯判别分析与逻辑回归的关系</h2><p>GDA模型和逻辑回归之间的关系很有趣。如果我们把p(y=1|x;φ,Σ,μ<sub>0</sub>,μ<sub>1</sub>)看作是关于x的函数，我们可以将其表示成如下形式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://upload-images.jianshu.io/upload_images/2245716-768a56bad58d4d4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中θ可以写成关于φ,Σ,μ<sub>0</sub>,μ<sub>1</sub>的函数。上式正好是逻辑回归的表达形式。</p>
<p>如果p(x|y)服从多元正态分布，那么p(y|x)可表达成逻辑回归的形式。相反地，如果p(y|x)可表达成逻辑回归的形式，那么<strong>不代表</strong>p(x|y)服从多元正态分布。这说明GDA比逻辑回归需要更加严格的模型假设。当GDA模型假设成立时，GDA的拟合效果比逻辑回归更好；而当假设不成立时，逻辑回归的拟合效果更好。</p>
<p>另外在补充一点，如果p(x|y)服从指数分布族，那么p(y|x)也可表达成逻辑回归的形式。但是用GDA去拟合非高斯分布的数据，它的预测效果是不可捉摸的，效果可能好也可能不好。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>对p(y|x)进行建模的算法称为判别学习算法，例如逻辑回归；对p(x|y)进行建模的算法称为生成学习算法，例如高斯判别分析(GDA)</li>
<li>对一个均值为μ，协方差矩阵为Σ的多元正态分布，μ值决定了中心的位置，Σ值决定了分布的幅度</li>
<li>如果p(x|y)服从多元正态分布，那么p(y|x)可表达成逻辑回归的形式；相反地，如果p(y|x)可表达成逻辑回归的形式，那么不代表p(x|y)服从多元正态分布</li>
<li>GDA需要更加严格的模型假设，当假设成立时，GDA的拟合效果比逻辑回归好，否则逻辑回归的拟合效果更好；逻辑回归的模型假设相对弱一点，这使得它在实际应用中更普遍</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes2.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/A/R/M6SGF6VB4_M6SGHMFAR.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;生成学习算法&quot;&gt;&lt;a href=&quot;#生成学习算法&quot; class=&quot;headerlink&quot; title=&quot;生成学习算法&quot;&gt;&lt;/a&gt;生成学习算法&lt;/h2&gt;&lt;p&gt;目前为止，我们主要讨论的学习算法基于p(y|x;θ)进行建模，即给定x的情况下y的条件分布。比如在逻辑回归里
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记3: 广义线性模型</title>
    <link href="http://www.secondplayer.top/2018/02/13/machine-learning-glm/"/>
    <id>http://www.secondplayer.top/2018/02/13/machine-learning-glm/</id>
    <published>2018-02-13T08:33:18.000Z</published>
    <updated>2018-02-13T08:33:18.819Z</updated>
    
    <content type="html"><![CDATA[<h2 id="牛顿方法"><a href="#牛顿方法" class="headerlink" title="牛顿方法"></a>牛顿方法</h2><p>之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。</p>
<p>我们先来看下如何用<strong>牛顿方法</strong>(Newton’s Method)求解θ使得f(θ)=0。如下图所示，首先我们选取一个初始点，比如说令θ=4.5，然后作出f(θ)在该点的切线，这条切线与x轴相交的点θ=2.8作为下一次迭代的点。下右图又一次重复了一轮迭代，f(θ)在θ=2.8处的切线与x轴相交于θ=1.8处，然后再次迭代到θ=1.3处。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-e223eff5a326a176.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>以此类推，我们得到迭代规则如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-65bfb54bbcf3f406.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>牛顿方法可以找到θ使得f(θ)=0，那么如何把它应用到最大化l(θ)上呢？当l(θ)达到最大点时，其导数为0，因此问题转化为找到θ使得l’(θ)=0。所以，令f(θ)=l’(θ)，我们推导出迭代规则：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-610721634ffcbce7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的θ是参数为实数的情况，当θ为向量时，我们可以推导出更通用的公式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-86dd59153cd92cde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中∇<sub>θ</sub>l(θ)是指l(θ)的梯度，H是一个n <em> n的矩阵，被称为<em>*海森矩阵</em></em>(Hessian Matrix)。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-5da2b2bf72902d9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>和梯度下降法相比，牛顿方法收敛的速度更快，迭代的次数也更少。但是牛顿方法每次迭代的计算量更大，因为每次都要计算一个n阶矩阵的逆。总体而言，当n不是很大时牛顿方法计算的速度更快。当牛顿方法用来求解最大化对数似然函数l(θ)时，这个方法也被称为<strong>Fisher Scoring</strong>。</p>
<h2 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h2><p>到目前为止，我们分别学习了<strong>分类</strong>(classification)和<strong>回归</strong>(regression)两类问题。在回归问题里，我们假设p(y|x;θ)服从高斯分布N(0,σ<sup>2</sup>)；在分类问题里，我们假设p(y|x;θ)服从伯努利分布B(φ)。后面我们会看到，这两类问题可以被统一到一个更通用的模型，这个模型被称为<strong>广义线性模型</strong>(Generalized Linear Models, GLM)。在介绍GLM前，我们先引入一个概念：<strong>指数分布族</strong>(exponential family)。</p>
<p>指数分布族是指一类可以被表示为如下形式的概率分布：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-42f222888a89a09b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中η被称为分布的<strong>自然参数</strong>(natural parameter)，或者是<strong>标准参数</strong>(canonical parameter)；T(y)是<strong>充分统计量</strong>(sufficient statistic)，通常T(y)=y；a(η)是<strong>对数分割函数</strong>(log partition function)。e<sup>-a(η)</sup>通常起着归一化的作用，使得整个分布的总和/积分为1。</p>
<p>如果固定参数T, a, b，就定义了一个以η为参数的函数族。当η取不同的值，我们就得到一个不同的分布函数。</p>
<p>现在我们来证明<strong>高斯分布</strong>(Gaussian distribution)和<strong>伯努利分布</strong>(Bernoulli distribution)都属于指数分布族。</p>
<p>对于伯努利分布B(φ)，其y值为0或1，因而有p(y=1;φ)=φ; p(y=0;φ)=1-φ 。所以可推导p(y;φ)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-db162b1115ffbef7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对比指数分布族的定义，可得η=log(φ/(1-φ))，进而可得φ=1/(1+e<sup>-η</sup>)，而这正是sigmoid函数的定义。同样对比其他参数，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-79cea267be2c0318.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>综上可得，伯努利分布属于指数分布族，且φ的形式与sigmoid函数一致。</p>
<p>接下来我们继续来看高斯分布N(μ,σ<sup>2</sup>)。回忆下之前推导线性回归的时候，σ<sup>2</sup>的值与θ和h<sub>θ</sub>(x)无关，因此为了简化证明，我们令σ<sup>2</sup>=1，所以可推导p(y;μ)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-d8bc9a34458adf12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对比指数分布族的定义，进而可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-451c3a74a1da1157.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因而我们证明了高斯分布也属于指数分布族。事实上，大多数概率分布都属于指数分布族，我们列举一些如下：</p>
<ul>
<li><strong>多项式分布</strong>(Multinomial distribution)：对有k个离散结果的事件建模</li>
<li><strong>泊松分布</strong>(Poisson distribution)：描述单位时间内独立事件发生次数的概率</li>
<li><strong>伽马分布</strong>(Gamma distribution)与<strong>指数分布</strong>(Exponential distribution)：描述独立事件的时间间隔的概率</li>
<li><strong>β分布(Beta distribution)</strong>：在(0,1)区间的连续概率分布</li>
<li><strong>Dirichlet分布(Dirichlet distribution)</strong>：分布的分布(for distributions over probabilities)</li>
</ul>
<h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><p>介绍完指数分布族后，我们开始正式介绍广义线性模型(GLM)。对回归或者分类问题来说，我们都可以借助于广义线性模型进行预测。广义线性模型基于如下三个假设：</p>
<ul>
<li>假设1: p(y|x;θ) 服从以η为参数的指数分布族中的某个分布</li>
<li>假设2: 给定x，我们的目标是预测T(y)的期望值，大多数情况下T(y)=y，所以假设函数可以写为h(x)=E[T(y)|x]</li>
<li>假设3: η与x是线性相关的，即η=θ<sup>T</sup>x</li>
</ul>
<p>依据这三个假设，我们可以推导出一个非常优雅的学习算法，也就是GLM。接下来我们分别看几个通过GLM推导出来的算法。</p>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>假设p(y|x;θ)服从高斯分布N(μ,σ<sup>2</sup>)，我们可以推导如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-6158a49bfff73c84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中第一个等号来自假设2，第二个等号是高斯分布的特性，第三个等号<br>来自上一节中我们已经证明了η=μ，第四个等号来自假设3。</p>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>假设p(y|x;θ)服从伯努利分布B(φ)，我们可以推导如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-46fb19bd575608f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中第一个等号来自假设2，第二个等号是伯努利分布的特性，第三个等号<br>来自上一节中我们已经证明了φ=1/(1+e<sup>-η</sup>)，第四个等号来自假设3。</p>
<p>这里多介绍一些术语：将η与原始概率分布中的参数联系起来的函数g(即g(η)=E[T(y);η])称为<strong>标准响应函数</strong>(canonical response function)，它的逆函数g<sup>-1</sup>称为<strong>标准关联函数</strong>(canonical link function)。</p>
<h3 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h3><p>接下来我们来看一个更复杂的模型。在分类问题上，我们不止预测0和1两个值，假设我们预测的值有k个，即y∈{1,2,…,k}。那么我们就不能再使用伯努利分布了，我们考虑用<strong>多项式分布</strong>(Multinomial distribution)建模。</p>
<p>我们用φ<sub>1</sub>, φ<sub>2</sub>, … ,φ<sub>k</sub>表示每个结果出现的概率，即P(y=k)=φ<sub>k</sub>。由于所有结果概率之和为1，所以实际上k个参数中有1个是多余的，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-15c413b119e8e891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>为了使多项式分布能表示成指数分布族的形式，我们定义T(y)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-9a827bba15dd4d75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>和我们之前的例子不一样，T(y)这次不等于y，而是一个k-1维的向量。我们用(T(y))<sub>i</sub>表示T(y)的第i个元素。</p>
<p>接下来我们引入<strong>指示函数</strong>(indicator function)：1{·}。如果参数表达式为真，则指示函数取值为1；表达式为假，指示函数取值为0，即1{True} = 1, 1{False} = 0。基于上述定义，我们可以得到：(T(y))<sub>i</sub> = 1{y = i}，进一步可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-c59f55335a32a473.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>现在我们可以证明多项式分布也属于指数分布族，证明如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-461bd6d335040316.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由η的表达式，我们可以得到η和φ的对应关系：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-ca081bc1d3aa237d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-72434761f391890c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个从η和φ的映射函数被称为<strong>softmax函数</strong>(softmax function)。有了softmax函数并结合假设3，我们可以求出p(y|x;θ)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-d74f7c7d6b907b62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个k分类问题的算法被称为<strong>softmax回归</strong>(softmax regression)，它是逻辑回归更一般化的形式。</p>
<p>最后我们可以求出假设函数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-e31af89c3572377c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果要求解参数θ，我们可以先求出它的对数似然函数l(θ)，然后用梯度上升或牛顿方法进行迭代。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>梯度上升和牛顿方法都能用于求解最大化l(θ)的问题，区别是牛顿方法收敛速度更快，但是它每次迭代的计算量也更大，当数据规模不大时总体上性能更优</li>
<li>指数分布族描述了一大类我们常见的概率分布，高斯分布、伯努利分布、多项式分布等都属于指数分布族</li>
<li>广义线性模型(GLM)描述了一种更通用的学习模型，最小二乘法和逻辑回归都可以从GLM推导出来</li>
<li>k分类问题可以用softmax回归建模，逻辑回归可以看作是softmax回归的特例(k=2)</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/E/D/M6SGF6VB4_M6SGHKAED.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;牛顿方法&quot;&gt;&lt;a href=&quot;#牛顿方法&quot; class=&quot;headerlink&quot; title=&quot;牛顿方法&quot;&gt;&lt;/a&gt;牛顿方法&lt;/h2&gt;&lt;p&gt;之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。&lt;/p&gt;
&lt;p&gt;我们先来看下如何用&lt;stro
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记2: 欠拟合与过拟合</title>
    <link href="http://www.secondplayer.top/2018/01/29/machine-learning-underfitting-and-overfitting/"/>
    <id>http://www.secondplayer.top/2018/01/29/machine-learning-underfitting-and-overfitting/</id>
    <published>2018-01-29T14:55:48.000Z</published>
    <updated>2018-01-29T14:55:48.432Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性回归的概率解释"><a href="#线性回归的概率解释" class="headerlink" title="线性回归的概率解释"></a>线性回归的概率解释</h2><p>在解决线性回归问题时，我们为什么要使用最小二乘法作为代价函数？这个问题我们会通过概率统计来进行解释。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-55c71a4a1d633d7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="使用最小二乘法作为代价函数" title="">
                </div>
                <div class="image-caption">使用最小二乘法作为代价函数</div>
            </figure>
<p>假设对每个样本数据，输出值与预测值存在一定的误差ε<sup>(i)</sup>，误差可能来自未被建模的其他因素，也可能是随机的噪音。因而预测函数可写为</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-56433a18e2ab9045.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>另外我们假设误差属于<strong>独立同分布</strong>(independently and identically distributed)，并且服从高斯分布N(0,σ<sup>2</sup>)，所以ε<sup>(i)</sup>的概率密度函数为</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-4807496089e8a961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因此可推导出</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-0845db09df9e8962.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>P(y<sup>(i)</sup>|x<sup>(i)</sup>;θ)表示：在θ为给定的参数的情况下，概率y<sup>(i)</sup>以x<sup>(i)</sup>为随机变量的概率分布，注意θ不是随机变量。</p>
<p>给定X(输入矩阵)和θ，Y(输出矩阵)的分布记为p(Y|X;θ)，这个概率的值我们定义为以θ为变量的<strong>似然函数</strong>(likelihood function)</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-d386154759d3b75e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由于每个误差值是独立分布的，所以</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-66bb77823b288481.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>在θ作为参数的情况下，我们希望给定X时出现Y的概率是最大，因此问题变成最大化L(θ)。在求解最大化L(θ)的过程中，对L(θ)取对数将简化一些运算，因此我们最大化对数似然函数l(θ)：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-87470e39e25be670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上面的公式可以得出，最大化似然函数L(θ)等价于最小化代价函数J(θ)，这就是我们为什么取最小二乘法作为代价函数的原因。</p>
<h2 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h2><p>如下左图显示了用线性函数y=θ<sub>0</sub>+θ<sub>1</sub>x拟合数据集的结果，由于数据集并不是一条直线，因此拟合效果不太理想。如果我们增加一个特征项x<sup>2</sup>，即用y=θ<sub>0</sub>+θ<sub>1</sub>x+θ<sub>2</sub>x<sup>2</sup>拟合数据集，那么得到的结果如中间所示。粗看起来，增加更多的特征项可以使拟合效果更好，然而事实上并非如此。如果我们把特征项增加到6项，即y= Σ<sub>j∈[0,5]</sub>θ<sub>j</sub>x<sup>j</sup>，我们得到的结果如右图所示。尽管这个曲线完美拟合整个数据集，但是我们很难说它能准确预测未知的新数据。我们把左图这种情况称为<strong>欠拟合</strong>(underfitting)，就是说模型没有很好地捕捉到数据特征，不能够很好地拟合数据；右图这种情况称为<strong>过拟合</strong>(overfitting)，就是说模型把数据学习得太彻底，以至于不能很好地预测新的数据。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-66b4f132b66203f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="欠拟合与过拟合" title="">
                </div>
                <div class="image-caption">欠拟合与过拟合</div>
            </figure>
<p>这里我们介绍一个新的方法称为<strong>局部加权线性回归</strong>(locally weighted linear regression)，它可以弥补普通线性回归模型欠拟合或者过拟合的问题。假设我们要预测x这个点对应的值，局部加权线性回归对x附近的每一个点赋予一定的权重，离x越近权重越大，离x越远权重越小。通过赋予权重，使得x附近的点对结果影响最大，离x很远的点对结果的影响可以忽略不计。因此代价函数表示如下，其中w<sup>(i)</sup>表示权重。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-6e6f5dd843be7dc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由上述对权重特性的描述，w<sup>(i)</sup>的图像应该是个钟形曲线。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-ad864c94f7694643.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>通常我们定义w<sup>(i)</sup>的函数如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-0158d0c97cfac7d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的τ称为<strong>波长</strong>(bandwidth)，波长的大小取决了附近点的下降速率，参数根据对数据集的实验进行调整。</p>
<p>局部加权线性回归是一种<strong>非参数学习算法</strong>(non-parametric learning algorithm)，而之前我们学的普通线性回归是一种<strong>参数学习算法</strong>(parametric learning algorithm)。参数学习算法有固定的明确的参数，参数一旦确定，就不会改变了，我们不需要保留训练集中的训练样本。而非参数学习算法每进行一次预测，需要重新计算数据，因此需要保留训练数据。当训练数据较多时，非参数学习算法需要占用更多的存储空间。</p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>现在我们开始讨论<strong>分类</strong>(classification)问题。分类问题和回归问题很类似，只不过预测的y值从连续值变成了离散值。我们先从最简单的<strong>二分类</strong>(binary classification)问题开始讨论，此时y值只有0和1两个取值。0被称为<strong>负类</strong>(negative class)，1被称为<strong>正类</strong>(positive class)，有时也会用符号<code>-</code>和<code>+</code>标记。给定x<sup>(i)</sup>，对应的y<sup>(i)</sup>值也被称为训练集的<strong>标签</strong>(label)。</p>
<p>一个二分类的例子是，通过给定肿瘤的大小(x<sup>(i)</sup>)来预测是否为恶性肿瘤(y<sup>(i)</sup>)。我们先用之前线性回归的方法求解这个问题，如下图所示，从结果上看线性回归的效果并不好。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-b00a67c315f07b13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>直觉上看，h<sub>θ</sub>(x)的取值应该是介于0到1之间的。为了达到这一点，通常我们选取h<sub>θ</sub>(x)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-a8a80817260240a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中g(z)被称为<strong>逻辑函数</strong>(logistic function)或者<strong>sigmoid函数</strong>(sigmoid function)。它的图形如下所示：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-01eb10145c14b353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>g(z)的值域在0到1之间，当z趋向正无穷时，g(z)趋向于1；当z趋向负无穷是，g(z)趋向于0。g(z)还有另外一个有用的特性，g(z)对z的导数可以用其自身来表示，具体推导如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-1b58479be5233116.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>那么对于这样一个逻辑回归模型，我们如何选取θ进行拟合呢？套用之前极大似然估计的思想，我们为这个模型赋予一些概率假设：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-ad006cb4a5315ff5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这两个式子可以简化成一个：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-8cce75ae7e277b99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其似然函数L(θ)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-3c80a98aad0f0299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对数似然函数l(θ)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-0df7e539db557017.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如何最大化l(θ)呢？类似于在线性回归里求代价函数最小值是用的梯度下降法，我们可以用梯度上升法求函数的最大值。因此θ的每次迭代如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-773a75268974c40c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对l(θ)进行求导：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-f44dface9d4ad5d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>所以我们得到梯度上升法则：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-b8d55b4d7edc27bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个迭代规则和线性回归的最小均方算法(LMS)看上去非常类似，但它们并不是同一个算法，因为现在的h<sub>θ</sub>(x)是一个非线性函数。然而它们都拥有相似的形式，这究竟是巧合还是有更深层次的原因呢？这个我们后面会讲到。</p>
<h2 id="感知器学习算法"><a href="#感知器学习算法" class="headerlink" title="感知器学习算法"></a>感知器学习算法</h2><p>最后我们再简短地介绍一个新的算法。之前我们选取sigmoid函数作为h<sub>θ</sub>(x)，如果我们换成另外一个函数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-154bc8ee13075948.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>然后用同样的迭代规则：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-fa01e348ae445c08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这样我们得到的算法称为<strong>感知器学习算法</strong>(perceptron learning algorithm)。在上世纪60年代，<strong>感知器</strong>(perceptron)被认为是神经网络组成单元的一个粗糙的模型，这个我们后续会详细展开。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>线性回归的概率解释：最大化似然函数等价于最小化代价函数，这就是我们为什么取最小二乘法作为代价函数的原因</li>
<li>为了避免普通线性回归欠拟合和过拟合的问题，可以采用局部加权线性回归方法，通过赋予权重来强化离x近的点的结果，弱化离x远的点的结果</li>
<li>局部加权线性回归是一种非参数学习算法，普通线性回归是一种参数学习算法</li>
<li>二分类问题通常取h<sub>θ</sub>(x)为sigmoid函数，其迭代规则与线性回归的规则形式相似</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>Coursera机器学习课程讲义 <a href="https://www.coursera.org/learn/machine-learning/resources/Zi29t" target="_blank" rel="external">Week 3 Lecture Notes</a></li>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/E/B/M6SGF6VB4_M6SGHM4EB.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;线性回归的概率解释&quot;&gt;&lt;a href=&quot;#线性回归的概率解释&quot; class=&quot;headerlink&quot; title=&quot;线性回归的概率解释&quot;&gt;&lt;/a&gt;线性回归的概率解释&lt;/h2&gt;&lt;p&gt;在解决线性回归问题时，我们为什么要使用最小二乘法作为代价函数？这个问题我们会通过概率
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记1: 线性回归</title>
    <link href="http://www.secondplayer.top/2018/01/01/machine-learning-linear-regression/"/>
    <id>http://www.secondplayer.top/2018/01/01/machine-learning-linear-regression/</id>
    <published>2018-01-01T05:38:16.000Z</published>
    <updated>2018-01-01T05:38:16.483Z</updated>
    
    <content type="html"><![CDATA[<h2 id="监督学习与非监督学习"><a href="#监督学习与非监督学习" class="headerlink" title="监督学习与非监督学习"></a>监督学习与非监督学习</h2><p>机器学习是指给定一些训练数据，使机器能够利用它们分析未知数据。任何机器学习问题都可以分为两类：<strong>监督学习</strong>(Supervised Learning)和<strong>非监督学习</strong>(Unsupervised Learning)。这两类的区别在于：监督学习的训练数据有特征有标签，而非监督学习的训练数据没有。</p>
<p>监督学习问题一般是指给定输入预测输出，根据输出值的不同可以分为两类：<strong>回归</strong>(regression)和<strong>分类</strong>(classification)。回归预测的是连续值，分类预测的是离散值。</p>
<p>举例来说，给定房子的面积来预测房价是一个回归问题，因为房价是个连续值。如果把它改成预测房价是否超过某个阈值，那么这是一个离散问题，因为输出是个“是”或“否”的离散值。同理，给定一个人的图片预测TA的年龄是个回归问题，预测TA的性别是个分类问题。</p>
<p>而非监督学习问题在给定输入时，不知道预测的结果长什么样子，我们是从一堆数据里推导出其中的结构。</p>
<p>非监督学习最常见的应用是<strong>聚类</strong>(clustering)。举例来说，给定《美国经济》的1000篇文章，按照不同主题进行自动分类。另一个非聚类的典型例子是<a href="https://en.wikipedia.org/wiki/Cocktail_party_effect" target="_blank" rel="external">鸡尾酒会效应</a>，指的是在一个嘈杂的鸡尾酒会环境中谈话中，尽管周围噪音很多，你仍能分辨出朋友对你说话的声音。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>让我们先从监督学习中最简单的一个问题开始，假设我们有一个数据集如下，我们假设房价受住房面积的影响。</p>
<table>
<thead>
<tr>
<th style="text-align:center">住房面积(英尺<sup>2</sup>)</th>
<th style="text-align:center">房价(1000$)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2104</td>
<td style="text-align:center">400</td>
</tr>
<tr>
<td style="text-align:center">1600</td>
<td style="text-align:center">330</td>
</tr>
<tr>
<td style="text-align:center">2400</td>
<td style="text-align:center">369</td>
</tr>
<tr>
<td style="text-align:center">1416</td>
<td style="text-align:center">232</td>
</tr>
<tr>
<td style="text-align:center">3000</td>
<td style="text-align:center">540</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
<p>我们的目标是对给定数据集学习出一个函数h: x → y，使得对每个输入x，h(x)都能很好的预测出输出y。由于历史原因，我们把h称为<strong>假设函数</strong>(Hypothesis Function)。下图描述了这一过程：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-c97f91f90d4d7c19.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="假设函数" title="">
                </div>
                <div class="image-caption">假设函数</div>
            </figure>
<p>我们需要对假设函数进行建模，最简单的方式是将它视为线性函数，因而可表示成：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-8129541fa2d92322.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中θ<sub>i</sub>称之为<strong>参数</strong>(parameter)或者<strong>权重</strong>(weight)。为了简化表述，我们定义θ<sub>0</sub>=1，那么：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-741f1cbdd2024319.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中最右面等式中的θ和x都是向量表示，n是输入变量的个数（在这个例子中n=1）。</p>
<p>那么我们应该如何选取θ，使得h(x)和y的误差最小。为此我们定义<strong>代价函数</strong>(cost function)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-55c71a4a1d633d7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中x<sup>(i)</sup>这种上标表示方式是指第i个训练集的输入数据，y<sup>(i)</sup>是第i个训练集的输出值，m是训练集的个数。</p>
<h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>引入了代价函数后，我们的目标变成了：选择合适的θ，使得J(θ)最小。在这方面我们主要介绍<strong>梯度下降算法</strong>(Gradient Descent)。这个算法的主要思想是先选取一个初始点θ<sub>0</sub>，然后不断改变θ的值使得J(θ)变小，直到J(θ)收敛到最小值。特别的，为了使J(θ)变得最小，我们选择下一个θ值时应该选择能使J(θ)下降最快的那个值，在数学上就是对J(θ)求导，具体来说下一个选取的θ值就是：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-e275f29c10ed4961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中α是<strong>学习率</strong>(learning rate)，它会影响梯度下降的幅度。在每次迭代中，可以选取不同的α值。下图是梯度下降算法的图示，在选取初始点后，每次都按下降速率最快的方式寻找下一个点，直到找到最低点。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-43a69533c5c27c4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="梯度下降算法图示" title="">
                </div>
                <div class="image-caption">梯度下降算法图示</div>
            </figure>
<p>我们将J(θ)展开进行推导，由此得到：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-cf6d28c5a8647aa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因而迭代规则更新为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-cc63a51efd8c5698.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个规则被称为<strong>最小均方算法</strong>(Least Mean Squares，缩写为LMS)或者<strong>Widrow-Hoff算法</strong>。</p>
<p>这个算法在每次迭代时都要计算一遍训练集的数据，因而被称为<strong>批量梯度下降法</strong>(Batch Gradient Descent)。当训练集数据量很大时，计算速度将变得很慢。为了解决这个问题，我们可以在每次迭代时随机选取训练集数据的一部分来代替整体，这种方法称之为<strong>随机梯度下降法</strong>(Stochastic Gradient Descent)。随机梯度下降法由于只选取了部分样本数据，因此迭代过程会比较不稳定，虽然每次迭代不一定按着全体最优解靠近，但整体上趋于全体最优解。</p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>梯度下降法求解的缺点是需要很多次迭代，是否存在更好的方法呢。<strong>正规方程</strong>(Normal Equation)就是一个不需要进行迭代就能求解的方法，其公式如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-ce158343f4051248.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中X和y定义如下，X<sup>T</sup>是矩阵X的转置。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-e9010c8daf99c0a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-14756e220caa693a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个公式证明需要大量线性代数的知识，详细证明可以查阅参考资料。下表给出了梯度下降和正规函数两个算法的对比。</p>
<table>
<thead>
<tr>
<th style="text-align:center">梯度下降</th>
<th style="text-align:center">正规函数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">需要选择学习率α</td>
<td style="text-align:center">不需要选择学习率α</td>
</tr>
<tr>
<td style="text-align:center">需要很多次迭代</td>
<td style="text-align:center">不需要迭代</td>
</tr>
<tr>
<td style="text-align:center">O(kn<sup>2</sup>)</td>
<td style="text-align:center">O(n<sup>3</sup>)，需要计算X<sup>T</sup>X的逆矩阵</td>
</tr>
<tr>
<td style="text-align:center">n很大时也能正常工作</td>
<td style="text-align:center">n很大时计算很慢</td>
</tr>
</tbody>
</table>
<p>在实践中，当n&gt;=10000时不适合用正规函数，推荐改用梯度下降算法。</p>
<p>另外正规方程还有一个问题，就是X<sup>T</sup>X可能是不可逆的。不可逆的可能原因是我们使用了冗余的特征(比如两个特征线性相关)或者使用了太多的特征(比如特征数超过了样本数)。解决方法是删除一些多余的特征。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>机器学习问题可以分为监督学习和非监督学习，区别在于训练数据是否有特征</li>
<li>监督学习问题根据预测值的不同分为两类：预测值是连续值的叫回归，预测值是离散值的叫分类</li>
<li>最简单的回归模型是线性回归，求解线性回归的两个方法是：梯度下降和正规方程</li>
<li>当训练数据量较大时(n&gt;=10000)时推荐用梯度下降，数据量较小时用正规函数</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>Coursera机器学习课程讲义 <a href="https://www.coursera.org/learn/machine-learning/resources/JXWWS" target="_blank" rel="external">1</a> <a href="https://www.coursera.org/learn/machine-learning/resources/QQx8l" target="_blank" rel="external">2</a></li>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/B/O/M6SGF6VB4_M6SGHJ9BO.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;监督学习与非监督学习&quot;&gt;&lt;a href=&quot;#监督学习与非监督学习&quot; class=&quot;headerlink&quot; title=&quot;监督学习与非监督学习&quot;&gt;&lt;/a&gt;监督学习与非监督学习&lt;/h2&gt;&lt;p&gt;机器学习是指给定一些训练数据，使机器能够利用它们分析未知数据。任何机器学习问
    
    </summary>
    
    
      <category term="机器学习" scheme="http://www.secondplayer.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>从SQLAlchemy的“缓存”问题说起</title>
    <link href="http://www.secondplayer.top/2017/11/21/sqlalchemy-cache/"/>
    <id>http://www.secondplayer.top/2017/11/21/sqlalchemy-cache/</id>
    <published>2017-11-21T15:37:21.000Z</published>
    <updated>2017-11-21T16:05:29.576Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>最近在排查一个问题，为了方便说明，我们假设现在有如下一个API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@app.route("/sqlalchemy/test", methods=['GET'])</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sqlalchemy_test_api</span><span class="params">()</span>:</span></div><div class="line">    data = &#123;&#125;</div><div class="line">    <span class="comment"># 获取商品价格</span></div><div class="line">    product = Product.query.get(<span class="number">1</span>)</div><div class="line">    data[<span class="string">'old_price'</span>] = product.present_price</div><div class="line">    <span class="comment"># 休眠10秒，等待外部修改价格</span></div><div class="line">    time.sleep(<span class="number">10</span>)</div><div class="line">    product = Product.query.get(<span class="number">1</span>)</div><div class="line">    data[<span class="string">'new_price'</span>] = product.present_price</div><div class="line">    <span class="keyword">return</span> jsonify(status=<span class="string">'ok'</span>, data=data)</div></pre></td></tr></table></figure>
<p>这里我们的后台使用了<a href="http://flask.pocoo.org/" target="_blank" rel="external">Flask</a>作为服务端框架，<a href="https://www.sqlalchemy.org/" target="_blank" rel="external">SQLAlchemy</a>作为数据库ORM框架。Product是一张商品表的ORM模型，假设原来id=1的商品价格为10，在程序休眠的10秒内价格被修改为20，那么你觉得返回的结果是多少？</p>
<p>old_price显然是10，那么new_price呢？讲道理的话由于外部修改价格为20了，同时程序在sleep后立刻又query了一次，你可能觉得new_price应该是20。但结果并不是，真实测试的结果是10，给人感觉就像是SQLAlchemy“缓存”了上一次的结果。</p>
<p>另外在测试的过程还发现一个现象，虽然在第一次API调用时两个price都是10，但是在第二次调用API时，读到的price是20。也就是说，在一个新的API开始时，之前“缓存”的结果被清除了。</p>
<h2 id="SQLAlchemy的session状态管理"><a href="#SQLAlchemy的session状态管理" class="headerlink" title="SQLAlchemy的session状态管理"></a>SQLAlchemy的session状态管理</h2><p>之前我们提出了一个猜测：第二次查询是否“缓存”了第一次查询。为了验证这个猜想，我们可以把<code>SQLALCHEMY_ECHO</code>这个配置项打开，这是个全局配置项，官方文档定义如下：</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>SQLALCHEMY_ECHO</code></td>
<td style="text-align:left">If set to True SQLAlchemy will log all the statements issued to stderr which can be useful for debugging.</td>
</tr>
</tbody>
</table>
<p>在这个配置项打开的情况下，我们可以看到查询语句输出到终端下。我们再次调用API，可以发现第一次查询会输出类似<code>SELECT * FROM product WHERE id = 1</code>的语句，而第二次查询则没有这样的输出。如此看来，SQLAlchemy确实缓存了上次的结果，在第二次查询的时候直接使用了上次的结果。</p>
<p>实际上，当执行第一句<code>product = Product.query.get(1)</code>时，product这个对象处于持久状态(persistent)了，我们可以通过一些工具看到ORM对象目前处于的状态。详细的状态列表可在<a href="http://docs.sqlalchemy.org/en/latest/orm/session_state_management.html#quickie-intro-to-object-states" target="_blank" rel="external">官方文档</a>中找到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> inspect</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>insp = inspect(product)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>insp.persistent</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>product.__dict__</div><div class="line">&#123;</div><div class="line">  <span class="string">'id'</span>: <span class="number">1</span>, <span class="string">'present_price'</span>: <span class="number">10</span>,</div><div class="line">  <span class="string">'_sa_instance_state'</span>: &lt;sqlalchemy.orm.state.InstanceState object at <span class="number">0x1106a3350</span>&gt;,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>为了清除该对象的缓存，程度从低到高有下面几种做法。<code>expire</code>会清除对象里缓存的数据，这样下次查询时会直接从数据库进行查询。<code>refresh</code>不仅清除对象里缓存的数据，还会立刻触发一次数据库查询更新数据。<code>expire_all</code>的效果和<code>expire</code>一样，只不过会清除session里所有对象的缓存。<code>flush</code>会把所有本地修改写入到数据库，但没有提交。<code>commit</code>不仅把所有本地修改写入到数据库，同时也提交了该事务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">db.session.expire(product)</div><div class="line">db.session.refresh(product)</div><div class="line">db.session.expire_all()</div><div class="line">db.session.flush()</div><div class="line">db.session.commit()</div></pre></td></tr></table></figure>
<p>我们对这几种方法依次做实验，结果发现这5个操作都会让下次查询直接从数据库进行查询，但只有<code>commit</code>会读到最新的price。那这个又是什么原因呢，我们已经强制每次查询走数据库，为何还是读到“缓存”的数据。这个就要用数据库的事务隔离机制来解释了。</p>
<h2 id="事务隔离"><a href="#事务隔离" class="headerlink" title="事务隔离"></a>事务隔离</h2><p>在数据库系统中，事务<a href="https://en.wikipedia.org/wiki/Isolation_level" target="_blank" rel="external">隔离级别</a>(isolation level)决定了数据在系统中的可见性。隔离级别从低到高分为四种：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可串行化(Serializable)。他们的区别如下表所示。</p>
<table>
<thead>
<tr>
<th>隔离级别</th>
<th style="text-align:right">脏读</th>
<th style="text-align:right">不可重复读</th>
<th style="text-align:right">幻读</th>
</tr>
</thead>
<tbody>
<tr>
<td>未提交读(RU)</td>
<td style="text-align:right">可能</td>
<td style="text-align:right">可能</td>
<td style="text-align:right">可能</td>
</tr>
<tr>
<td>已提交读(RC)</td>
<td style="text-align:right">不可能</td>
<td style="text-align:right">可能</td>
<td style="text-align:right">可能</td>
</tr>
<tr>
<td>可重复读(RR)</td>
<td style="text-align:right">不可能</td>
<td style="text-align:right">不可能</td>
<td style="text-align:right">可能</td>
</tr>
<tr>
<td>可串行化</td>
<td style="text-align:right">不可能</td>
<td style="text-align:right">不可能</td>
<td style="text-align:right">不可能</td>
</tr>
</tbody>
</table>
<p>脏读(dirty read)是指一个事务可以读到其他事务还未提交的数据。不可重复读(non-repeatable read)是指在一个事务中同一行被读取了多次，可以读到不同的值。幻读(phantom read)是指在一个事务中执行同一个语句多次，读到的数据行发生了改变，即可能行数增加了或减少了。</p>
<p>前面提到的问题其实就涉及到不可重复读这个特性，即在一个事务中我们query了product.id=1的数据多次，但读到了重复的数据。对于MySQL来说，默认的事务隔离级别是RR，通过上表我们可知RR是可重复读的，因此可以解释这个现象。</p>
<table>
<thead>
<tr>
<th>事务A</th>
<th>事务B</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>BEGIN;</code></td>
<td><code>BEGIN;</code></td>
</tr>
<tr>
<td><code>SELECT present_price FROM product WHERE id = 1;</code>  <code>/* id=1的商品价格为10 */</code></td>
<td></td>
</tr>
<tr>
<td></td>
<td><code>UPDATE product SET present_price = 20 WHERE id = 1;</code>  <code>/* 修改id=1的商品价格为20 */</code></td>
</tr>
<tr>
<td></td>
<td><code>COMMIT;</code></td>
</tr>
<tr>
<td><code>SELECT present_price FROM product WHERE id = 1;</code>  <code>/* 再次查询id=1的商品价格 */</code></td>
<td></td>
</tr>
<tr>
<td><code>COMMIT;</code></td>
</tr>
</tbody>
</table>
<p>对于前面的问题，我们可以把两个事务的执行时序图画出来如上所示。因此为了使第二次查询得到正确的值，我们可以把隔离级别设为RC，或者在第二次查询前进行<code>COMMIT</code>新起一个事务。</p>
<h2 id="Flask-SQLAlchemy的自动提交"><a href="#Flask-SQLAlchemy的自动提交" class="headerlink" title="Flask-SQLAlchemy的自动提交"></a>Flask-SQLAlchemy的自动提交</h2><p>前面还遗留一个问题没有搞清楚：在一个新的API开始时，之前“缓存”的结果似乎被清除了。由于打开了<code>SQLALCHEMY_ECHO</code>配置项，我们可以观察到每次API结束的时候都会自动触发一次<code>COMMIT</code>，而正是这个自动提交清空了所有的“缓存”。通过查找源代码，我们发现是下面这段代码在起作用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@teardown</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">shutdown_session</span><span class="params">(response_or_exc)</span>:</span></div><div class="line">    <span class="keyword">if</span> app.config[<span class="string">'SQLALCHEMY_COMMIT_ON_TEARDOWN'</span>]:</div><div class="line">        <span class="keyword">if</span> response_or_exc <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self.session.commit()</div><div class="line">    self.session.remove()</div><div class="line">    <span class="keyword">return</span> response_or_exc</div></pre></td></tr></table></figure>
<p>如果配置项<code>SQLALCHEMY_COMMIT_ON_TEARDOWN</code>为<code>True</code>，那么首先触发<code>COMMIT</code>，最后统一执行<code>session.remove()</code>操作，即释放连接并回滚事务操作。</p>
<p>有意思的是，这个配置项在Flask2.0版本的Changelog中被移除了。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-7b55f7db34ac4c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Flask2.0 Changelog" title="">
                </div>
                <div class="image-caption">Flask2.0 Changelog</div>
            </figure>
<p>关于删除的原因，作者在<a href="https://stackoverflow.com/questions/23301968/invalid-transaction-persisting-across-requests" target="_blank" rel="external">stackoverflow</a>的一个帖子里进行了说明。这个帖子同时也解释了为什么在我们的生产环境中经常报这个错误：<br><code>InvalidRequestError: This session is in &#39;prepared&#39; state; no further SQL can be emitted within this transaction.</code>，而且只有重启才能解决问题。有兴趣的同学可以深入阅读一下。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在MySQL的同一个事务中，多次查询同一行的数据得到的结果是相同的，这里既有SQLAlchemy本身“缓存”结果的原因，也受到数据库隔离级别的影响。如果要强制读取最新的结果，最简单的办法就是在查询前手动<code>COMMIT</code>一次。根据这个原则，我们可以再仔细阅读下自己项目中的代码，看看会不会有一些隐藏的问题。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h2&gt;&lt;p&gt;最近在排查一个问题，为了方便说明，我们假设现在有如下一个API：&lt;/p&gt;
&lt;figure class=&quot;highlight 
    
    </summary>
    
    
      <category term="Flask" scheme="http://www.secondplayer.top/tags/Flask/"/>
    
      <category term="SQLAlchemy" scheme="http://www.secondplayer.top/tags/SQLAlchemy/"/>
    
  </entry>
  
  <entry>
    <title>迁移博客到阿里云</title>
    <link href="http://www.secondplayer.top/2017/10/31/migrate-to-aliyun-ecs/"/>
    <id>http://www.secondplayer.top/2017/10/31/migrate-to-aliyun-ecs/</id>
    <published>2017-10-31T15:00:38.000Z</published>
    <updated>2017-10-31T15:00:38.984Z</updated>
    
    <content type="html"><![CDATA[<h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>去年在<a href="https://aws.amazon.com/" target="_blank" rel="external">AWS</a>上搭建的博客已经过去一年多了，之前在<a href="http://secondplayer.top/2016/06/12/hexo-blog-setup/" target="_blank" rel="external">使用Hexo搭建个人静态博客</a>这篇文章中提到，使用AWS可以免费使用一年的VPS，然而一年到了后发现一个月要收费12.94美元，感觉实在性价比不高。听说<a href="https://www.aliyun.com/" target="_blank" rel="external">阿里云</a>在2016年开始进军海外业务，所以趁这次机会迁移过去。于是在官网上购买了美国西部（硅谷）节点的服务器，目前在双11活动期间处于优惠价，有兴趣的朋友可以趁现在入手试一下。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-12ffd8f63fe7cc71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="购买阿里云ECS" title="">
                </div>
                <div class="image-caption">购买阿里云ECS</div>
            </figure>
<h2 id="ECS环境配置"><a href="#ECS环境配置" class="headerlink" title="ECS环境配置"></a>ECS环境配置</h2><p>购买完服务器后就开始配置环境了。首先是登录服务器，默认是密码方式登录。然而每次输入密码实在是太麻烦了，建议使用密钥方式登录，在ECS后台-网络和安全-密钥对里创建一个新的密钥对，然后将其与你的实例绑定，之后就可以用私钥登录了。注意密钥对创建完成后一定要马上下载私钥，因为阿里云只给你一次下载私钥的机会，并且不要将私钥泄露给别人。</p>
<p>登录到服务器之后开始安装环境，在此之前需要检查一下服务器是否能访问外网。如果无法访问外网，需要到ECS后台-网络和安全-安全组里新建安全组，给安全组配置默认规则，默认规则的出方向即为允许访问任意ip的任意端口。这个安全组后面还会用到，如果你想开放一个自定义端口允许外网访问，也需要新建一个安全组并配置相应规则。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-ace2a00f71c5347a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="配置安全组规则" title="">
                </div>
                <div class="image-caption">配置安全组规则</div>
            </figure>
<h2 id="迁移博客"><a href="#迁移博客" class="headerlink" title="迁移博客"></a>迁移博客</h2><p>一切准备就绪后开始迁移博客。由于hexo是静态博客，所以只需把相应的静态文件拷贝的新机器上即可。这里列一下遇到的坑以及一些升级改动。</p>
<h3 id="全局安装hexo报错"><a href="#全局安装hexo报错" class="headerlink" title="全局安装hexo报错"></a>全局安装hexo报错</h3><p>旧服务器上的node版本是v4.4.5，转眼一年过去了，最新版本是v8.4.0。在新版本下执行<code>npm install hexo-cli -g</code>安装hexo会有报错，解决办法详见<a href="https://github.com/hexojs/hexo/issues/2505" target="_blank" rel="external">官方issues</a>，简而言之就是先执行一句<code>npm config set unsafe-perm true</code>再安装即可。</p>
<h3 id="升级主题"><a href="#升级主题" class="headerlink" title="升级主题"></a>升级主题</h3><p>我的博客一直在使用这个Material Design风格的主题，名叫<a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank" rel="external">indigo</a>。在一年内这个主题也有了较大的更新，升级之后界面变得更简洁了，优化了分享功能，增加了赞赏功能。升级的话也很简单，直接将代码更新到最新，按照文档更新配置即可。</p>
<h3 id="评论系统切换"><a href="#评论系统切换" class="headerlink" title="评论系统切换"></a>评论系统切换</h3><p>旧博客使用的评论系统是多说，然而这家公司业务调整，已经关闭该系统了。知乎上有很多关于<a href="(https://www.zhihu.com/question/57426274/answer/153065672">替代方案的讨论</a>)，最终我选择了用<a href="https://github.com/imsun/gitment" target="_blank" rel="external">gitment</a>作为新博客的评论系统。这套评论系统最大的特点是基于GitHub Issues的评论系统，主要面向程序员群体。使用上也很方便，而且indigo主题已经支持gitment，所以只需简单配置几个参数就能使用了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整个迁移步骤，主要在熟悉阿里云后台配置上花的时间最多。由于AWS是行业先行者，可以看得出阿里云的后台功能有点仿照AWS的意思，但可能是功能太多的缘故，给人感觉布局很拥挤。不管怎样，博客还是成功迁移了，在阿里云海外服务器上搭建科学上网工具也很流畅。</p>
<p>最后打个广告，如果有兴趣购买阿里云的相关产品可以使用这个<a href="https://promotion.aliyun.com/ntms/act/ambassador/sharetouser.html?userCode=cqbcj3wz&amp;utm_source=cqbcj3wz" target="_blank" rel="external">推广链接</a>，点击链接可以领取优惠券。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;起因&quot;&gt;&lt;a href=&quot;#起因&quot; class=&quot;headerlink&quot; title=&quot;起因&quot;&gt;&lt;/a&gt;起因&lt;/h2&gt;&lt;p&gt;去年在&lt;a href=&quot;https://aws.amazon.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;AW
    
    </summary>
    
    
      <category term="Hexo" scheme="http://www.secondplayer.top/tags/Hexo/"/>
    
      <category term="阿里云" scheme="http://www.secondplayer.top/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>重构: 改善既有代码的设计</title>
    <link href="http://www.secondplayer.top/2017/09/19/refactoring-book/"/>
    <id>http://www.secondplayer.top/2017/09/19/refactoring-book/</id>
    <published>2017-09-18T16:12:24.000Z</published>
    <updated>2017-09-18T16:12:24.786Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://book.douban.com/subject/4262627/" target="_blank" rel="external">重构</a>这本书由著名的世界软件开发大师<a href="https://martinfowler.com/" target="_blank" rel="external">Martin Fowler</a>编写，是软件开发领域的经典书籍。书中的部分内容在<a href="https://refactoring.com/" target="_blank" rel="external">refactoring.com</a>上也有提及。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-2b484ff5d8bcc8e0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="重构: 改善既有代码的设计" title="">
                </div>
                <div class="image-caption">重构: 改善既有代码的设计</div>
            </figure>
<h2 id="什么是重构"><a href="#什么是重构" class="headerlink" title="什么是重构"></a>什么是重构</h2><p>视上下文不同，重构有两个定义：</p>
<ul>
<li>重构(名词)：对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本</li>
<li>重构(动词)：使用一系列重构手法，在不改变软件可观察行为的前提下，调整其结构</li>
</ul>
<h2 id="为什么要重构"><a href="#为什么要重构" class="headerlink" title="为什么要重构"></a>为什么要重构</h2><p>重构是个工具，它可以用于以下几个目的：</p>
<ul>
<li>重构改进软件设计</li>
<li>重构使软件更容易理解</li>
<li>重构帮助找到bug</li>
<li>重构提高编程速度</li>
</ul>
<h2 id="何时重构"><a href="#何时重构" class="headerlink" title="何时重构"></a>何时重构</h2><p>不需要专门拨出时间进行重构，重构应该随时随地进行。你之所以重构，是因为你想做别的什么事，而重构可以帮助你把那些事做好。</p>
<ul>
<li>事不过三，三则重构</li>
<li>添加功能时重构</li>
<li>修补错误时重构</li>
<li>复审代码时重构</li>
</ul>
<h2 id="何时不该重构"><a href="#何时不该重构" class="headerlink" title="何时不该重构"></a>何时不该重构</h2><ul>
<li>当既有代码实在太混乱，重构不如重写来得简单</li>
<li>当项目已接近最后期限，应该避免进行重构，因为已经没有时间了</li>
</ul>
<h2 id="代码的坏味道"><a href="#代码的坏味道" class="headerlink" title="代码的坏味道"></a>代码的坏味道</h2><p>「如果尿布臭了，就换掉它」。代码的坏味道指出了重构的可能性。</p>
<ul>
<li>重复代码 (Duplicated Code)</li>
<li>过长函数 (Long Method)</li>
<li>过大的类 (Large Class)</li>
<li>过长参数列 (Long Parameter List)</li>
<li>发散式变化 (Divergent Change)</li>
<li>switch语句 (Switch Statements)</li>
<li>中间人 (Middle Man)</li>
<li>异曲同工的类 (Alternative Classes with Different Interfaces)</li>
<li>过多的注释 (Comments)</li>
<li>…</li>
</ul>
<h2 id="构筑测试体系"><a href="#构筑测试体系" class="headerlink" title="构筑测试体系"></a>构筑测试体系</h2><p>重构的基本技巧「小步前进，频繁测试」已经得到了多年的实践检验。因此如果你想进行重构，首要前提就是拥有一个可靠的测试体系。</p>
<h2 id="常用重构方法"><a href="#常用重构方法" class="headerlink" title="常用重构方法"></a>常用重构方法</h2><h3 id="提炼函数-Extract-Method"><a href="#提炼函数-Extract-Method" class="headerlink" title="提炼函数 (Extract Method)"></a>提炼函数 (Extract Method)</h3><blockquote>
<p>当我看见一个过长的函数或者一段需要注释才能让人理解用途的代码，我就会将这段代码放进一个独立函数中</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">printOwing</span><span class="params">()</span> </span>&#123;</div><div class="line">  printBanner();</div><div class="line"></div><div class="line">  <span class="comment">//print details</span></div><div class="line">  System.out.println (<span class="string">"name:  "</span> + _name);</div><div class="line">  System.out.println (<span class="string">"amount "</span> + amount);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">printOwing</span><span class="params">()</span> </span>&#123;</div><div class="line">  printBanner();</div><div class="line">  printDetails(amount);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">printDetails</span> <span class="params">(<span class="keyword">double</span> amount)</span> </span>&#123;</div><div class="line">  System.out.println (<span class="string">"name:  "</span> + _name);</div><div class="line">  System.out.println (<span class="string">"amount "</span> + amount);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="引入解释性变量-Introduce-Explaining-Variable"><a href="#引入解释性变量-Introduce-Explaining-Variable" class="headerlink" title="引入解释性变量 (Introduce Explaining Variable)"></a>引入解释性变量 (Introduce Explaining Variable)</h3><blockquote>
<p>表达式有可能非常复杂而难以阅读。这种情况下，临时变量可以帮助你将表达式分解为比较容易管理的形式。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> ((platform.toUpperCase().indexOf(<span class="string">"MAC"</span>) &gt; -<span class="number">1</span>) &amp;&amp;</div><div class="line">    (browser.toUpperCase().indexOf(<span class="string">"IE"</span>) &gt; -<span class="number">1</span>) &amp;&amp;</div><div class="line">    wasInitialized() &amp;&amp; resize &gt; <span class="number">0</span>)</div><div class="line">&#123;</div><div class="line">    <span class="comment">// do something</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> <span class="keyword">boolean</span> isMacOs = platform.toUpperCase().indexOf(<span class="string">"MAC"</span>) &gt; -<span class="number">1</span>;</div><div class="line"><span class="keyword">final</span> <span class="keyword">boolean</span> isIEBrowser = browser.toUpperCase().indexOf(<span class="string">"IE"</span>) &gt; -<span class="number">1</span>;</div><div class="line"><span class="keyword">final</span> <span class="keyword">boolean</span> wasResized = resize &gt; <span class="number">0</span>;</div><div class="line"><span class="keyword">if</span> (isMacOs &amp;&amp; isIEBrowser &amp;&amp; wasInitialized() &amp;&amp; wasResized) &#123;</div><div class="line">    <span class="comment">// do something</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="分解临时变量-Split-Temporary-Variable"><a href="#分解临时变量-Split-Temporary-Variable" class="headerlink" title="分解临时变量 (Split Temporary Variable)"></a>分解临时变量 (Split Temporary Variable)</h3><blockquote>
<p>如果临时变量承担多个责任，它就应该被替换(分解)为多个临时变量，每个变量只承担一个责任。同一个临时变量承担两件不同的事情，会令代码阅读者糊涂。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">double</span> temp = <span class="number">2</span> * (_height + _width);</div><div class="line">System.out.println (temp);</div><div class="line">temp = _height * _width;</div><div class="line">System.out.println (temp);</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> <span class="keyword">double</span> perimeter = <span class="number">2</span> * (_height + _width);</div><div class="line">System.out.println (perimeter);</div><div class="line"><span class="keyword">final</span> <span class="keyword">double</span> area = _height * _width;</div><div class="line">System.out.println (area);</div></pre></td></tr></table></figure>
<h3 id="移除对参数的赋值-Remove-Assignments-to-Parameters"><a href="#移除对参数的赋值-Remove-Assignments-to-Parameters" class="headerlink" title="移除对参数的赋值 (Remove Assignments to Parameters)"></a>移除对参数的赋值 (Remove Assignments to Parameters)</h3><blockquote>
<p>我之所以不喜欢(对参数赋值)这样的做法，因为它降低了代码的清晰度，而且混淆了按值传递和按引用传递这两种参数传递方式。<br>当然，面对那些使用「输出式参数」(output parameters)的语言，你不必遵循这条规则。不过在那些语言中我会尽量少用输出式参数。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">discount</span> <span class="params">(<span class="keyword">int</span> inputVal, <span class="keyword">int</span> quantity, <span class="keyword">int</span> yearToDate)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (inputVal &gt; <span class="number">50</span>) &#123;</div><div class="line">        inputVal -= <span class="number">2</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">discount</span> <span class="params">(<span class="keyword">int</span> inputVal, <span class="keyword">int</span> quantity, <span class="keyword">int</span> yearToDate)</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> result = inputVal;</div><div class="line">    <span class="keyword">if</span> (inputVal &gt; <span class="number">50</span>) &#123;</div><div class="line">        result -= <span class="number">2</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="提炼类-Extract-Class"><a href="#提炼类-Extract-Class" class="headerlink" title="提炼类 (Extract Class)"></a>提炼类 (Extract Class)</h3><blockquote>
<p>某个类做了应该由两个类做的事。<br>此时你需要考虑哪些部分可以分离出去，并将它们分离到一个单独的类中。</p>
</blockquote>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-5da7e72435862df7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="提炼类" title="">
                </div>
                <div class="image-caption">提炼类</div>
            </figure>
<h3 id="移除中间人-Remove-Middle-Man"><a href="#移除中间人-Remove-Middle-Man" class="headerlink" title="移除中间人 (Remove Middle Man)"></a>移除中间人 (Remove Middle Man)</h3><blockquote>
<p>每当客户要使用受托类的新特性时，你就必须在服务端添加一个简单委托函数。随着受托类的特性(功能)越来越多，这一过程会让你痛苦不已。服务类完全变成了一个“中间人”，此时你就应该让客户直接调用受托类。</p>
</blockquote>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-614d9c5bb4893d61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="移除中间人" title="">
                </div>
                <div class="image-caption">移除中间人</div>
            </figure>
<h3 id="以字面常量取代魔法数-Replace-Magic-Number-with-Symbolic-Constant"><a href="#以字面常量取代魔法数-Replace-Magic-Number-with-Symbolic-Constant" class="headerlink" title="以字面常量取代魔法数 (Replace Magic Number with Symbolic Constant)"></a>以字面常量取代魔法数 (Replace Magic Number with Symbolic Constant)</h3><blockquote>
<p>所谓魔法数(magic number)是指拥有特殊意义，却又不能明确表现出这种意义的数字。如果你需要在不同的地点引用同一个逻辑数，魔法数会让你烦恼不已，因为一旦这些数发生改变，你就必须在程序中找到所有魔法数，并将它们全部修改一遍，这简直就是一场噩梦。就算你不需要修改，要准确指出每个魔法数的用途，也会让你颇费脑筋。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">double</span> <span class="title">potentialEnergy</span><span class="params">(<span class="keyword">double</span> mass, <span class="keyword">double</span> height)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> mass * <span class="number">9.81</span> * height;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">double</span> <span class="title">potentialEnergy</span><span class="params">(<span class="keyword">double</span> mass, <span class="keyword">double</span> height)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> mass * GRAVITATIONAL_CONSTANT * height;</div><div class="line">&#125;</div><div class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span> GRAVITATIONAL_CONSTANT = <span class="number">9.81</span>;</div></pre></td></tr></table></figure>
<h3 id="分解条件表达式-Decompose-Conditional"><a href="#分解条件表达式-Decompose-Conditional" class="headerlink" title="分解条件表达式 (Decompose Conditional)"></a>分解条件表达式 (Decompose Conditional)</h3><blockquote>
<p>程序之中，复杂的条件逻辑是最常导致复杂度上升的地点之一。你必须编写代码来检查不同的条件分支、根据不同的分支做不同的事，然后你很快就会得到一个相当长的函数。<br>对于条件逻辑，将每个分支条件分解成新函数可以给你带来更多好处：可以突出条件逻辑，更清楚地表明每个分支的作用，并且突出每个分支的原因。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (date.before (SUMMER_START) || date.after(SUMMER_END))</div><div class="line">  charge = quantity * _winterRate + _winterServiceCharge;</div><div class="line"><span class="keyword">else</span> charge = quantity * _summerRate;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (notSummer(date))</div><div class="line">  charge = winterCharge(quantity);</div><div class="line"><span class="keyword">else</span> charge = summerCharge (quantity);</div></pre></td></tr></table></figure>
<h3 id="合并条件表达式-Consolidate-Conditional-Expression"><a href="#合并条件表达式-Consolidate-Conditional-Expression" class="headerlink" title="合并条件表达式 (Consolidate Conditional Expression)"></a>合并条件表达式 (Consolidate Conditional Expression)</h3><blockquote>
<p>之所以要合并条件代码，有两个重要原因。首先，合并后的条件代码会告诉你“实际上只有一次条件检查，只不过有多个并列条件需要检查而已”，从而使这一次检查的用意更清晰。其次，这项重构往往可以为你使用提炼函数(Extract Method)做好准备。将检查条件提炼成一个独立函数对于厘清代码意义非常有用，因为它把描述“做什么”的语句换成了“为什么这样做”。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">double</span> <span class="title">disabilityAmount</span><span class="params">()</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (_seniority &lt; <span class="number">2</span>) <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">if</span> (_monthsDisabled &gt; <span class="number">12</span>) <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="keyword">if</span> (_isPartTime) <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="comment">// compute the disability amount</span></div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">double</span> <span class="title">disabilityAmount</span><span class="params">()</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (isNotEligableForDisability()) <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">  <span class="comment">// compute the disability amount</span></div></pre></td></tr></table></figure>
<h3 id="合并重复的条件片段-Consolidate-Duplicate-Conditional-Fragments"><a href="#合并重复的条件片段-Consolidate-Duplicate-Conditional-Fragments" class="headerlink" title="合并重复的条件片段 (Consolidate Duplicate Conditional Fragments)"></a>合并重复的条件片段 (Consolidate Duplicate Conditional Fragments)</h3><blockquote>
<p>有时你会发现，一组条件表达式的所有分支都执行了相同的某段代码。如果是这样，你就应该将这段代码搬移到条件表达式外面。这样，代码才能更清楚地表明哪些东西随条件的变化而变化、哪些东西保持不变。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (isSpecialDeal()) &#123;</div><div class="line">  total = price * <span class="number">0.95</span>;</div><div class="line">  send();</div><div class="line">&#125;</div><div class="line"><span class="keyword">else</span> &#123;</div><div class="line">  total = price * <span class="number">0.98</span>;</div><div class="line">  send();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (isSpecialDeal())</div><div class="line">  total = price * <span class="number">0.95</span>;</div><div class="line"><span class="keyword">else</span></div><div class="line">  total = price * <span class="number">0.98</span>;</div><div class="line">send();</div></pre></td></tr></table></figure>
<h3 id="移除控制标记-Remove-Control-Flag"><a href="#移除控制标记-Remove-Control-Flag" class="headerlink" title="移除控制标记 (Remove Control Flag)"></a>移除控制标记 (Remove Control Flag)</h3><blockquote>
<p>人们之所以会使用这样的控制标记，因为结构化编程原则告诉他们：每个子程序只能有一个入口和一个出口。我赞同“单一入口”原则（而且现代编程语言也强迫我们这样做），但是“单一出口”原则会让你在代码中加入讨厌的控制标记，大大降低条件表达式的可读性。这就是编程语言提供break语句和continue语句的原因：用它们跳出复杂的条件语句。去掉控制标记所产生的效果往往让你大吃一惊：条件语句真正的用途会清晰得多。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">checkSecurity</span><span class="params">(String[] people)</span> </span>&#123;</div><div class="line">  <span class="keyword">boolean</span> found = <span class="keyword">false</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; people.length; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (!found)&#123;</div><div class="line">      <span class="keyword">if</span> (people[i].equals(<span class="string">"Don"</span>)) &#123;</div><div class="line">        sendAlert();</div><div class="line">        found = <span class="keyword">true</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (people[i].equals(<span class="string">"John"</span>)) &#123;</div><div class="line">        sendAlert();</div><div class="line">        found = <span class="keyword">true</span>;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> found;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">checkSecurity</span><span class="params">(String[] people)</span> </span>&#123;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; people.length; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (!found)&#123;</div><div class="line">      <span class="keyword">if</span> (people[i].equals(<span class="string">"Don"</span>)) &#123;</div><div class="line">        sendAlert();</div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (people[i].equals(<span class="string">"John"</span>)) &#123;</div><div class="line">        sendAlert();</div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="以卫语句取代嵌套条件表达式-Replace-Nested-Conditional-with-Guard-Clauses"><a href="#以卫语句取代嵌套条件表达式-Replace-Nested-Conditional-with-Guard-Clauses" class="headerlink" title="以卫语句取代嵌套条件表达式 (Replace Nested Conditional with Guard Clauses)"></a>以卫语句取代嵌套条件表达式 (Replace Nested Conditional with Guard Clauses)</h3><blockquote>
<p>如果条件表达式的两条分支都是正常行为，就应该使用形如if…else…的条件表达式；如果某个条件极其罕见，就应该单独检查该条件，并在该条件为真时立刻从函数中返回。这样的单独检查常常被称为“卫语句”(guard clauses)。</p>
<p>这个方法的精髓是：给某一条分支以特别的重视。它告诉阅读者：这种情况很罕见，如果它真地发生了，请做一些必要的整理工作，然后退出。</p>
<p>“每个函数只能有一个入口和一个出口”的观念，根深蒂固于某些程序员的脑海里。现今的编程语言都会强制保证每个函数只有一个入口，至于“单一出口”规则，其实不是那么有用。保持代码清晰才是最关键的：如果单一出口能使这个函数更清晰易读，那么就使用单一出口；否则就不必这么做。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">double</span> <span class="title">getPayAmount</span><span class="params">()</span> </span>&#123;</div><div class="line">  <span class="keyword">double</span> result;</div><div class="line">  <span class="keyword">if</span> (_isDead) result = deadAmount();</div><div class="line">  <span class="keyword">else</span> &#123;</div><div class="line">    <span class="keyword">if</span> (_isSeparated) result = separatedAmount();</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">if</span> (_isRetired) result = retiredAmount();</div><div class="line">      <span class="keyword">else</span> result = normalPayAmount();</div><div class="line">    &#125;;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> result;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">double</span> <span class="title">getPayAmount</span><span class="params">()</span> </span>&#123;</div><div class="line">  <span class="keyword">if</span> (_isDead) <span class="keyword">return</span> deadAmount();</div><div class="line">  <span class="keyword">if</span> (_isSeparated) <span class="keyword">return</span> separatedAmount();</div><div class="line">  <span class="keyword">if</span> (_isRetired) <span class="keyword">return</span> retiredAmount();</div><div class="line">  <span class="keyword">return</span> normalPayAmount();</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>扩展阅读：关于如何重构嵌套条件表达式，可以阅读<a href="https://coolshell.cn/articles/17757.html" target="_blank" rel="external">如何重构“箭头型”代码</a>，这篇文章更深层次地讨论了这个问题。</p>
<h3 id="将查询函数和修改函数分离-Separate-Query-from-Modifier"><a href="#将查询函数和修改函数分离-Separate-Query-from-Modifier" class="headerlink" title="将查询函数和修改函数分离 (Separate Query from Modifier)"></a>将查询函数和修改函数分离 (Separate Query from Modifier)</h3><blockquote>
<p>下面是一条好规则：任何有返回值的函数，都不应该有看得到的副作用。</p>
<p>如果你遇到一个“既有返回值又有副作用”的函数，就应该试着将查询动作从修改动作中分割出来。</p>
</blockquote>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-7f9ff8bd03c36865.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="将查询函数和修改函数分离" title="">
                </div>
                <div class="image-caption">将查询函数和修改函数分离</div>
            </figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://book.douban.com/subject/4262627/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;重构&lt;/a&gt;这本书由著名的世界软件开发大师&lt;a href=&quot;https://martinfowler.com/
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://www.secondplayer.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="重构" scheme="http://www.secondplayer.top/tags/%E9%87%8D%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>常见推荐系统介绍</title>
    <link href="http://www.secondplayer.top/2017/09/08/recommendation-system-book/"/>
    <id>http://www.secondplayer.top/2017/09/08/recommendation-system-book/</id>
    <published>2017-09-07T16:07:37.000Z</published>
    <updated>2017-09-07T16:09:51.518Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要是对项亮的<a href="https://book.douban.com/subject/10769749/" target="_blank" rel="external">推荐系统实践</a>部分章节进行了一些总结，先从什么是推荐系统开始讲起，然后介绍了评测推荐系统的指标和方法，最后介绍了常见的推荐系统算法。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-7457fe3c0981d437.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="《推荐系统实践》封面" title="">
                </div>
                <div class="image-caption">《推荐系统实践》封面</div>
            </figure>
<h2 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h2><p>随着信息技术和互联网的快速发展，人们逐渐从信息匮乏的时代走入了信息过载的时代。每天都有海量的信息被生产出来，用户如何从中找到自己感兴趣的内容变得越来越困难，内容生产者也在想方设法让自己生成的内容从海量信息中脱颖而出。为了解决信息过载的问题，历史上出现过的代表方案有分类目录和搜索引擎，这两者都要求用户明确知道自己需要的内容关键词。而推荐系统不需要用户提供明确的需求，而是通过分析用户的历史行为给用户的兴趣建模，从而主动给用户推荐能够满足它们兴趣的内容。推荐系统通过发掘用户的行为，找到用户的个性化需求，从而将<a href="https://zh.wikipedia.org/wiki/%E9%95%BF%E5%B0%BE" target="_blank" rel="external">长尾</a>商品准确地推荐给需要它的用户，帮助用户发现那些他们感兴趣但很难发现的商品。</p>
<h2 id="推荐系统的应用"><a href="#推荐系统的应用" class="headerlink" title="推荐系统的应用"></a>推荐系统的应用</h2><p>在互联网的各类网站中都可以看到推荐系统的应用，尽管不同网站使用的技术不同，但总的来说几乎所有的推荐系统应用都是由前台的展示页面、后台的日志系统以及推荐算法系统构成。</p>
<ul>
<li>电子商务：<a href="https://www.taobao.com/" target="_blank" rel="external">淘宝</a>、<a href="https://www.jd.com/" target="_blank" rel="external">京东</a>、<a href="https://www.amazon.com/" target="_blank" rel="external">亚马逊</a></li>
<li>电影/视频：<a href="https://www.netflix.com/" target="_blank" rel="external">Netflix</a>、<a href="https://www.youtube.com/" target="_blank" rel="external">YouTube</a>、<a href="http://www.iqiyi.com/" target="_blank" rel="external">爱奇艺</a></li>
<li>音乐：<a href="http://www.pandora.com/" target="_blank" rel="external">Pandora</a>、<a href="http://music.163.com/" target="_blank" rel="external">网易云音乐</a>、<a href="https://douban.fm/" target="_blank" rel="external">豆瓣FM</a></li>
<li>社交网络：<a href="https://www.facebook.com/" target="_blank" rel="external">Facebook</a>、<a href="https://twitter.com/" target="_blank" rel="external">Twitter</a>、<a href="http://www.linkedin.com/" target="_blank" rel="external">LinkedIn</a>、<a href="http://weibo.com/" target="_blank" rel="external">新浪微博</a></li>
<li>个性化阅读：<a href="http://digg.com/" target="_blank" rel="external">Digg</a>、<a href="https://flipboard.com/" target="_blank" rel="external">Flipboard</a>、<a href="http://www.toutiao.com/" target="_blank" rel="external">今日头条</a></li>
<li>基于位置的服务：<a href="https://foursquare.com/" target="_blank" rel="external">Foursquare</a></li>
<li>个性化广告：<a href="https://developers.facebook.com/docs/audience-network/" target="_blank" rel="external">Facebook Audience Network</a></li>
</ul>
<h2 id="推荐系统实验方法"><a href="#推荐系统实验方法" class="headerlink" title="推荐系统实验方法"></a>推荐系统实验方法</h2><p>在推荐系统中，主要有三种评测推荐效果的实验方法：离线实验、用户调查、在线实验。</p>
<h2 id="推荐系统评测指标"><a href="#推荐系统评测指标" class="headerlink" title="推荐系统评测指标"></a>推荐系统评测指标</h2><ul>
<li>用户满意度：用户的主观感受，主要通过用户调查的方式获得，也可以间接从用户行为统计中得到。</li>
<li>预测准确度：度量一个推荐系统或推荐算法预测用户行为的能力。评分预测的预测准确度一般通过计算测试集和训练集的均方根误差(RMSE)和平均绝对误差(MAE)得到。TopN推荐的预测准确度一般通过计算测试集和训练集的准确率(precison)和召回率(recall)得到。</li>
</ul>
<blockquote>
<p>令r<sub>ui</sub>是用户u对物品i的实际评分，r<sup>^</sup><sub>ui</sub>是推荐算法给出的预测评分，T是测试集，那么：<br>RMSE = sqrt(Σ<sub>u,i∈T</sub>(r<sub>ui</sub>-r<sup>^</sup><sub>ui</sub>)<sup>2</sup> / |T|)<br>MAE = Σ<sub>u,i∈T</sub>|r<sub>ui</sub>-r<sup>^</sup><sub>ui</sub>| / |T|</p>
<p>令R(u)是用户u在训练集上的推荐结果，T(u)是用户u在测试集上的行为结果，U是用户集合，那么：<br> Precision = Σ<sub>u∈U</sub>|R(u) ∩ T(u)| / Σ<sub>u∈U</sub>|R(u)|<br> Recall = Σ<sub>u∈U</sub>|R(u) ∩ T(u)| / Σ<sub>u∈U</sub>|T(u)|</p>
</blockquote>
<ul>
<li>覆盖率：描述一个推荐系统对物品长尾的发掘能力。</li>
</ul>
<blockquote>
<p>假设用户集合为U，物品集合为I，推荐系统给每个用户推荐一个长度为N的物品列表R(u)，那么：<br>Coverage = |∪<sub>u∈U</sub>R(u)| / |I|</p>
</blockquote>
<ul>
<li>多样性：为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域。</li>
<li>新颖性：是指给用户推荐那些他们以前没听说过的商品。</li>
<li>惊喜度(serendipity)：如果推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高。</li>
<li>信任度：提高信任度的方法是给出合理的推荐解释。</li>
<li>实时性：推荐系统需要实时地更新推荐列表来满足用户新的行为变化，并且需要能够将新加入系统的物品推荐给用户。</li>
<li>健壮性(robust)：衡量一个推荐系统抗击作弊的能力。</li>
</ul>
<p>在众多指标中，作者认为：对于可以离线优化的指标，应该在给定覆盖率、多样性、新颖性等限制条件下，尽量优化预测准确度。</p>
<h2 id="常见推荐系统算法"><a href="#常见推荐系统算法" class="headerlink" title="常见推荐系统算法"></a>常见推荐系统算法</h2><p>推荐系统是联系用户和物品的媒介，而推荐联系用户和物品的方式主要有3种，如下图所示。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-b37cbd23e2424475.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="3种联系用户和物品的推荐系统" title="">
                </div>
                <div class="image-caption">3种联系用户和物品的推荐系统</div>
            </figure>
<p>第一种方法，首先找到用户喜欢的物品，然后找到与这些物品相似的物品推荐给用户。基于这种方法可以给出如下的推荐解释：购买了该商品的用户也经常购买这些商品。这种方法通常被称为基于物品的协同过滤算法(item-based collaborative filtering)。<br>第二种方法，首先找到和用户有相似兴趣的其他用户，然后推荐这些其他用户喜欢的物品。这种方法通常被称为基于用户的协同过滤算法(user-based collaborative filtering)。<br>第三种方法，首先找到用户感兴趣的物品特征，然后推荐包含这些特征的物品。这种方法核心思想是通过隐含特征联系用户兴趣和物品，通常被称为隐语义模型算法(latent factor model)。</p>
<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>个性化推荐系统的一个重要算法是基于用户行为分析，学术界一般将这种类型的算法称为协同过滤算法(collaborative filtering)。</p>
<blockquote>
<p>顾名思义，协同过滤就是指用户可以齐心协力，通过不断地和网站互动，使自己的推荐列表能够不断过滤掉自己不感兴趣的物品，从而越来越满足自己的需求。</p>
</blockquote>
<h2 id="基于物品的协同过滤算法"><a href="#基于物品的协同过滤算法" class="headerlink" title="基于物品的协同过滤算法"></a>基于物品的协同过滤算法</h2><p>基于物品的协同过滤算法(以下简称ItemCF)，是目前业界应用最多的算法，最早由电子商务公司亚马逊提出。ItemCF算法给用户推荐那些和他们之前喜欢的物品相似的物品，它的主要步骤分为两步。</p>
<blockquote>
<p>(1) 计算物品之间的相似度<br>   (2) 根据物品的相似度和用户的历史行为给用户生成推荐列表</p>
</blockquote>
<p>第一步计算相似度可用余弦相似度公式</p>
<blockquote>
<p>令N(i)是喜欢物品i的用户集合，那么物品i和物品j的相似度可定义为：<br>   w<sub>ij</sub> = |N(i) ∩ N(j)| / sqrt(|N(i)||N(j)|)</p>
</blockquote>
<p>第二步计算用户对物品的兴趣，如下公式的含义是：和用户历史上感兴趣的物品越相似的物品，越有可能在用户的推荐列表中获得比较高的排名。</p>
<blockquote>
<p>令p<sub>uj</sub>为用户u对物品j的兴趣，w<sub>ji</sub>是物品j和物品i的相似度，r<sub>ui</sub>是用户u对物品i的兴趣（对于隐反馈数据集，如果用户u对物品i有过行为，可简单令r<sub>ui</sub>=1），S(j,K)是和物品j最相似的K个物品的集合，那么：<br>   p<sub>uj</sub> = Σ<sub>i∈N(u)∩S(j,K)</sub> w<sub>ji</sub>r<sub>ui</sub></p>
</blockquote>
<p>最后选取该用户兴趣值最高的N的物品作为推荐列表。</p>
<h2 id="基于用户的协同过滤算法"><a href="#基于用户的协同过滤算法" class="headerlink" title="基于用户的协同过滤算法"></a>基于用户的协同过滤算法</h2><p>基于用户的协同过滤算法(以下简称UserCF)，是推荐系统中最古老的算法。UserCF算法先找到和他有相似兴趣的其他用户，然后把那些用户喜欢的、而他没有听说过的物品推荐给他，它的主要步骤分为两步。</p>
<blockquote>
<p>(1) 找到和目标用户兴趣相似的用户集合<br>   (2) 找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户</p>
</blockquote>
<p>第一步计算用户的兴趣相似度可用余弦相似度公式</p>
<blockquote>
<p>令N(u)是用户u曾经有过正反馈的物品集合，那么用户u和用户v的相似度可定义为：<br>   w<sub>uv</sub> = |N(u) ∩ N(v)| / sqrt(|N(u)||N(v)|)</p>
</blockquote>
<p>第二步计算用户对物品的兴趣</p>
<blockquote>
<p>令p<sub>ui</sub>为用户u对物品i的兴趣，w<sub>uv</sub>是用户u和用户v的相似度，r<sub>vi</sub>是用户v对物品i的兴趣（对于隐反馈数据集，如果用户v对物品i有过行为，可简单令r<sub>vi</sub>=1），S(u,K)是和用户u兴趣最相似的K个用户的集合，那么：<br>   p<sub>ui</sub> = Σ<sub>v∈N(i)∩S(u,K)</sub> w<sub>uv</sub>r<sub>vi</sub></p>
</blockquote>
<p>最后选取该用户兴趣值最高的N的物品作为推荐列表。</p>
<h2 id="隐语义模型"><a href="#隐语义模型" class="headerlink" title="隐语义模型"></a>隐语义模型</h2><p>隐语义模型算法(以下简称LFM)，是最近几年推荐系统领域最为热门的研究话题。LFM算法的核心思想是通过隐含特征联系用户兴趣和物品，它的主要步骤分为三步。</p>
<blockquote>
<p>(1) 对物品进行分类<br>   (2) 确定用户对哪些类的物品感兴趣以及感兴趣的程度<br>   (3) 对于给定的类，确定物品在这个类的权重，并且选择性地推荐给用户</p>
</blockquote>
<p>关于如何给物品分类，一个简单方案是由编辑来手动分类，但这样存在很强的主观性和较大的工作量。为了解决这个困难，研究人员提出可以从用户数据出发，基于隐含语义分析技术(latent variable analysis)自动找到哪些类，然后进行个性化推荐。隐含语义分析技术有很多著名的模型和方法，比如pLSA、LDA、隐含类别模型、隐含主题模型、矩阵分解等。</p>
<blockquote>
<p>LFM通过如下公式计算用户u对物品i的兴趣：<br>   Preference<sub>ui</sub> =  Σ<sub>k∈[1,K]</sub> p<sub>u,k</sub>q<sub>i,k</sub><br>   其中p<sub>u,k</sub>度量了用户u的兴趣和第k个隐类的关系，而q<sub>i,k</sub>度量了第k个隐类和物品i的关系。这两个参数的计算需要一点最优化理论或者机器学习的知识，这里不多作介绍。</p>
</blockquote>
<h2 id="三种算法的优缺点比较"><a href="#三种算法的优缺点比较" class="headerlink" title="三种算法的优缺点比较"></a>三种算法的优缺点比较</h2><ul>
<li>LFM是一种基于机器学习的算法，有较好的理论基础。ItemCF/UserCF是基于邻域的方法，更多的是一种基于统计的方法，没有学习过程。</li>
<li>假设有M个用户和N个物品，选取F个隐类。UserCF需要存储用户的相似度矩阵，存储空间是O(M*M)。ItemCF需要存储物品的相似度矩阵，存储空间是O(N*N)。LFM需要的存储空间是O(F*(M+N))。如果用户数很多，UserCF将会占据很大的内存。如果物品数很多，ItemCF将会占据很大的内存。LFM存储空间最少，这在M和N很大时可以很好地节省离线计算的内存。</li>
<li>假设有M个用户和N个物品和K条用户对物品的行为记录。那么，UserCF计算用户表的时间复杂度是O(N*(K/N)<sup>2</sup>)，而ItemCF计算物品表的时间复杂度是O(M*(K/M)<sup>2</sup>)。而对于LFM，如果用F个隐类，迭代S次，那么它的时间复杂度是O(K*F*S)。在一般情况下，LFM的时间复杂度要稍微高于UserCF和ItemCF，主要是因为该算法需要多次迭代。</li>
<li>ItemCF算法支持很好的推荐解释，它可以利用用户的历史行为解释推荐结果，但LFM无法提供这样的解释。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在互联网应用中可以看到大量推荐系统的应用，它主要解决了信息过载的问题，通过算法主动帮助用户找到自己感兴趣的内容。常见的推荐系统算法有三种，分别代表三种联系用户和物品的方式，它们是：基于物品的协同过滤算法(ItemCF)，基于用户的协同过滤算法(ItemCF)，隐语义模型算法(LFM)。三种方法各有优劣，需要根据实际场景选择合适的算法，通过不断优化指标找到最优算法。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要是对项亮的&lt;a href=&quot;https://book.douban.com/subject/10769749/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;推荐系统实践&lt;/a&gt;部分章节进行了一些总结，先从什么是推荐系统开始讲起，然后介绍了评测推荐
    
    </summary>
    
    
      <category term="推荐系统" scheme="http://www.secondplayer.top/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="读书笔记" scheme="http://www.secondplayer.top/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>使用redis的有序集合实现排行榜功能</title>
    <link href="http://www.secondplayer.top/2017/07/23/redis-sorted-set/"/>
    <id>http://www.secondplayer.top/2017/07/23/redis-sorted-set/</id>
    <published>2017-07-23T05:43:36.000Z</published>
    <updated>2017-09-04T15:34:43.987Z</updated>
    
    <content type="html"><![CDATA[<p>排行榜是业务开发中常见的一个场景，如何设计一个好的数据结构能够满足高效实时的查询，下面我们结合一个实际例子来讨论一下。</p>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>选手报名参加活动，观众可以对选手进行投票，每个观众对同一名选手只能投一票，活动期间最多投四票。后台需要提供如下接口：</p>
<ul>
<li>接口1：返回TOP 10的选手信息及投票数</li>
<li>接口2：返回活动总参与选手数及总投票数</li>
<li>接口3：对于每个选手，返回自己的投票数，排名，距离上一名差的票数</li>
</ul>
<h2 id="基于数据库的方案"><a href="#基于数据库的方案" class="headerlink" title="基于数据库的方案"></a>基于数据库的方案</h2><p>首先需要一张表存储投票记录，一次投票就是一条记录。这张表相当于投票明细，判断每人只投一张票以及最多投四张表都依赖对这张表的查询。<br>如果直接对这张表做TOP 10的查询，则需要根据选手id做聚合查询，这样每次查询必然耗时。为了优化查询，可以增加另一张排行榜表，用一个定时任务每隔一段时间对原表做聚合查询，然后将结果写进排行榜表里，表里包含投票数及排名的字段，这样查询TOP 10和排名的时候直接查这张表。引入另一张表加快了性能，但牺牲了实时性，活动说明里需加上类似“榜单数据每10分钟同步一次”的话来告知用户。</p>
<h2 id="基于redis的方案"><a href="#基于redis的方案" class="headerlink" title="基于redis的方案"></a>基于redis的方案</h2><p>对于排行榜的需求，redis有一个数据结构非常适合做这件事，那就是有序集合(sorted set)。</p>
<h3 id="redis的有序集合相关命令"><a href="#redis的有序集合相关命令" class="headerlink" title="redis的有序集合相关命令"></a>redis的有序集合相关命令</h3><p>有序集合和集合一样可以存储字符串，另外有序集合的成员可以关联一个分数(score)，这个分数用于集合排序。下面以投票为例说明常见的命令，vote_activity是有序集合的key。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">#给Alice投票</div><div class="line">redis&gt; zincrby vote_activity 1 Alice</div><div class="line">&quot;1&quot; </div><div class="line">#给Bob投票</div><div class="line">redis&gt; zincrby vote_activity 1 Bob</div><div class="line">&quot;1&quot;</div><div class="line">#给Alice投票</div><div class="line">redis&gt; zincrby vote_activity 1 Alice</div><div class="line">&quot;2&quot;</div><div class="line">#查看Alice投票数</div><div class="line">redis&gt; zscore vote_activity Alice</div><div class="line">&quot;2&quot;</div><div class="line">#获取Alice排名(从高到低，zero-based)</div><div class="line">redis&gt; zrevrank vote_activity Alice</div><div class="line">(integer) 0</div><div class="line">#获取前10名(从高到低)</div><div class="line">redis&gt; zrevrange vote_activity 0 9</div><div class="line">1) &quot;Alice&quot;</div><div class="line">2) &quot;Bob&quot;</div><div class="line">#获取前10名及对应的分数(从高到低)</div><div class="line">redis&gt; zrevrange vote_activity 0 9 withscores</div><div class="line">1) &quot;Alice&quot;</div><div class="line">2) &quot;2&quot;</div><div class="line">3) &quot;Bob&quot;</div><div class="line">4) &quot;1&quot;</div><div class="line">#获取总参与选手数</div><div class="line">redis&gt; zcard vote_activity</div><div class="line">(integer) 2</div></pre></td></tr></table></figure></p>
<h3 id="接口实现"><a href="#接口实现" class="headerlink" title="接口实现"></a>接口实现</h3><p>回到最开始的场景，大部分需求都已经得到满足，还剩下两个数据需要单独说一下。接口2中的总投票数没有直接的接口获得，一种方法是先用<a href="https://redis.io/commands/zrange" target="_blank" rel="external">ZRANGE</a>遍历所有的key，然后对score进行求和，另一种方法是对总票数单独用一个数据结构存储。接口3的距离上一名差的票数，先用<a href="https://redis.io/commands/zrevrank" target="_blank" rel="external">ZREVRANK</a>获取自己排名，然后用<a href="https://redis.io/commands/zrevrange" target="_blank" rel="external">ZREVRANGE</a>获取上一排名的分数，最后用自己的分数减去上一名的分数即可，代码示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_step</span><span class="params">(redis_key, member)</span>:</span></div><div class="line">    next_step = <span class="keyword">None</span></div><div class="line">    score = redis.zscore(redis_key, member)</div><div class="line">    rank = redis.zrevrank(redis_key, member)</div><div class="line">    <span class="keyword">if</span> rank &gt; <span class="number">0</span>:</div><div class="line">        next_member = redis.zrevrange(redis_key, rank - <span class="number">1</span>, rank - <span class="number">1</span>, withscores=<span class="keyword">True</span>)</div><div class="line">        next_step = next_member[<span class="number">0</span>][<span class="number">1</span>] - score</div><div class="line">    <span class="keyword">return</span> next_step</div></pre></td></tr></table></figure></p>
<p>另外如果两个key的score相同，排序逻辑是按照key的字母序排序。在有些情况下这个可能不满足实际要求，因此需要按实际情况重新设计key。比如如果要求同分数情况下按时间排序，那么key最好加上时间戳前缀。</p>
<h3 id="redis与数据库的同步"><a href="#redis与数据库的同步" class="headerlink" title="redis与数据库的同步"></a>redis与数据库的同步</h3><p>redis通常是作为缓存层加速查询的，如果数据没有做持久化则有概率会丢失数据。一个方案是用定时任务定时同步redis与数据库的数据，数据库里存储着原始数据，通过计算数据库的数据和redis做对比，可以修正由于redis不稳定导致的数据不一致。这里需要注意的是在同步过程时redis的数据有可能还在增长，因此最好先读redis的数据，然后记下时间，查询指定时间段里的数据库的数据，最后再用<a href="https://redis.io/commands/zincrby" target="_blank" rel="external">ZINCRBY</a>增量修正redis数据，而不是直接用<a href="https://redis.io/commands/zadd" target="_blank" rel="external">ZADD</a>覆盖redis数据。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>redis的有序集合是一个非常高效的数据结构，可以替代数据库里一些很难实现的操作。它的一个典型应用场景就是排行榜，通过ZRANK可以快速得到用户的排名，通过ZRANGE可以快速得到TOP N的用户列表，它们的复杂度都是O(log(N))，用来替代数据库查询可以大大提升性能。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;排行榜是业务开发中常见的一个场景，如何设计一个好的数据结构能够满足高效实时的查询，下面我们结合一个实际例子来讨论一下。&lt;/p&gt;
&lt;h2 id=&quot;场景&quot;&gt;&lt;a href=&quot;#场景&quot; class=&quot;headerlink&quot; title=&quot;场景&quot;&gt;&lt;/a&gt;场景&lt;/h2&gt;&lt;p&gt;选手报
    
    </summary>
    
    
      <category term="Redis" scheme="http://www.secondplayer.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>使用redis实现分布式锁</title>
    <link href="http://www.secondplayer.top/2017/07/16/redis-distribution-lock/"/>
    <id>http://www.secondplayer.top/2017/07/16/redis-distribution-lock/</id>
    <published>2017-07-16T05:40:10.000Z</published>
    <updated>2017-09-04T15:34:29.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在类似秒杀这样的并发场景下，为了确保同一时刻只能允许一个用户访问资源，需要利用加锁的机制控制资源的访问权。如果服务只在单台机器上运行，可以简单地用一个内存变量进行控制。而在多台机器的系统上，则需要用分布式锁的机制进行并发控制。基于redis的一些特性，利用redis可以既方便又高效地模拟锁的实现。</p>
<h2 id="一个简单方案"><a href="#一个简单方案" class="headerlink" title="一个简单方案"></a>一个简单方案</h2><p>让我们先从一个简单的实现说起，这里用到了redis的两个命令，<a href="https://redis.io/commands/setnx" target="_blank" rel="external">SETNX</a>和<a href="https://redis.io/commands/expire" target="_blank" rel="external">EXPIRE</a>。如果lock_key不存在，那么就设置lock_key的值为1，并且设置过期时间；如果lock_key存在，说明已经有人在使用这把锁，访问失败。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquire_lock</span><span class="params">(lock_key, expire_timeout=<span class="number">60</span>)</span>:</span></div><div class="line">    <span class="keyword">if</span> redis.setnx(lock_key, <span class="number">1</span>):</div><div class="line">        redis.expire(lock_key, expire_timeout)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure></p>
<p>逻辑上看似乎没有问题，但是考虑一下异常情况：如果setnx设置成功，但expire由于某些原因（比如超时）操作失败，那么这把锁就永远存在了，也就是所谓的死锁，后面的人永远无法访问这个资源。</p>
<h2 id="利用时间戳取值的方案"><a href="#利用时间戳取值的方案" class="headerlink" title="利用时间戳取值的方案"></a>利用时间戳取值的方案</h2><p>为了解决死锁，我们可以利用setnx的value来做文章。上例中的我们设的value是1，其实并没有派上用场。因此可以考虑将value设为当前时间加上expire_timeout，当setnx设置失败后，我们去读lock_key的value，并且和当前时间作比对，如果当前时间大于value，那么资源理当被释放。代码示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquire_lock</span><span class="params">(lock_key, expire_timeout=<span class="number">60</span>)</span>:</span></div><div class="line">    expire_time = int(time.time()) + expire_timeout</div><div class="line">    <span class="keyword">if</span> redis.setnx(lock_key, expire_time):</div><div class="line">        redis.expire(lock_key, expire_timeout)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    redis_value = redis.get(lock_key)</div><div class="line">    <span class="keyword">if</span> redis_value <span class="keyword">and</span> int(time.time()) &gt; int(redis_value):</div><div class="line">        redis.delete(lock_key)</div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure></p>
<p>然而仔细推敲下这段代码仍然能发现一些问题。第一，这个方案依赖时间，如果在分布式系统中的时间没有同步，则会对方案产生一定偏差。第二，假设C1和C2都没拿到锁，它们都去读value并对比时间，在竞态条件(race condition)下可能产生如下的时序：C1删除lock_key，C1获得锁，C2删除lock_key，C2获得锁。这样C1和C2同时拿到了锁，显然是不对的。</p>
<h2 id="改进后的方案"><a href="#改进后的方案" class="headerlink" title="改进后的方案"></a>改进后的方案</h2><p>幸运的是，redis里还有一个指令可以帮助我们解决这个问题。<a href="https://redis.io/commands/getset" target="_blank" rel="external">GETSET</a>指令在set新值的同时会返回老的值，这样的话我们可以检查返回的值，如果该值和之前读出来的值相同，那么这次操作有效，反之则无效。代码示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquire_lock</span><span class="params">(lock_key, expire_timeout=<span class="number">60</span>)</span>:</span></div><div class="line">    expire_time = int(time.time()) + expire_timeout</div><div class="line">    <span class="keyword">if</span> redis.setnx(lock_key, expire_time):</div><div class="line">        redis.expire(lock_key, expire_timeout)</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    redis_value = redis.get(lock_key)</div><div class="line">    <span class="keyword">if</span> redis_value <span class="keyword">and</span> int(time.time()) &gt; int(redis_value):</div><div class="line">        expire_time = int(time.time()) + expire_timeout</div><div class="line">        old_value = redis.getset(lock_key, expire_time)</div><div class="line">        <span class="keyword">if</span> int(old_value) == int(redis_value):</div><div class="line">            <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div></pre></td></tr></table></figure></p>
<p>这个方案基本可以满足要求，除了有一个小瑕疵，由于getset会去修改value，在竞态条件下可能会被修改多次导致timeout有细微的误差，但这个对结果影响不大。</p>
<h2 id="最终方案"><a href="#最终方案" class="headerlink" title="最终方案"></a>最终方案</h2><p>以上方案实现起来略显繁琐，但从redis 2.6.12版本开始有一个更为简便的方法。我们可以使用<a href="https://redis.io/commands/set" target="_blank" rel="external">SET</a>指令的扩展 <strong> SET key value [EX seconds] [PX milliseconds] [NX|XX] </strong>，这个指令相当于对SETNX和EXPIRES进行了合并，因而我们的算法可以简化为如下一行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquire_lock</span><span class="params">(lock_key, expire_timeout=<span class="number">60</span>)</span>:</span></div><div class="line">    ret = redis.set(lock_key, int(time.time()), nx=<span class="keyword">True</span>, ex=expire_timeout):</div><div class="line">    <span class="keyword">return</span> ret</div></pre></td></tr></table></figure></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在redis 2.6.12版本之后我们可以用一个简单的SET命令实现分布式锁，而在此版本之前则需要将SETNX和GETSET配合使用一个较为繁琐的方案。简化后的方案对于开发者来说当然是好事，但通过学习这一演变过程我们会对问题有更深刻的印象。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在类似秒杀这样的并发场景下，为了确保同一时刻只能允许一个用户访问资源，需要利用加锁的机制控制资源的访问权。如果服务只在单台机器上运行，可以简
    
    </summary>
    
    
      <category term="Redis" scheme="http://www.secondplayer.top/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>使用Hexo搭建个人静态博客</title>
    <link href="http://www.secondplayer.top/2016/06/12/hexo-blog-setup/"/>
    <id>http://www.secondplayer.top/2016/06/12/hexo-blog-setup/</id>
    <published>2016-06-12T06:42:23.000Z</published>
    <updated>2017-09-04T15:34:09.500Z</updated>
    
    <content type="html"><![CDATA[<p>最近有时间折腾了一下建一个个人博客，在对比了几家之后，最终决定用<a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>作为框架，<a href="https://github.com/" target="_blank" rel="external">GitHub</a>作为部署平台搭建博客。VPS选用的是<a href="https://aws.amazon.com/" target="_blank" rel="external">AWS</a>，新用户可以免费使用1年的EC2，足够用来体验了。</p>
<h2 id="申请VPS"><a href="#申请VPS" class="headerlink" title="申请VPS"></a>申请VPS</h2><p>VPS指的是虚拟服务器，国内推荐用<a href="https://www.aliyun.com/" target="_blank" rel="external">阿里云</a>，国外推荐用<a href="https://www.linode.com/" target="_blank" rel="external">Linode</a>, <a href="https://www.digitalocean.com/" target="_blank" rel="external">Digital Ocean</a>, <a href="https://aws.amazon.com/" target="_blank" rel="external">AWS</a>。我选择的是AWS，主要有几个原因，一是因为新用户可以试用免费1年，二是因为公司用的就是AWS，对其各项操作比较熟悉，最后一个原因是选择一个国外服务器可以自己搭建ShadowSocks<a href="https://segmentfault.com/a/1190000003101075" target="_blank" rel="external">科学上网</a>。</p>
<h2 id="申请域名"><a href="#申请域名" class="headerlink" title="申请域名"></a>申请域名</h2><p>域名申请服务商，国内有<a href="https://wanwang.aliyun.com/" target="_blank" rel="external">万网</a>，<a href="http://www.cndns.com/" target="_blank" rel="external">美橙</a>，国外有<a href="https://www.godaddy.com/" target="_blank" rel="external">GoDaddy</a>, <a href="https://www.namecheap.com/" target="_blank" rel="external">NameCheap</a>。我选择的是NameCheap，主要因为价格因素。你要问国内的那些更便宜为啥不选？呵呵国内的情况你懂的。</p>
<h2 id="DNS解析"><a href="#DNS解析" class="headerlink" title="DNS解析"></a>DNS解析</h2><p>DNS解析推荐<a href="https://www.dnspod.cn/" target="_blank" rel="external">DNSPOD</a>，业界良心，服务免费且强大。域名绑定前记得先到NameCheap控制台设置DNS解析到DNSPOD提供的两个免费DNS解析服务器，具体参考<a href="https://support.dnspod.cn/Kb/showarticle/?qtype=%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B&amp;tsid=47" target="_blank" rel="external">这里</a>。</p>
<h2 id="Hexo安装及配置"><a href="#Hexo安装及配置" class="headerlink" title="Hexo安装及配置"></a>Hexo安装及配置</h2><p>前面把主机和域名搞定了，现在开始在主机上搭建博客了。提到博客，一般都会选用经典的<a href="https://www.wordpress.com/" target="_blank" rel="external">WordPress</a>搭建。不过现在越来越多的个人博客都采用静态博客框架，典型的如<a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>， <a href="https://jekyllrb.com/" target="_blank" rel="external">Jekyll</a>， <a href="https://github.com/octopress/octopress" target="_blank" rel="external">Octopress</a>。从流行度和技术栈的角度来看，我倾向于选择Hexo。<br><a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>是一个用Node.js搭建的博客框架，简单强大易上手。静态文件用<a href="http://daringfireball.net/projects/markdown/" target="_blank" rel="external">Markdown</a>编写，Hexo会根据静态文件自动生成网页。</p>
<h3 id="安装依赖环境"><a href="#安装依赖环境" class="headerlink" title="安装依赖环境"></a>安装依赖环境</h3><ul>
<li>安装<a href="https://git-scm.com/" target="_blank" rel="external">Git</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install git</div></pre></td></tr></table></figure>
<ul>
<li>安装<a href="https://nodejs.org/" target="_blank" rel="external">Node.js</a></li>
</ul>
<p>通常用<a href="https://www.npmjs.com/package/nvm" target="_blank" rel="external">nvm</a>(Node.js Version Manager)安装Node环境<br>安装必要环境<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install build-essential libssl-dev</div></pre></td></tr></table></figure></p>
<p>下载nvm<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ curl https://raw.github.com/creationix/nvm/master/install.sh | sh</div></pre></td></tr></table></figure></p>
<p>查看可用版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nvm ls-remote</div></pre></td></tr></table></figure></p>
<p>选取最新版本，这里我们安装v4.4.5，并将其设为默认<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ nvm install 4.4.5</div><div class="line">$ nvm <span class="built_in">alias</span> default 4.4.5</div><div class="line">$ nvm use default</div></pre></td></tr></table></figure></p>
<ul>
<li>安装<a href="https://hexo.io/" target="_blank" rel="external">Hexo</a></li>
</ul>
<p>我们通过npm分别安装hexo客户端和服务端</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo-cli</div><div class="line">$ npm install -g hexo-server</div></pre></td></tr></table></figure>
<h3 id="生成文章"><a href="#生成文章" class="headerlink" title="生成文章"></a>生成文章</h3><ul>
<li>初始化hexo环境</li>
</ul>
<p>我们把hexo_blog作为博客目录名，首先初始化hexo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ hexo init ~/hexo_blog</div><div class="line">$ <span class="built_in">cd</span> ~/hexo_blog</div><div class="line">$ npm install</div></pre></td></tr></table></figure>
<ul>
<li>修改配置文件_config.yml</li>
</ul>
<p>配置Site, URL, Directory, Writing等基本信息，详细参考这篇<a href="https://hexo.io/docs/configuration.html" target="_blank" rel="external">配置文档</a><br>这里建议设置default_layout为draft，这样默认生成文章在Draft里，确认后再发布到Public。</p>
<h3 id="发布文章"><a href="#发布文章" class="headerlink" title="发布文章"></a>发布文章</h3><ul>
<li>新建文章，以名称first_post为例</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new first-post</div></pre></td></tr></table></figure>
<ul>
<li>编辑文章，文章都存放在source目录下</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ vim ~/hexo_blog/<span class="built_in">source</span>/_drafts/first-post.md</div></pre></td></tr></table></figure>
<ul>
<li>发布文章，这将会把文章从draft移到post目录</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo publish first-post</div></pre></td></tr></table></figure>
<h3 id="运行服务"><a href="#运行服务" class="headerlink" title="运行服务"></a>运行服务</h3><ul>
<li>启动服务器，默认起在4000端口，成功后访问<a href="http://localhost:4000" target="_blank" rel="external">http://localhost:4000</a> 预览效果</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<h3 id="部署博客"><a href="#部署博客" class="headerlink" title="部署博客"></a>部署博客</h3><p>我们需要选择一个静态文件的托管平台，首选<a href="https://github.com/" target="_blank" rel="external">GitHub</a>，国内可以考虑<a href="https://coding.net/" target="_blank" rel="external">Coding</a>（最近收购了<a href="http://gitcafe.com/" target="_blank" rel="external">GitCafe</a>）。</p>
<ul>
<li>创建GitHub Repository</li>
</ul>
<p>参考这个<a href="https://help.github.com/articles/creating-a-new-repository/" target="_blank" rel="external">步骤</a>，创建一个名为hexo_static的repo，注意设置为Public</p>
<ul>
<li>修改配置文件_config.yml，注意替换<em>$username</em></li>
</ul>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="attr">deploy:</span> </div><div class="line"><span class="attr">  type:</span> <span class="string">git</span> </div><div class="line"><span class="attr">  repo:</span> <span class="attr">https://github.com/$username/hexo_static.git</span></div><div class="line"><span class="attr">  branch:</span> <span class="string">master</span></div></pre></td></tr></table></figure>
<ul>
<li>安装hexo git插件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install hexo-deployer-git --save</div></pre></td></tr></table></figure>
<ul>
<li>部署</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>按照提示输入用户名和密码，一切步骤完成后，所有文件都已生成并提交到Git上了</p>
<h3 id="自动化"><a href="#自动化" class="headerlink" title="自动化"></a>自动化</h3><p>整个自动化的思路是：运行该脚本，生成博客静态文件，通过hexo deploy实现自动提交到Git，然后通过本地更新代码，对关联的空分支进行git push操作，触发post-receive钩子，从而将静态文件同步到/var/www/hexo目录，而该目录正是Nginx将80端口转发到本地的路径。</p>
<ul>
<li>初始化空仓库</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git init --bare ~/hexo_bare</div></pre></td></tr></table></figure>
<ul>
<li>创建git hooks</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ vim ~/hexo_bare/hooks/post-receive</div></pre></td></tr></table></figure>
<p>这里我们用到了post-receive这个钩子，当一个本地仓库执行git push后会触发。post-receive具体内容为</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line">git --work-tree=/var/www/hexo --git-dir=/home/<span class="variable">$USER</span>/hexo_bare checkout -f</div></pre></td></tr></table></figure>
<ul>
<li>将空仓库关联到主仓库</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ git <span class="built_in">clone</span> https://github.com/<span class="variable">$username</span>/hexo_static.git ~/hexo_static </div><div class="line">$ <span class="built_in">cd</span> ~/hexo_static</div><div class="line">$ git remote add live ~/hexo_bare</div></pre></td></tr></table></figure>
<ul>
<li>创建自动化脚本</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ vim ~/hexo_blog/hexo_git_deploy.sh</div></pre></td></tr></table></figure>
<p>脚本内容为</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line">hexo clean</div><div class="line">hexo generate </div><div class="line">hexo deploy</div><div class="line"></div><div class="line">( <span class="built_in">cd</span> ~/hexo_static ; git pull ; git push live master)</div></pre></td></tr></table></figure>
<ul>
<li>Nginx配置</li>
</ul>
<p>创建/var/www/hexo目录，稍后会将Nginx的请求映射到该目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo mkdir -p /var/www/hexo</div><div class="line">$ sudo chown -R <span class="variable">$USER</span>:<span class="variable">$USER</span> /var/www/hexo</div><div class="line">$ sudo chmod -R 755 /var/www/hexo</div></pre></td></tr></table></figure>
<p>编辑/etc/nginx/sites-available/default<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo vim /etc/nginx/sites-available/default</div></pre></td></tr></table></figure></p>
<p>配置Nginx将80端口的请求映射到/var/www/hexo目录下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">server &#123; </div><div class="line">    listen 80 default_server; </div><div class="line">    listen [::]:80 default_server ipv6only=on; </div><div class="line">    root /var/www/hexo; </div><div class="line">    index index.html index.htm;</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>重启Nginx</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo service nginx restart</div></pre></td></tr></table></figure>
<h3 id="发布流程"><a href="#发布流程" class="headerlink" title="发布流程"></a>发布流程</h3><p>至此，我们可以总结下今后发布文章或更新博客的流程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ hexo new my-post</div><div class="line">$ vim ~/hexo_blog/<span class="built_in">source</span>/_draft/my-post.md</div><div class="line">$ hexo publish my-post</div><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>接着运行hexo server，然后在<a href="http://localhost:4000" target="_blank" rel="external">http://localhost:4000</a> 上预览效果，如果不满意则继续修改my-post.md（此时在_post目录下），重新生成文件（hexo generate），再预览直到可以发布为止</p>
<p>而最终对外发布，我们只需要敲下一行命令就完成了</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ~/hexo_blog/hexo_git_deploy.sh</div></pre></td></tr></table></figure>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul>
<li>主题</li>
</ul>
<p>默认hexo的主题是<a href="https://github.com/hexojs/hexo-theme-landscape" target="_blank" rel="external">landscape</a>，如果你想与众不同的话，可以用下别的主题或者自定义主题。官方收录的请点击<a href="https://hexo.io/themes/" target="_blank" rel="external">这里</a>，我选择的是<a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank" rel="external">indigo</a>，主要看中的是他的Material Design风格</p>
<ul>
<li>评论</li>
</ul>
<p>常见的评论系统有<a href="https://disqus.com/" target="_blank" rel="external">Disqus</a>，<a href="http://duoshuo.com/" target="_blank" rel="external">多说</a>，<a href="http://www.uyan.cc/" target="_blank" rel="external">友言</a>等，我选择的是多说。接入非常简单，去网站上注册个账号，然后将示例代码插到网页中即可</p>
<ul>
<li>统计</li>
</ul>
<p>流量统计选择<a href="http://tongji.cnzz.com/" target="_blank" rel="external">cnzz</a>(现已被整合进<a href="https://www.umeng.com/" target="_blank" rel="external">Umeng+</a>)</p>
<ul>
<li>监控</li>
</ul>
<p>可以接入<a href="http://www.jiankongbao.com/" target="_blank" rel="external">监控宝</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://hexo.io/docs/" target="_blank" rel="external">Hexo Documentation</a></li>
<li><a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-blog-with-hexo-on-ubuntu-14-04" target="_blank" rel="external">How to Create a Blog with Hexo On Ubuntu 14.04</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近有时间折腾了一下建一个个人博客，在对比了几家之后，最终决定用&lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;作为框架，&lt;a href=&quot;https://github.com/&quot; targ
    
    </summary>
    
    
      <category term="Hexo" scheme="http://www.secondplayer.top/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://www.secondplayer.top/2016/06/11/hello-world/"/>
    <id>http://www.secondplayer.top/2016/06/11/hello-world/</id>
    <published>2016-06-10T16:00:00.000Z</published>
    <updated>2017-09-04T15:37:50.579Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
