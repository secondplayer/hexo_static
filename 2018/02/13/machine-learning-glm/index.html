<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    <link rel="canonical" href="http://secondplayer.top/2018/02/13/machine-learning-glm/">
    
    
    <title>机器学习笔记3: 广义线性模型 | SecondPlayer&#39;s Blog | 我读书多，你别骗我</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="机器学习">
    <meta name="description" content="牛顿方法之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。 我们先来看下如何用牛顿方法(Newton’s Method)求解θ使得f(θ)=0。如下图所示，首先我们选取一个初始点，比如说令θ=4.5，然后作出f(θ)在该点的切线，这条切线与x轴相交的点θ=2.8作为下一次迭代的点。下右图又一次重复了一轮迭代，f(θ)在θ=2.8处的切线与x轴相交于θ=1.8处，然后再">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记3: 广义线性模型">
<meta property="og:url" content="http://www.secondplayer.top/2018/02/13/machine-learning-glm/index.html">
<meta property="og:site_name" content="SecondPlayer&#39;s Blog">
<meta property="og:description" content="牛顿方法之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。 我们先来看下如何用牛顿方法(Newton’s Method)求解θ使得f(θ)=0。如下图所示，首先我们选取一个初始点，比如说令θ=4.5，然后作出f(θ)在该点的切线，这条切线与x轴相交的点θ=2.8作为下一次迭代的点。下右图又一次重复了一轮迭代，f(θ)在θ=2.8处的切线与x轴相交于θ=1.8处，然后再">
<meta property="og:locale" content="zh">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-e223eff5a326a176.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-65bfb54bbcf3f406.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-610721634ffcbce7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-86dd59153cd92cde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-5da2b2bf72902d9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-42f222888a89a09b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-db162b1115ffbef7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-79cea267be2c0318.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-d8bc9a34458adf12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-451c3a74a1da1157.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-6158a49bfff73c84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-46fb19bd575608f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-15c413b119e8e891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-9a827bba15dd4d75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-c59f55335a32a473.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-461bd6d335040316.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-ca081bc1d3aa237d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-72434761f391890c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-d74f7c7d6b907b62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2245716-e31af89c3572377c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2018-02-13T08:33:18.819Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习笔记3: 广义线性模型">
<meta name="twitter:description" content="牛顿方法之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。 我们先来看下如何用牛顿方法(Newton’s Method)求解θ使得f(θ)=0。如下图所示，首先我们选取一个初始点，比如说令θ=4.5，然后作出f(θ)在该点的切线，这条切线与x轴相交的点θ=2.8作为下一次迭代的点。下右图又一次重复了一轮迭代，f(θ)在θ=2.8处的切线与x轴相交于θ=1.8处，然后再">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/2245716-e223eff5a326a176.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
    
        <link rel="alternate" type="application/atom+xml" title="SecondPlayer&#39;s Blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/gravatar.png">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">secondplayer</h5>
          <a href="mailto:secondplayer1990@gmail.com" title="secondplayer1990@gmail.com" class="mail">secondplayer1990@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/secondplayer" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="http://www.weibo.com/1901978451" target="_blank" >
                <i class="icon icon-lg icon-weibo"></i>
                Weibo
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">机器学习笔记3: 广义线性模型</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">机器学习笔记3: 广义线性模型</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-02-13T08:33:18.000Z" itemprop="datePublished" class="page-time">
  2018-02-13
</time>


            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#牛顿方法"><span class="post-toc-number">1.</span> <span class="post-toc-text">牛顿方法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#指数分布族"><span class="post-toc-number">2.</span> <span class="post-toc-text">指数分布族</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#广义线性模型"><span class="post-toc-number">3.</span> <span class="post-toc-text">广义线性模型</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#最小二乘法"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">最小二乘法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#逻辑回归"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">逻辑回归</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Softmax回归"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Softmax回归</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总结"><span class="post-toc-number">4.</span> <span class="post-toc-text">总结</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#参考资料"><span class="post-toc-number">5.</span> <span class="post-toc-text">参考资料</span></a></li></ol>
        </nav>
    </aside>


<article id="post-machine-learning-glm"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">机器学习笔记3: 广义线性模型</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-02-13 16:33:18" datetime="2018-02-13T08:33:18.000Z"  itemprop="datePublished">2018-02-13</time>

            


            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h2 id="牛顿方法"><a href="#牛顿方法" class="headerlink" title="牛顿方法"></a>牛顿方法</h2><p>之前我们在最大化对数似然函数l(θ)时用到了梯度上升法，现在我们介绍另一种方法。</p>
<p>我们先来看下如何用<strong>牛顿方法</strong>(Newton’s Method)求解θ使得f(θ)=0。如下图所示，首先我们选取一个初始点，比如说令θ=4.5，然后作出f(θ)在该点的切线，这条切线与x轴相交的点θ=2.8作为下一次迭代的点。下右图又一次重复了一轮迭代，f(θ)在θ=2.8处的切线与x轴相交于θ=1.8处，然后再次迭代到θ=1.3处。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-e223eff5a326a176.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>以此类推，我们得到迭代规则如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-65bfb54bbcf3f406.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>牛顿方法可以找到θ使得f(θ)=0，那么如何把它应用到最大化l(θ)上呢？当l(θ)达到最大点时，其导数为0，因此问题转化为找到θ使得l’(θ)=0。所以，令f(θ)=l’(θ)，我们推导出迭代规则：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-610721634ffcbce7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中的θ是参数为实数的情况，当θ为向量时，我们可以推导出更通用的公式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-86dd59153cd92cde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中∇<sub>θ</sub>l(θ)是指l(θ)的梯度，H是一个n <em> n的矩阵，被称为<em>*海森矩阵</em></em>(Hessian Matrix)。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-5da2b2bf72902d9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>和梯度下降法相比，牛顿方法收敛的速度更快，迭代的次数也更少。但是牛顿方法每次迭代的计算量更大，因为每次都要计算一个n阶矩阵的逆。总体而言，当n不是很大时牛顿方法计算的速度更快。当牛顿方法用来求解最大化对数似然函数l(θ)时，这个方法也被称为<strong>Fisher Scoring</strong>。</p>
<h2 id="指数分布族"><a href="#指数分布族" class="headerlink" title="指数分布族"></a>指数分布族</h2><p>到目前为止，我们分别学习了<strong>分类</strong>(classification)和<strong>回归</strong>(regression)两类问题。在回归问题里，我们假设p(y|x;θ)服从高斯分布N(0,σ<sup>2</sup>)；在分类问题里，我们假设p(y|x;θ)服从伯努利分布B(φ)。后面我们会看到，这两类问题可以被统一到一个更通用的模型，这个模型被称为<strong>广义线性模型</strong>(Generalized Linear Models, GLM)。在介绍GLM前，我们先引入一个概念：<strong>指数分布族</strong>(exponential family)。</p>
<p>指数分布族是指一类可以被表示为如下形式的概率分布：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-42f222888a89a09b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>其中η被称为分布的<strong>自然参数</strong>(natural parameter)，或者是<strong>标准参数</strong>(canonical parameter)；T(y)是<strong>充分统计量</strong>(sufficient statistic)，通常T(y)=y；a(η)是<strong>对数分割函数</strong>(log partition function)。e<sup>-a(η)</sup>通常起着归一化的作用，使得整个分布的总和/积分为1。</p>
<p>如果固定参数T, a, b，就定义了一个以η为参数的函数族。当η取不同的值，我们就得到一个不同的分布函数。</p>
<p>现在我们来证明<strong>高斯分布</strong>(Gaussian distribution)和<strong>伯努利分布</strong>(Bernoulli distribution)都属于指数分布族。</p>
<p>对于伯努利分布B(φ)，其y值为0或1，因而有p(y=1;φ)=φ; p(y=0;φ)=1-φ 。所以可推导p(y;φ)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-db162b1115ffbef7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对比指数分布族的定义，可得η=log(φ/(1-φ))，进而可得φ=1/(1+e<sup>-η</sup>)，而这正是sigmoid函数的定义。同样对比其他参数，可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-79cea267be2c0318.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>综上可得，伯努利分布属于指数分布族，且φ的形式与sigmoid函数一致。</p>
<p>接下来我们继续来看高斯分布N(μ,σ<sup>2</sup>)。回忆下之前推导线性回归的时候，σ<sup>2</sup>的值与θ和h<sub>θ</sub>(x)无关，因此为了简化证明，我们令σ<sup>2</sup>=1，所以可推导p(y;μ)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-d8bc9a34458adf12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>对比指数分布族的定义，进而可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-451c3a74a1da1157.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>因而我们证明了高斯分布也属于指数分布族。事实上，大多数概率分布都属于指数分布族，我们列举一些如下：</p>
<ul>
<li><strong>多项式分布</strong>(Multinomial distribution)：对有k个离散结果的事件建模</li>
<li><strong>泊松分布</strong>(Poisson distribution)：描述单位时间内独立事件发生次数的概率</li>
<li><strong>伽马分布</strong>(Gamma distribution)与<strong>指数分布</strong>(Exponential distribution)：描述独立事件的时间间隔的概率</li>
<li><strong>β分布(Beta distribution)</strong>：在(0,1)区间的连续概率分布</li>
<li><strong>Dirichlet分布(Dirichlet distribution)</strong>：分布的分布(for distributions over probabilities)</li>
</ul>
<h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><p>介绍完指数分布族后，我们开始正式介绍广义线性模型(GLM)。对回归或者分类问题来说，我们都可以借助于广义线性模型进行预测。广义线性模型基于如下三个假设：</p>
<ul>
<li>假设1: p(y|x;θ) 服从以η为参数的指数分布族中的某个分布</li>
<li>假设2: 给定x，我们的目标是预测T(y)的期望值，大多数情况下T(y)=y，所以假设函数可以写为h(x)=E[T(y)|x]</li>
<li>假设3: η与x是线性相关的，即η=θ<sup>T</sup>x</li>
</ul>
<p>依据这三个假设，我们可以推导出一个非常优雅的学习算法，也就是GLM。接下来我们分别看几个通过GLM推导出来的算法。</p>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>假设p(y|x;θ)服从高斯分布N(μ,σ<sup>2</sup>)，我们可以推导如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-6158a49bfff73c84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中第一个等号来自假设2，第二个等号是高斯分布的特性，第三个等号<br>来自上一节中我们已经证明了η=μ，第四个等号来自假设3。</p>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>假设p(y|x;θ)服从伯努利分布B(φ)，我们可以推导如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-46fb19bd575608f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>上式中第一个等号来自假设2，第二个等号是伯努利分布的特性，第三个等号<br>来自上一节中我们已经证明了φ=1/(1+e<sup>-η</sup>)，第四个等号来自假设3。</p>
<p>这里多介绍一些术语：将η与原始概率分布中的参数联系起来的函数g(即g(η)=E[T(y);η])称为<strong>标准响应函数</strong>(canonical response function)，它的逆函数g<sup>-1</sup>称为<strong>标准关联函数</strong>(canonical link function)。</p>
<h3 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h3><p>接下来我们来看一个更复杂的模型。在分类问题上，我们不止预测0和1两个值，假设我们预测的值有k个，即y∈{1,2,…,k}。那么我们就不能再使用伯努利分布了，我们考虑用<strong>多项式分布</strong>(Multinomial distribution)建模。</p>
<p>我们用φ<sub>1</sub>, φ<sub>2</sub>, … ,φ<sub>k</sub>表示每个结果出现的概率，即P(y=k)=φ<sub>k</sub>。由于所有结果概率之和为1，所以实际上k个参数中有1个是多余的，即：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-15c413b119e8e891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>为了使多项式分布能表示成指数分布族的形式，我们定义T(y)如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-9a827bba15dd4d75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>和我们之前的例子不一样，T(y)这次不等于y，而是一个k-1维的向量。我们用(T(y))<sub>i</sub>表示T(y)的第i个元素。</p>
<p>接下来我们引入<strong>指示函数</strong>(indicator function)：1{·}。如果参数表达式为真，则指示函数取值为1；表达式为假，指示函数取值为0，即1{True} = 1, 1{False} = 0。基于上述定义，我们可以得到：(T(y))<sub>i</sub> = 1{y = i}，进一步可得：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-c59f55335a32a473.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>现在我们可以证明多项式分布也属于指数分布族，证明如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-461bd6d335040316.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>由η的表达式，我们可以得到η和φ的对应关系：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-ca081bc1d3aa237d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-72434761f391890c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个从η和φ的映射函数被称为<strong>softmax函数</strong>(softmax function)。有了softmax函数并结合假设3，我们可以求出p(y|x;θ)为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-d74f7c7d6b907b62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这个k分类问题的算法被称为<strong>softmax回归</strong>(softmax regression)，它是逻辑回归更一般化的形式。</p>
<p>最后我们可以求出假设函数：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/2245716-e31af89c3572377c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" title="">
                </div>
                <div class="image-caption"></div>
            </figure>
<p>如果要求解参数θ，我们可以先求出它的对数似然函数l(θ)，然后用梯度上升或牛顿方法进行迭代。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>梯度上升和牛顿方法都能用于求解最大化l(θ)的问题，区别是牛顿方法收敛速度更快，但是它每次迭代的计算量也更大，当数据规模不大时总体上性能更优</li>
<li>指数分布族描述了一大类我们常见的概率分布，高斯分布、伯努利分布、多项式分布等都属于指数分布族</li>
<li>广义线性模型(GLM)描述了一种更通用的学习模型，最小二乘法和逻辑回归都可以从GLM推导出来</li>
<li>k分类问题可以用softmax回归建模，逻辑回归可以看作是softmax回归的特例(k=2)</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>斯坦福大学机器学习课CS229讲义 <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" target="_blank" rel="external">pdf</a></li>
<li>网易公开课：机器学习课程 <a href="https://open.163.com/movie/2008/1/E/D/M6SGF6VB4_M6SGHKAED.html" target="_blank" rel="external">双语字幕视频</a></li>
</ul>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2018-02-13T08:33:18.819Z" itemprop="dateUpdated">2018-02-13 16:33:18</time>
</span><br>


        
        <a href="/2018/02/13/machine-learning-glm/" target="_blank" rel="external">http://www.secondplayer.top/2018/02/13/machine-learning-glm/</a>
        
    </div>
    
    <footer>
        <a href="http://www.secondplayer.top">
            <img src="/img/gravatar.png" alt="secondplayer">
            secondplayer
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/&title=《机器学习笔记3: 广义线性模型》 — SecondPlayer's Blog&pic=http://www.secondplayer.top/img/gravatar.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/&title=《机器学习笔记3: 广义线性模型》 — SecondPlayer's Blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://www.secondplayer.top/2018/02/13/machine-learning-glm/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习笔记3: 广义线性模型》 — SecondPlayer's Blog&url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/&via=http://www.secondplayer.top" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/01/29/machine-learning-underfitting-and-overfitting/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">机器学习笔记2: 欠拟合与过拟合</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'false' == 'true',
            verify: 'false' == 'true',
            appId: "9Igi0ekMnJy1GPd4IP3n1AvX-gzGzoHsz",
            appKey: "4YLq9PqAFlVgrofe0MbTQ746",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->




</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat_pay_minicode.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat_pay_minicode.jpg" data-alipay="/img/alipay_qrcode.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>secondplayer &copy; 2016 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/&title=《机器学习笔记3: 广义线性模型》 — SecondPlayer's Blog&pic=http://www.secondplayer.top/img/gravatar.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/&title=《机器学习笔记3: 广义线性模型》 — SecondPlayer's Blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://www.secondplayer.top/2018/02/13/machine-learning-glm/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习笔记3: 广义线性模型》 — SecondPlayer's Blog&url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/&via=http://www.secondplayer.top" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://www.secondplayer.top/2018/02/13/machine-learning-glm/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKklEQVR42u3a0U7DMAwF0P3/T49nBJmunYBU5+Sp6kqXA5JxbL9e8XovVvJ8/kzyU1sLAwPjsYz3x7V65ucb8o2urpPvXVIxMDAuYKw20dtospJQnuwNAwMDoxocq29InsTAwMDYCbj7h9Kch4GBgVFN0VZ3Pt8/lThiYGDcxsir7v9//Sf9DQwMjEcxdoJgr7hfTRCjXWFgYIxm9A6oeTKXpJLRATUZ5sDAwBjKSA6ivZGLXuEsPxi/qudpDAyMhzOqwTEfvNj/NGqCYmBgjGb0WpK90Yry15+qIGJgYAxiJG3CXlGsegzeOuJiYGCMY+wX2j6H5mo6mIxrNKMyBgbGCMbORnd+Kb3C37c7GBgYFzCSMYheES1vXlb3EFUNMTAwRjDKoS1uVSZJ3uH2AwYGxlBGryjWW9XGQyFAY2BgjGZUG4enQnOvDVBuhWJgYAxiVMNuUpPP26L52w4niBgYGA9hJI/mQbDaIu2V+aK/AwYGxiBGdWDr2Aza9rAFBgbGnYxTLcZqC2EnPf3lvwcGBsaVjJ3rKubAsAUGBsY1jDywVpPI/PnyCBoGBsZQRm9gIk8Ek3Sz934MDIx7GPlKkrOk3dhrZDZnRjAwMAYxqqleL/gm4bUacDEwMG5jVIcq9jfXS/uWgRsDAwMjZlS3mB+DoxM5BgYGRutMnB9fc+qBgIuBgfFARq8ZkBfmeoE7H9rAwMCYzaiGvN7I107KmCesGBgY4xhfYTHRP/a9R2AAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


lazyScripts.push('//s95.cnzz.com/z_stat.php?id=1259512342&web_id=1259512342')

</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>






<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
